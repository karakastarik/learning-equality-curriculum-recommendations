{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3> Some materials needed for this step:</h3>\n",
    "    \n",
    "* Finetuned model from Step 1\n",
    "\n",
    "* Kfold dataset from Step 1\n",
    "    \n",
    "    \n",
    "<h3> Note:</h3>\n",
    "    \n",
    "* The 0's fold is used for validation\n",
    "\n",
    "* After generating Top-K candidates for training in the next step, we need to use the correlations file to add more label 1 in the training set, because although we get a very high max positive score at stage 1 for Top-K, some topics may have no label 1\n",
    "    \n",
    "**Reference**: https://www.kaggle.com/code/ragnar123/lecr-unsupervised-train-set-public "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T23:00:27.678127Z",
     "iopub.status.busy": "2023-02-20T23:00:27.677736Z",
     "iopub.status.idle": "2023-02-20T23:00:39.569017Z",
     "shell.execute_reply": "2023-02-20T23:00:39.567751Z",
     "shell.execute_reply.started": "2023-02-20T23:00:27.678096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 1959, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py\", line 155, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 111, in __init__\n",
      "    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_433753/2586126580.py\", line 99, in <module>\n",
      "    class CFG8:\n",
      "  File \"/tmp/ipykernel_433753/2586126580.py\", line 102, in CFG8\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\", line 658, in from_pretrained\n",
      "    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 1804, in from_pretrained\n",
      "    return cls._from_pretrained(\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 1959, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/posixpath.py\", line 392, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/posixpath.py\", line 426, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.9/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# We have a serialization from tokenizers which let us directly build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_tokenizer_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mslow_tokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_433753/2586126580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCFG8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_433753/2586126580.py\u001b[0m in \u001b[0;36mCFG8\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model/stage-1-all-MiniLM-L6-v2-epochs-1-tuned/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 )\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   1805\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2076\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# =========================================================================================\n",
    "# Libraries\n",
    "# =========================================================================================\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "#import cupy as cp\n",
    "from sklearn.model_selection import KFold\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# =========================================================================================\n",
    "# Configurations\n",
    "# =========================================================================================\n",
    "class CFG1:\n",
    "    num_workers = 24\n",
    "    model = 'model/stage-1-paraphrase-multilingual-mpnet-base-v2-epochs18'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    batch_size = 128\n",
    "    top_n = 50\n",
    "    seed = 42\n",
    "    used_columns = ['title']\n",
    "    \n",
    "class CFG2:\n",
    "    num_workers = 24\n",
    "    model = 'model/stage-1-paraphrase-MiniLM-L12-v2-epochs-20-tuned'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    batch_size = 32\n",
    "    top_n = 50\n",
    "    seed = 42\n",
    "    used_columns = ['title','text']\n",
    "    \n",
    "class CFG3:\n",
    "    num_workers = 24\n",
    "    model = 'model/stage-1-paraphrase-multilingual-mpnet-base-v2-tuned-4747'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    batch_size = 32\n",
    "    top_n = 50\n",
    "    seed = 42\n",
    "    used_columns = 'title'\n",
    "    \n",
    "class CFG4:\n",
    "    num_workers = 24\n",
    "    model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    batch_size = 32\n",
    "    top_n = 50\n",
    "    seed = 42\n",
    "    used_columns = 'title'\n",
    "    \n",
    "class CFG5:\n",
    "    num_workers = 24\n",
    "    model = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    batch_size = 32\n",
    "    top_n = 50\n",
    "    seed = 42\n",
    "    used_columns = 'title'\n",
    "    \n",
    "#class CFG6:\n",
    "#    num_workers = 24\n",
    "#    model = 'model/stage-1-all-MiniLM-L6-v2-epochs-10-tuned'\n",
    "#    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "#    batch_size = 32\n",
    "#    top_n = 50\n",
    "#    seed = 42\n",
    "#    used_columns = 'title'  \n",
    "    \n",
    "class CFG7:\n",
    "    num_workers = 24\n",
    "    model = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    batch_size = 32\n",
    "    top_n = 50\n",
    "    seed = 42\n",
    "    used_columns = 'title'\n",
    "\n",
    "class CFG8:\n",
    "    num_workers = 24\n",
    "    model = \"model/stage-1-all-MiniLM-L6-v2-epochs-1-tuned/\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    batch_size = 128\n",
    "    top_n = 50\n",
    "    seed = 42\n",
    "    used_columns = 'title'\n",
    "    \n",
    "class CFG9:\n",
    "    num_workers = 24\n",
    "    model = \"model/paraphrase-multilingual-mpnet-base-v2-epochs-2.5-tuned/\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    batch_size = 64\n",
    "    top_n = 50\n",
    "    seed = 42\n",
    "    used_columns = 'title'\n",
    "    \n",
    "CFG_list = [CFG9]\n",
    "    \n",
    "# =========================================================================================\n",
    "# Data Loading\n",
    "# =========================================================================================\n",
    "def read_data(cfg):\n",
    "    topics = pd.read_csv('data/topics.csv')\n",
    "    #topics = topics[topics.category != 'source']\n",
    "    content = pd.read_csv('data/content.csv')\n",
    "    correlations = pd.read_csv('data/kfold_correlations.csv')\n",
    "    #kfolds = cv_split(correlations, 5, 42)\n",
    "    #kfolds.to_csv('kfold_correlations_subset.csv',index=False)\n",
    "    correlations = correlations[correlations.fold == 0]\n",
    "    topics = topics.merge(correlations, how = 'inner', left_on = 'id', right_on = 'topic_id')\n",
    "    #correlations = pd.read_csv('/kaggle/input/lcrs-kfolds/kfold_correlations.csv')\n",
    "    #correlations = correlations[correlations.fold == 0]\n",
    "    # Fillna titles \n",
    "    #topics = preprocess(topics,['title','description'])\n",
    "    #content = preprocess(content,['title','description'])\n",
    "    \n",
    "    topics['title'].fillna(\"\", inplace = True)\n",
    "    content['title'].fillna(\"\", inplace = True)\n",
    "    topics['description'].fillna(\"\", inplace = True)\n",
    "    content['description'].fillna(\"\", inplace = True)\n",
    "    \n",
    "    content['text'].fillna(\"\", inplace = True)\n",
    "    content['license'].fillna(\"\", inplace = True)\n",
    "    topics['title'] =  '[CLS] ' + topics['title'] + ' <|=t_sep=|> '  + topics['description'] + ' [SEP]'\n",
    "    content['title'] = '[CLS] ' + content['title'] + ' <|=t_sep=|> '  + content['description'] + ' <|=t_sep=|> ' + content['text'] + ' <|=t_sep=|> ' + content['kind'] + ' [SEP]'\n",
    "    #corr['text1'] = corr['text1'].progress_apply(lambda x: white_spaces(x))\n",
    "    #corr['text2'] = corr['text2'].progress_apply(lambda x: white_spaces(x))\n",
    "    \n",
    "    #topics['title'] =  '[CLS] ' + topics['title'].str.lower() + ' <|=t_sep=|> '  + topics['description'] + ' [SEP]'\n",
    "    #content['title'] = '[CLS] ' + content['title'].str.lower() + ' <|=t_sep=|> ' + content['description'] + '<|=t_sep=|> ' + content['text'] + ' <|=t_sep=|> ' + content['license'] + ' [SEP]'\n",
    "    #topics['title'] = topics['title'].progress_apply(lambda x: white_spaces(x))\n",
    "    #content['title'] = content['title'].progress_apply(lambda x: white_spaces(x))\n",
    "    \n",
    "    # Sort by title length to make inference faster\n",
    "    topics['length'] = topics['title'].astype(str).apply(lambda x: len(x))\n",
    "    content['length'] = content['title'].astype(str).apply(lambda x: len(x))\n",
    "    topics.sort_values('length', inplace = True)\n",
    "    content.sort_values('length', inplace = True)\n",
    "    \n",
    "    #language_filtering = (topics.language.value_counts(normalize=True)>0.).reset_index()\n",
    "    #language_filtering.columns=['language','shape']\n",
    "    #language_filtering.loc[language_filtering[\"shape\"] == False, \"language_final\"] = \"Other\"\n",
    "    #language_filtering.loc[language_filtering[\"shape\"] == True, \"language_final\"] = language_filtering['language']\n",
    "    #content = content.merge(language_filtering[['language','language_final']])\n",
    "    #topics = topics.merge(language_filtering[['language','language_final']])\n",
    "    \n",
    "    # Drop cols\n",
    "    topics.drop(['description', 'channel', 'category', 'level', 'language', 'parent', 'has_content', 'length'], axis = 1, inplace = True)\n",
    "    content.drop(['description', 'kind', 'language', 'text', 'copyright_holder', 'license', 'length'], axis = 1, inplace = True)\n",
    "    # Reset index\n",
    "    topics.reset_index(drop = True, inplace = True)\n",
    "    content.reset_index(drop = True, inplace = True)\n",
    "    print(' ')\n",
    "    print('-' * 50)\n",
    "    print(f\"topics.shape: {topics.shape}\")\n",
    "    print(f\"content.shape: {content.shape}\")\n",
    "    print(f\"correlations.shape: {correlations.shape}\")\n",
    "    return topics, content, correlations\n",
    "\n",
    "# =========================================================================================\n",
    "# Prepare input, tokenize\n",
    "# =========================================================================================\n",
    "def prepare_input(text, cfg):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        max_length = 64,\n",
    "        truncation=True,\n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs\n",
    "\n",
    "# =========================================================================================\n",
    "# Unsupervised dataset\n",
    "# =========================================================================================\n",
    "class uns_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['title'].values\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.texts[item], self.cfg)\n",
    "        return inputs\n",
    "    \n",
    "# =========================================================================================\n",
    "# Mean pooling class\n",
    "# =========================================================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "# =========================================================================================\n",
    "# Unsupervised model\n",
    "# =========================================================================================\n",
    "class uns_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.model)\n",
    "        self.model = AutoModel.from_pretrained(cfg.model, config = self.config)\n",
    "        self.pool = MeanPooling()\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        return feature\n",
    "    \n",
    "# =========================================================================================\n",
    "# Get embeddings\n",
    "# =========================================================================================\n",
    "def get_embeddings(loader, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for step, inputs in enumerate(tqdm(loader)):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds\n",
    "\n",
    "# =========================================================================================\n",
    "# Get the amount of positive classes based on the total\n",
    "# =========================================================================================\n",
    "def get_pos_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    int_true = np.array([len(x[0] & x[1]) / len(x[0]) for x in zip(y_true, y_pred)])\n",
    "    return round(np.mean(int_true), 5)\n",
    "\n",
    "# =========================================================================================\n",
    "# F2 Score \n",
    "def f2_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4)\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "# =========================================================================================\n",
    "# Build our training set\n",
    "# =========================================================================================\n",
    "def build_training_set(topics, content, cfg):\n",
    "    # Create lists for training\n",
    "    topics_ids = []\n",
    "    content_ids = []\n",
    "    title1 = []\n",
    "    title2 = []\n",
    "    targets = []\n",
    "    folds = []\n",
    "    # Iterate over each topic\n",
    "    for k in tqdm(range(len(topics))):\n",
    "        row = topics.iloc[k]\n",
    "        topics_id = row['id']\n",
    "        topics_title = row['title']\n",
    "        predictions = row['predictions'].split(' ')\n",
    "        ground_truth = row['content_ids'].split(' ')\n",
    "        fold = row['fold']\n",
    "        for pred in predictions:\n",
    "            content_title = content.loc[pred, 'title']\n",
    "            topics_ids.append(topics_id)\n",
    "            content_ids.append(pred)\n",
    "            title1.append(topics_title)\n",
    "            title2.append(content_title)\n",
    "            folds.append(fold)\n",
    "            # If pred is in ground truth, 1 else 0\n",
    "            if pred in ground_truth:\n",
    "                targets.append(1)\n",
    "            else:\n",
    "                targets.append(0)\n",
    "    # Build training dataset\n",
    "    train = pd.DataFrame(\n",
    "        {'topics_ids': topics_ids, \n",
    "         'content_ids': content_ids, \n",
    "         'title1': title1, \n",
    "         'title2': title2, \n",
    "         'target': targets,\n",
    "         'fold' : folds}\n",
    "    )\n",
    "    # Release memory\n",
    "    del topics_ids, content_ids, title1, title2, targets\n",
    "    gc.collect()\n",
    "    return train\n",
    "    \n",
    "# =========================================================================================\n",
    "# Get neighbors\n",
    "# =========================================================================================\n",
    "\n",
    "def cv_split(train, n_folds, seed):\n",
    "    kfold = KFold(n_splits = n_folds, shuffle = True, random_state = seed)\n",
    "    for num, (train_index, val_index) in enumerate(kfold.split(train)):\n",
    "        train.loc[val_index, 'fold'] = int(num)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "    return train\n",
    "\n",
    "def white_spaces(x):\n",
    "    return re.sub(' +', ' ', x)\n",
    "\n",
    "def preprocess(df,columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].fillna(\"\")\n",
    "        #df[col] = df[col].str.strip('123.!? \\n\\t')\n",
    "        #df[col] = df[col].str[:100]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(topics, content, cfg):\n",
    "    # Create topics dataset\n",
    "    topics_dataset = uns_dataset(topics, cfg)\n",
    "    # Create content dataset\n",
    "    content_dataset = uns_dataset(content, cfg)\n",
    "    # Create topics and content dataloaders\n",
    "    topics_loader = DataLoader(\n",
    "        topics_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        collate_fn = DataCollatorWithPadding(tokenizer = cfg.tokenizer, padding = 'longest'),\n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    content_loader = DataLoader(\n",
    "        content_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        collate_fn = DataCollatorWithPadding(tokenizer = cfg.tokenizer, padding = 'longest'),\n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "        )\n",
    "    # Create unsupervised model to extract embeddings\n",
    "    model = uns_model(cfg)\n",
    "    model.to(device)\n",
    "    # Predict topics\n",
    "    topics_embeds = get_embeddings(topics_loader, model, device)\n",
    "    content_embeds = get_embeddings(content_loader, model, device)\n",
    "    return topics_embeds,content_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_embeds_all = []\n",
    "content_embeds_all = []\n",
    "for _idx, CFG in enumerate(CFG_list):\n",
    "    print(f'trying:{CFG.model}')\n",
    "    topics, content, correlations = read_data(CFG)\n",
    "    topics_embeds,content_embeds = get_neighbors(topics,content,CFG)\n",
    "    topics_embeds_all.append(topics_embeds)\n",
    "    content_embeds_all.append(content_embeds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer, models, InputExample, losses,util\n",
    "topics, content, correlations = read_data(CFG9)\n",
    "model = SentenceTransformer(CFG9.model)\n",
    "topics_embeds,content_embeds = model.encode(topics['title'],convert_to_tensor=False), model.encode(content['title'],convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "topics_embeds = np.array(query_embeddings.cpu())\n",
    "content_embeds = np.array(corpus_embeddings.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics_embeds = np.concatenate(topics_embeds_all,axis=1)\n",
    "#content_embeds = np.concatenate(content_embeds_all,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_embeds.shape, content_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.rename(columns=lambda x: \"topic_\" + x, inplace=True)\n",
    "content.rename(columns=lambda x: \"content_\" + x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "model = AnnoyIndex(topics_embeds.shape[1], 'angular')\n",
    "\n",
    "for idx in tqdm(range(content_embeds.shape[0])):\n",
    "    model.add_item(idx, content_embeds[idx])\n",
    "    \n",
    "model.build(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de6c937ad2947c79f07e471ffd2b991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs = []\n",
    "for topic_idx in tqdm(range(topics_embeds.shape[0])):\n",
    "    content_idx = model.get_nns_by_vector(topics_embeds[topic_idx],n=50)\n",
    "    df_temp = content[content.index.isin(content_idx)]\n",
    "    df_temp['topic_id'] = list(topics[topics.index.isin([topic_idx])].topic_id)[0]\n",
    "    df_temp['topic_title'] = list(topics[topics.index.isin([topic_idx])].topic_title)[0]\n",
    "    dfs.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = pd.concat(dfs).reset_index(drop=True)\n",
    "aa = candidates.groupby(['topic_id'])['content_id'].agg(list).reset_index()\n",
    "aa['predictions'] = aa.content_id.apply(lambda x: ' '.join(x))\n",
    "aa.drop('content_id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_test = aa.merge(correlations, how = 'inner', left_on = ['topic_id'], right_on = ['topic_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our max positive score is 0.42087\n",
      "Our f2_score is 0.1054\n"
     ]
    }
   ],
   "source": [
    "pos_score = get_pos_score(topics_test['content_ids'], topics_test['predictions'])\n",
    "print(f'Our max positive score is {pos_score}')\n",
    "\n",
    "f_score = f2_score(topics_test['content_ids'], topics_test['predictions'])\n",
    "print(f'Our f2_score is {f_score}')\n",
    "\n",
    "#Validation\n",
    "#Our max positive score is 0.78403\n",
    "#Our f2_score is 0.2239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our max positive score is 0.52121\n",
    "Our f2_score is 0.1356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_test['predictions'] = topics_test.predictions.apply(lambda x: x.split(' '))\n",
    "topics_test['content_ids'] = topics_test.content_ids.apply(lambda x: x.split(' '))\n",
    "gt = topics_test[['topic_id','content_ids','fold']].explode('content_ids')\n",
    "preds = topics_test[['topic_id','predictions','fold']].explode('predictions')\n",
    "candidates_df = preds.merge(gt[['topic_id','content_ids']],how='left',left_on=['topic_id','predictions'], right_on=['topic_id','content_ids'])\n",
    "candidates_df.loc[candidates_df.content_ids.isnull(),'target'] = 0\n",
    "['target'] = candidates_df.target.fillna(1)\n",
    "candidates_df.drop('content_ids',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df = candidates_df.rename(columns={'predictions':'content_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df = candidates_df.merge(topics[['topic_id','topic_title']],on='topic_id')\n",
    "candidates_df = candidates_df.merge(content[['content_id','content_title']],on='content_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>topic_title</th>\n",
       "      <th>content_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_c1de9b7501b7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>Капацитет</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_261fb7043ad1</td>\n",
       "      <td>c_c1de9b7501b7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Електричен ток и електрично напрежение</td>\n",
       "      <td>Капацитет</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_3a1f5ae9f991</td>\n",
       "      <td>c_c1de9b7501b7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Вериги с кондензатори</td>\n",
       "      <td>Капацитет</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_46415b46914b</td>\n",
       "      <td>c_c1de9b7501b7</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Електростатична индукция и кондензатори</td>\n",
       "      <td>Капацитет</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_a76d0d45b2e9</td>\n",
       "      <td>c_c1de9b7501b7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Електричен ток: преговор</td>\n",
       "      <td>Капацитет</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075845</th>\n",
       "      <td>t_fff05585df72</td>\n",
       "      <td>c_743e6319d5ae</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11: Systems of Equations and Inequalities</td>\n",
       "      <td>11.9: Solving Systems with Cramer's Rule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075846</th>\n",
       "      <td>t_fff05585df72</td>\n",
       "      <td>c_d9bbe8422c6b</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11: Systems of Equations and Inequalities</td>\n",
       "      <td>11.0: Prelude to Systems of Equations and Ineq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075847</th>\n",
       "      <td>t_fff9e5407d13</td>\n",
       "      <td>c_b43d07ea6eef</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA_U06 - El periódico</td>\n",
       "      <td>La noria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075848</th>\n",
       "      <td>t_fff9e5407d13</td>\n",
       "      <td>c_d64037a72376</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NA_U06 - El periódico</td>\n",
       "      <td>Introducción: El periódico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075849</th>\n",
       "      <td>t_fff9e5407d13</td>\n",
       "      <td>c_5a80e03b571a</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NA_U06 - El periódico</td>\n",
       "      <td>Ponte a prueba: El periódico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3075850 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               topic_id      content_id  fold  target  \\\n",
       "0        t_00004da3a1b2  c_c1de9b7501b7     1     0.0   \n",
       "1        t_261fb7043ad1  c_c1de9b7501b7     3     0.0   \n",
       "2        t_3a1f5ae9f991  c_c1de9b7501b7     0     1.0   \n",
       "3        t_46415b46914b  c_c1de9b7501b7     4     1.0   \n",
       "4        t_a76d0d45b2e9  c_c1de9b7501b7     2     0.0   \n",
       "...                 ...             ...   ...     ...   \n",
       "3075845  t_fff05585df72  c_743e6319d5ae     0     1.0   \n",
       "3075846  t_fff05585df72  c_d9bbe8422c6b     0     1.0   \n",
       "3075847  t_fff9e5407d13  c_b43d07ea6eef     4     0.0   \n",
       "3075848  t_fff9e5407d13  c_d64037a72376     4     1.0   \n",
       "3075849  t_fff9e5407d13  c_5a80e03b571a     4     1.0   \n",
       "\n",
       "                                       topic_title  \\\n",
       "0                       Откриването на резисторите   \n",
       "1           Електричен ток и електрично напрежение   \n",
       "2                            Вериги с кондензатори   \n",
       "3          Електростатична индукция и кондензатори   \n",
       "4                         Електричен ток: преговор   \n",
       "...                                            ...   \n",
       "3075845  11: Systems of Equations and Inequalities   \n",
       "3075846  11: Systems of Equations and Inequalities   \n",
       "3075847                      NA_U06 - El periódico   \n",
       "3075848                      NA_U06 - El periódico   \n",
       "3075849                      NA_U06 - El periódico   \n",
       "\n",
       "                                             content_title  \n",
       "0                                                Капацитет  \n",
       "1                                                Капацитет  \n",
       "2                                                Капацитет  \n",
       "3                                                Капацитет  \n",
       "4                                                Капацитет  \n",
       "...                                                    ...  \n",
       "3075845           11.9: Solving Systems with Cramer's Rule  \n",
       "3075846  11.0: Prelude to Systems of Equations and Ineq...  \n",
       "3075847                                           La noria  \n",
       "3075848                         Introducción: El periódico  \n",
       "3075849                       Ponte a prueba: El periódico  \n",
       "\n",
       "[3075850 rows x 6 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_ids</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>title1</th>\n",
       "      <th>title2</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_8a2c8da77d0c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agenda</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_3f51421a7c85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_db7818729577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_eb7d5e2e1744</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simon</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_60dd2fc8a271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ihab</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119822</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_70b185780f10</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>Video No. 1: Covalent Bonding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119823</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_40b1fea5ad01</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>More on the dot structure for sulfur dioxide</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119824</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_a73aa42d1be9</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>Covalent bond</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119825</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_dbce33468856</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>Diamagnetism</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119826</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_335661ac7b89</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>Ionic Bonding2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3119827 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             topics_ids     content_ids  \\\n",
       "0        t_3d9ad9931021  c_8a2c8da77d0c   \n",
       "1        t_3d9ad9931021  c_3f51421a7c85   \n",
       "2        t_3d9ad9931021  c_db7818729577   \n",
       "3        t_3d9ad9931021  c_eb7d5e2e1744   \n",
       "4        t_3d9ad9931021  c_60dd2fc8a271   \n",
       "...                 ...             ...   \n",
       "3119822  t_70da08637930  c_70b185780f10   \n",
       "3119823  t_70da08637930  c_40b1fea5ad01   \n",
       "3119824  t_70da08637930  c_a73aa42d1be9   \n",
       "3119825  t_70da08637930  c_dbce33468856   \n",
       "3119826  t_70da08637930  c_335661ac7b89   \n",
       "\n",
       "                                                    title1  \\\n",
       "0                                                      NaN   \n",
       "1                                                      NaN   \n",
       "2                                                      NaN   \n",
       "3                                                      NaN   \n",
       "4                                                      NaN   \n",
       "...                                                    ...   \n",
       "3119822  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "3119823  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "3119824  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "3119825  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "3119826  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "\n",
       "                                               title2  target  fold  \n",
       "0                                              Agenda       1     3  \n",
       "1                                                ABCD       0     3  \n",
       "2                                                 NaN       0     3  \n",
       "3                                               Simon       0     3  \n",
       "4                                                Ihab       0     3  \n",
       "...                                               ...     ...   ...  \n",
       "3119822                 Video No. 1: Covalent Bonding       0     2  \n",
       "3119823  More on the dot structure for sulfur dioxide       0     2  \n",
       "3119824                                Covalent bond        0     2  \n",
       "3119825                                  Diamagnetism       0     2  \n",
       "3119826                                Ionic Bonding2       0     2  \n",
       "\n",
       "[3119827 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/train_top50_fold0_cv_with_groundtruth_final_72044.csv')#.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df.columns = ['topics_ids','content_ids','fold','target','title1','title2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df[['topics_ids', 'content_ids', 'title1', 'title2', 'target', 'fold']].to_parquet('data/candidates_50_train_7840.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL DATA\n",
    "------------------------------------------------------------------\n",
    "#### NO TUNE\n",
    "----------------\n",
    "TOP 50\n",
    "/kaggle/input/sbert-models/paraphrase-multilingual-MiniLM-L12-v2\n",
    "Our max positive score is 0.41649\n",
    "Our f2_score is 0.1007\n",
    "\n",
    "----------------\n",
    "TOP 50\n",
    "/kaggle/input/sentence-embedding-models/paraphrase-MiniLM-L12-v2\n",
    "Our max positive score is 0.44421\n",
    "Our f2_score is 0.1099\n",
    "\n",
    "----------------\n",
    "TOP 50\n",
    "/kaggle/input/sentence-embedding-models/paraphrase-mpnet-base-v2\n",
    "Our max positive score is 0.45422\n",
    "Our f2_score is 0.1133\n",
    "\n",
    "----------------\n",
    "TOP 50\n",
    "/kaggle/input/sbert-models/paraphrase-multilingual-mpnet-base-v2\n",
    "Our max positive score is 0.42578\n",
    "Our f2_score is 0.1033\n",
    "\n",
    "---------------\n",
    "TOP 50\n",
    "/kaggle/input/paraphrasemultilingualmpnetbasev2/all-MiniLM-L6-v2\n",
    "Our max positive score is 0.47988\n",
    "Our f2_score is 0.1216\n",
    "\n",
    "------------------------------------------------------------------\n",
    "#### TUNED\n",
    "----------------\n",
    "TOP 50\n",
    "'/kaggle/input/paraphrase-multilingual-mpnet-base-v2-tuned/paraphrase-multilingual-mpnet-base-v2-exp_fold0_epochs8'\n",
    "Our max positive score is 0.68706\n",
    "Our f2_score is 0.1902\n",
    "\n",
    "----------------\n",
    "TOP 50\n",
    "'/kaggle/input/stage-1-tuned/paraphrase-multilingual-mpnet-base-v2-tuned' ##15 epoch\n",
    "Our max positive score is 0.72044\n",
    "Our f2_score is 0.201\n",
    "\n",
    "---------------\n",
    "TOP 50\n",
    "'/kaggle/input/all-minilm-l6-v2-tuned/all-MiniLM-L6-v2_fold0_epochs20/all-MiniLM-L6-v2_fold0_epochs20'\n",
    "Our max positive score is 0.62932\n",
    "Our f2_score is 0.1713\n",
    "\n",
    "---------------\n",
    "TOP 50\n",
    "'/kaggle/input/all-minilm-l6-v2-tuned/all-MiniLM-L6-v2_fold0_epochs8/all-MiniLM-L6-v2_fold0_epochs8'\n",
    "Our max positive score is 0.59703\n",
    "Our f2_score is 0.1607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T20:20:01.244452Z",
     "iopub.status.busy": "2023-02-18T20:20:01.244038Z",
     "iopub.status.idle": "2023-02-18T20:21:04.060718Z",
     "shell.execute_reply": "2023-02-18T20:21:04.059568Z",
     "shell.execute_reply.started": "2023-02-18T20:20:01.244415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fd1ae060bc44848dfe46e34056eb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 3119827 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_ids</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>title1</th>\n",
       "      <th>title2</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_8a2c8da77d0c</td>\n",
       "      <td></td>\n",
       "      <td>Agenda</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_3f51421a7c85</td>\n",
       "      <td></td>\n",
       "      <td>ABCD</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_db7818729577</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_eb7d5e2e1744</td>\n",
       "      <td></td>\n",
       "      <td>Simon</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_60dd2fc8a271</td>\n",
       "      <td></td>\n",
       "      <td>Ihab</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics_ids     content_ids title1  title2  target  fold\n",
       "0  t_3d9ad9931021  c_8a2c8da77d0c         Agenda       1     3\n",
       "1  t_3d9ad9931021  c_3f51421a7c85           ABCD       0     3\n",
       "2  t_3d9ad9931021  c_db7818729577                      0     3\n",
       "3  t_3d9ad9931021  c_eb7d5e2e1744          Simon       0     3\n",
       "4  t_3d9ad9931021  c_60dd2fc8a271           Ihab       0     3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build training set\n",
    "full_correlations = pd.read_csv('/kaggle/input/all-minilm-l6-v2-tuned/kfold_correlations.csv')\n",
    "topics_full = topics.merge(full_correlations, how = 'inner', left_on = ['id'], right_on = ['topic_id'])\n",
    "topics_full['predictions'] = topics_full.apply(lambda x: ' '.join(list(set(x.predictions.split(' ') + x.content_ids.split(' ')))) \\\n",
    "                                               if x.fold != 0 else x.predictions, axis = 1)\n",
    "train = build_training_set(topics_full, content, CFG)\n",
    "print(f'Our training set has {len(train)} rows')\n",
    "# Save train set to disk to train on another notebook\n",
    "train.to_csv(f'train_top{CFG.top_n}_fold0_cv_with_groundtruth_final_72044.csv', index = False)\n",
    "train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
