{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T11:10:46.276009Z",
     "iopub.status.busy": "2023-02-17T11:10:46.275558Z",
     "iopub.status.idle": "2023-02-17T11:10:52.662982Z",
     "shell.execute_reply": "2023-02-17T11:10:52.662029Z",
     "shell.execute_reply.started": "2023-02-17T11:10:46.275925Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Libraries\n",
    "# =========================================================================================\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "#%env TOKENIZERS_PARALLELISM=true\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:21\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# =========================================================================================\n",
    "# Configurations\n",
    "# =========================================================================================\n",
    "class CFG:\n",
    "    print_freq = 1000\n",
    "    num_workers = 24\n",
    "    model = 'model/paraphrase-multilingual-mpnet-base-v2-epochs-30-tuned'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    gradient_checkpointing = False\n",
    "    num_cycles = 0.5\n",
    "    warmup_ratio = 0.1\n",
    "    epochs = 6\n",
    "    encoder_lr = 1e-5\n",
    "    decoder_lr = 1e-4\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 32\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 0.012\n",
    "    #max_len = 512\n",
    "    max_len = 128\n",
    "    n_folds = 5\n",
    "    seed = 42\n",
    "    \n",
    "# =========================================================================================\n",
    "# Seed everything for deterministic results\n",
    "# =========================================================================================\n",
    "def seed_everything(cfg):\n",
    "    random.seed(cfg.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    torch.cuda.manual_seed(cfg.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# =========================================================================================\n",
    "# F2 score metric\n",
    "# =========================================================================================\n",
    "def f2_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4)\n",
    "\n",
    "# =========================================================================================\n",
    "# Data Loading\n",
    "# =========================================================================================\n",
    "def read_data(cfg):\n",
    "    train = pd.read_parquet('data/candidates_50_train_79927.parquet')\n",
    "    train['title1'].fillna(\"no title\", inplace = True)\n",
    "    train['title2'].fillna(\"no title\", inplace = True)\n",
    "    #topics['description'].fillna(\"no description\", inplace = True)\n",
    "    #content['description'].fillna(\"no description\", inplace = True)\n",
    "    \n",
    "    correlations = pd.read_csv('data/kfold_correlations.csv')\n",
    "    \n",
    "    # Create feature column\n",
    "    train['text'] = train['title1'] + '[SEP]' + train['title2']\n",
    "    print(' ')\n",
    "    print('-' * 50)\n",
    "    print(f\"train.shape: {train.shape}\")\n",
    "    print(f\"correlations.shape: {correlations.shape}\")\n",
    "    return train, correlations\n",
    "\n",
    "\n",
    "# =========================================================================================\n",
    "# Get max length\n",
    "# =========================================================================================\n",
    "def get_max_length(train, cfg):\n",
    "    lengths = []\n",
    "    for text in tqdm(train['text'].fillna(\"\").values, total = len(train)):\n",
    "        length = len(cfg.tokenizer(text, add_special_tokens = False)['input_ids'])\n",
    "        lengths.append(length)\n",
    "    cfg.max_len = max(lengths) + 2 # cls & sep\n",
    "    print(f\"max_len: {cfg.max_len}\")\n",
    "\n",
    "# =========================================================================================\n",
    "# Prepare input, tokenize\n",
    "# =========================================================================================\n",
    "def prepare_input(text, cfg):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "        max_length = cfg.max_len,\n",
    "        pad_to_max_length = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs\n",
    "\n",
    "# =========================================================================================\n",
    "# Custom dataset\n",
    "# =========================================================================================\n",
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df['target'].values\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.texts[item], self.cfg)\n",
    "        label = torch.tensor(self.labels[item], dtype = torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "# =========================================================================================\n",
    "# Collate function for training\n",
    "# =========================================================================================\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs\n",
    "\n",
    "# =========================================================================================\n",
    "# Mean pooling class\n",
    "# =========================================================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "# =========================================================================================\n",
    "# Model\n",
    "# =========================================================================================\n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states = True)\n",
    "        self.config.hidden_dropout = 0.0\n",
    "        self.config.hidden_dropout_prob = 0.0\n",
    "        self.config.attention_dropout = 0.0\n",
    "        self.config.attention_probs_dropout_prob = 0.0\n",
    "        self.model = AutoModel.from_pretrained(cfg.model, config = self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "    \n",
    "# =========================================================================================\n",
    "# Helper functions\n",
    "# =========================================================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "# =========================================================================================\n",
    "# Train function loop\n",
    "# =========================================================================================\n",
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = True):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, \n",
    "                          step, \n",
    "                          len(train_loader), \n",
    "                          remain = timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss = losses,\n",
    "                          grad_norm = grad_norm,\n",
    "                          lr = scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "# =========================================================================================\n",
    "# Valid function loop\n",
    "# =========================================================================================\n",
    "def valid_fn(valid_loader, model, criterion, device, cfg):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, target) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, \n",
    "                          len(valid_loader),\n",
    "                          loss = losses,\n",
    "                          remain = timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds, axis = 0)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "# =========================================================================================\n",
    "# Get best threshold\n",
    "# =========================================================================================\n",
    "def get_best_threshold(x_val, val_predictions, correlations):\n",
    "    best_score = 0\n",
    "    best_threshold = None\n",
    "    for thres in np.arange(0.001, 0.99, 0.001):\n",
    "        x_val['predictions'] = np.where(val_predictions > thres, 1, 0)\n",
    "        x_val1 = x_val[x_val['predictions'] == 1]\n",
    "        x_val1 = x_val1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n",
    "        x_val1['content_ids'] = x_val1['content_ids'].apply(lambda x: ' '.join(x))\n",
    "        x_val1.columns = ['topic_id', 'predictions']\n",
    "        x_val0 = pd.Series(x_val['topics_ids'].unique())\n",
    "        x_val0 = x_val0[~x_val0.isin(x_val1['topic_id'])]\n",
    "        x_val0 = pd.DataFrame({'topic_id': x_val0.values, 'predictions': \"\"})\n",
    "        x_val_r = pd.concat([x_val1, x_val0], axis = 0, ignore_index = True)\n",
    "        x_val_r = x_val_r.merge(correlations, how = 'left', on = 'topic_id')\n",
    "        score = f2_score(x_val_r['content_ids'], x_val_r['predictions'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = thres\n",
    "    return best_score, best_threshold\n",
    "    \n",
    "# =========================================================================================\n",
    "# Train & Evaluate\n",
    "# =========================================================================================\n",
    "def train_and_evaluate_one_fold(train, correlations, fold, cfg):\n",
    "    print(' ')\n",
    "    print(f\"========== fold: {fold} training ==========\")\n",
    "    # Split train & validation\n",
    "    x_train = train[train['fold'] != fold]\n",
    "    x_val = train[train['fold'] == fold]\n",
    "    valid_labels = x_val['target'].values\n",
    "    train_dataset = custom_dataset(x_train, cfg)\n",
    "    valid_dataset = custom_dataset(x_val, cfg)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = True, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    # Get model\n",
    "    model = custom_model(cfg)\n",
    "    model.to(device)\n",
    "    # Optimizer\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay = 0.0):\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "        model, \n",
    "        encoder_lr = cfg.encoder_lr, \n",
    "        decoder_lr = cfg.decoder_lr,\n",
    "        weight_decay = cfg.weight_decay\n",
    "    )\n",
    "    optimizer = AdamW(\n",
    "        optimizer_parameters, \n",
    "        lr = cfg.encoder_lr, \n",
    "        eps = cfg.eps, \n",
    "        betas = cfg.betas\n",
    "    )\n",
    "    num_train_steps = int(len(x_train) / cfg.batch_size * cfg.epochs)\n",
    "    num_warmup_steps = num_train_steps * cfg.warmup_ratio\n",
    "    # Scheduler\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps = num_warmup_steps, \n",
    "        num_training_steps = num_train_steps, \n",
    "        num_cycles = cfg.num_cycles\n",
    "        )\n",
    "    # Training & Validation loop\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "    best_score = 0\n",
    "    for epoch in range(cfg.epochs):\n",
    "        start_time = time.time()\n",
    "        # Train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg)\n",
    "        # Validation\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device, cfg)\n",
    "        # Compute f2_score\n",
    "        score, threshold = get_best_threshold(x_val, predictions, correlations)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        print(f'Epoch {epoch+1} - Score: {score:.4f} - Threshold: {threshold:.5f}')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(\n",
    "                {'model': model.state_dict(), 'predictions': predictions}, \n",
    "                f\"{cfg.model.replace('/', '-')}_fold{fold}_{cfg.seed}_30_{epoch}.pth\"\n",
    "                )\n",
    "            val_predictions = predictions\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # Get best threshold\n",
    "    best_score, best_threshold = get_best_threshold(x_val, val_predictions, correlations)\n",
    "    print(f'Our CV score is {best_score} using a threshold of {best_threshold}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T11:10:52.666005Z",
     "iopub.status.busy": "2023-02-17T11:10:52.665003Z",
     "iopub.status.idle": "2023-02-17T11:10:56.708092Z",
     "shell.execute_reply": "2023-02-17T11:10:56.707004Z",
     "shell.execute_reply.started": "2023-02-17T11:10:52.665964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "train.shape: (3075850, 7)\n",
      "correlations.shape: (61517, 3)\n"
     ]
    }
   ],
   "source": [
    "# Seed everything\n",
    "seed_everything(CFG)\n",
    "# Read data\n",
    "train, correlations = read_data(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_ids</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>title1</th>\n",
       "      <th>title2</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_0feaaa5dc39d</td>\n",
       "      <td>Откриването на резисторите. Language_bg. Descr...</td>\n",
       "      <td>Успоредно свързани резистори. Language_bg. Des...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Откриването на резисторите. Language_bg. Descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_09ad67f245fc</td>\n",
       "      <td>c_0feaaa5dc39d</td>\n",
       "      <td>Електрични заряди и електрично поле. Language_...</td>\n",
       "      <td>Успоредно свързани резистори. Language_bg. Des...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Електрични заряди и електрично поле. Language_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_261fb7043ad1</td>\n",
       "      <td>c_0feaaa5dc39d</td>\n",
       "      <td>Електричен ток и електрично напрежение. Langua...</td>\n",
       "      <td>Успоредно свързани резистори. Language_bg. Des...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Електричен ток и електрично напрежение. Langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_2b1b6dfd096b</td>\n",
       "      <td>c_0feaaa5dc39d</td>\n",
       "      <td>Електрично поле. Language_bg. Description: no ...</td>\n",
       "      <td>Успоредно свързани резистори. Language_bg. Des...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Електрично поле. Language_bg. Description: no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_3a1f5ae9f991</td>\n",
       "      <td>c_0feaaa5dc39d</td>\n",
       "      <td>Вериги с кондензатори. Language_bg. Descriptio...</td>\n",
       "      <td>Успоредно свързани резистори. Language_bg. Des...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Вериги с кондензатори. Language_bg. Descriptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075845</th>\n",
       "      <td>t_fea53cc2a5bb</td>\n",
       "      <td>c_c4c2b22ec356</td>\n",
       "      <td>Suma y Resta de Fracciones. Language_es. Descr...</td>\n",
       "      <td>Calcula Expresiones con Números Mixtos. Langua...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Suma y Resta de Fracciones. Language_es. Descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075846</th>\n",
       "      <td>t_ff0a0977e1fc</td>\n",
       "      <td>c_b49da9b18f9e</td>\n",
       "      <td>يتعرف التمثيل البياني للدوال المثلثية (جـا س ،...</td>\n",
       "      <td>يتعرف التمثيل البياني للدوال المثلثية (جـا س ،...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>يتعرف التمثيل البياني للدوال المثلثية (جـا س ،...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075847</th>\n",
       "      <td>t_fff9e5407d13</td>\n",
       "      <td>c_20de77522603</td>\n",
       "      <td>NA_U06 - El periódico. Language_es. Descriptio...</td>\n",
       "      <td>Resumen: El periódico. Language_es. Descriptio...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NA_U06 - El periódico. Language_es. Descriptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075848</th>\n",
       "      <td>t_fff9e5407d13</td>\n",
       "      <td>c_d64037a72376</td>\n",
       "      <td>NA_U06 - El periódico. Language_es. Descriptio...</td>\n",
       "      <td>Introducción: El periódico. Language_es. Descr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NA_U06 - El periódico. Language_es. Descriptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075849</th>\n",
       "      <td>t_fff9e5407d13</td>\n",
       "      <td>c_5a80e03b571a</td>\n",
       "      <td>NA_U06 - El periódico. Language_es. Descriptio...</td>\n",
       "      <td>Ponte a prueba: El periódico. Language_es. Des...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NA_U06 - El periódico. Language_es. Descriptio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3075850 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             topics_ids     content_ids  \\\n",
       "0        t_00004da3a1b2  c_0feaaa5dc39d   \n",
       "1        t_09ad67f245fc  c_0feaaa5dc39d   \n",
       "2        t_261fb7043ad1  c_0feaaa5dc39d   \n",
       "3        t_2b1b6dfd096b  c_0feaaa5dc39d   \n",
       "4        t_3a1f5ae9f991  c_0feaaa5dc39d   \n",
       "...                 ...             ...   \n",
       "3075845  t_fea53cc2a5bb  c_c4c2b22ec356   \n",
       "3075846  t_ff0a0977e1fc  c_b49da9b18f9e   \n",
       "3075847  t_fff9e5407d13  c_20de77522603   \n",
       "3075848  t_fff9e5407d13  c_d64037a72376   \n",
       "3075849  t_fff9e5407d13  c_5a80e03b571a   \n",
       "\n",
       "                                                    title1  \\\n",
       "0        Откриването на резисторите. Language_bg. Descr...   \n",
       "1        Електрични заряди и електрично поле. Language_...   \n",
       "2        Електричен ток и електрично напрежение. Langua...   \n",
       "3        Електрично поле. Language_bg. Description: no ...   \n",
       "4        Вериги с кондензатори. Language_bg. Descriptio...   \n",
       "...                                                    ...   \n",
       "3075845  Suma y Resta de Fracciones. Language_es. Descr...   \n",
       "3075846  يتعرف التمثيل البياني للدوال المثلثية (جـا س ،...   \n",
       "3075847  NA_U06 - El periódico. Language_es. Descriptio...   \n",
       "3075848  NA_U06 - El periódico. Language_es. Descriptio...   \n",
       "3075849  NA_U06 - El periódico. Language_es. Descriptio...   \n",
       "\n",
       "                                                    title2  target  fold  \\\n",
       "0        Успоредно свързани резистори. Language_bg. Des...     0.0     1   \n",
       "1        Успоредно свързани резистори. Language_bg. Des...     0.0     1   \n",
       "2        Успоредно свързани резистори. Language_bg. Des...     0.0     3   \n",
       "3        Успоредно свързани резистори. Language_bg. Des...     0.0     0   \n",
       "4        Успоредно свързани резистори. Language_bg. Des...     0.0     0   \n",
       "...                                                    ...     ...   ...   \n",
       "3075845  Calcula Expresiones con Números Mixtos. Langua...     1.0     2   \n",
       "3075846  يتعرف التمثيل البياني للدوال المثلثية (جـا س ،...     1.0     3   \n",
       "3075847  Resumen: El periódico. Language_es. Descriptio...     1.0     4   \n",
       "3075848  Introducción: El periódico. Language_es. Descr...     1.0     4   \n",
       "3075849  Ponte a prueba: El periódico. Language_es. Des...     1.0     4   \n",
       "\n",
       "                                                      text  \n",
       "0        Откриването на резисторите. Language_bg. Descr...  \n",
       "1        Електрични заряди и електрично поле. Language_...  \n",
       "2        Електричен ток и електрично напрежение. Langua...  \n",
       "3        Електрично поле. Language_bg. Description: no ...  \n",
       "4        Вериги с кондензатори. Language_bg. Descriptio...  \n",
       "...                                                    ...  \n",
       "3075845  Suma y Resta de Fracciones. Language_es. Descr...  \n",
       "3075846  يتعرف التمثيل البياني للدوال المثلثية (جـا س ،...  \n",
       "3075847  NA_U06 - El periódico. Language_es. Descriptio...  \n",
       "3075848  NA_U06 - El periódico. Language_es. Descriptio...  \n",
       "3075849  NA_U06 - El periódico. Language_es. Descriptio...  \n",
       "\n",
       "[3075850 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_11a1dc0bfb99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>c_0c6473c3480d c_1c57a1316568 c_5e375cf14c47 c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_0008768bdee6</td>\n",
       "      <td>c_34e1424229b4 c_7d1a964d66d5 c_aab93ee667f4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61512</th>\n",
       "      <td>t_fff830472691</td>\n",
       "      <td>c_61fb63326e5d c_8f224e321c87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61513</th>\n",
       "      <td>t_fff9e5407d13</td>\n",
       "      <td>c_026db653a269 c_0fb048a6412c c_20de77522603 c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61514</th>\n",
       "      <td>t_fffbe1d5d43c</td>\n",
       "      <td>c_46f852a49c08 c_6659207b25d5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61515</th>\n",
       "      <td>t_fffe14f1be1e</td>\n",
       "      <td>c_cece166bad6a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61516</th>\n",
       "      <td>t_fffe811a6da9</td>\n",
       "      <td>c_92b8fad372ee</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61517 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic_id                                        content_ids  fold\n",
       "0      t_00004da3a1b2  c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c...     1\n",
       "1      t_00068291e9a4  c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c...     1\n",
       "2      t_00069b63a70a                                     c_11a1dc0bfb99     4\n",
       "3      t_0006d41a73a8  c_0c6473c3480d c_1c57a1316568 c_5e375cf14c47 c...     2\n",
       "4      t_0008768bdee6       c_34e1424229b4 c_7d1a964d66d5 c_aab93ee667f4     0\n",
       "...               ...                                                ...   ...\n",
       "61512  t_fff830472691                      c_61fb63326e5d c_8f224e321c87     1\n",
       "61513  t_fff9e5407d13  c_026db653a269 c_0fb048a6412c c_20de77522603 c...     4\n",
       "61514  t_fffbe1d5d43c                      c_46f852a49c08 c_6659207b25d5     2\n",
       "61515  t_fffe14f1be1e                                     c_cece166bad6a     2\n",
       "61516  t_fffe811a6da9                                     c_92b8fad372ee     3\n",
       "\n",
       "[61517 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get max length\n",
    "#get_max_length(train, CFG)\n",
    "#2321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T11:10:59.390099Z",
     "iopub.status.busy": "2023-02-17T11:10:59.389739Z",
     "iopub.status.idle": "2023-02-17T11:19:59.654542Z",
     "shell.execute_reply": "2023-02-17T11:19:59.652840Z",
     "shell.execute_reply.started": "2023-02-17T11:10:59.390068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "========== fold: 0 training ==========\n",
      "Epoch: [1][0/76895] Elapsed 0m 1s (remain 2482m 15s) Loss: 0.7196(0.7196) Grad: 1.2228  LR: 0.00000000  \n",
      "Epoch: [1][1000/76895] Elapsed 2m 32s (remain 193m 5s) Loss: 0.5699(0.6673) Grad: 1.1713  LR: 0.00000022  \n",
      "Epoch: [1][2000/76895] Elapsed 5m 10s (remain 193m 48s) Loss: 0.2935(0.5236) Grad: 0.5687  LR: 0.00000043  \n",
      "Epoch: [1][3000/76895] Elapsed 7m 50s (remain 193m 2s) Loss: 0.2428(0.4421) Grad: 0.3255  LR: 0.00000065  \n",
      "Epoch: [1][4000/76895] Elapsed 10m 31s (remain 191m 52s) Loss: 0.3126(0.4017) Grad: 0.6944  LR: 0.00000087  \n",
      "Epoch: [1][5000/76895] Elapsed 13m 12s (remain 189m 59s) Loss: 0.2343(0.3758) Grad: 0.5461  LR: 0.00000108  \n",
      "Epoch: [1][6000/76895] Elapsed 15m 28s (remain 182m 43s) Loss: 0.0680(0.3580) Grad: 0.9157  LR: 0.00000130  \n",
      "Epoch: [1][7000/76895] Elapsed 18m 0s (remain 179m 43s) Loss: 0.2476(0.3454) Grad: 0.7912  LR: 0.00000152  \n",
      "Epoch: [1][8000/76895] Elapsed 20m 38s (remain 177m 45s) Loss: 0.2605(0.3361) Grad: 1.4408  LR: 0.00000173  \n",
      "Epoch: [1][9000/76895] Elapsed 23m 12s (remain 175m 4s) Loss: 0.4030(0.3285) Grad: 2.3047  LR: 0.00000195  \n",
      "Epoch: [1][10000/76895] Elapsed 25m 52s (remain 173m 6s) Loss: 0.2624(0.3223) Grad: 1.2459  LR: 0.00000217  \n",
      "Epoch: [1][11000/76895] Elapsed 28m 32s (remain 171m 0s) Loss: 0.2236(0.3164) Grad: 1.5069  LR: 0.00000238  \n",
      "Epoch: [1][12000/76895] Elapsed 31m 9s (remain 168m 31s) Loss: 0.3287(0.3115) Grad: 2.0797  LR: 0.00000260  \n",
      "Epoch: [1][13000/76895] Elapsed 33m 42s (remain 165m 40s) Loss: 0.3427(0.3074) Grad: 5.6238  LR: 0.00000282  \n",
      "Epoch: [1][14000/76895] Elapsed 36m 21s (remain 163m 17s) Loss: 0.1111(0.3033) Grad: 1.3795  LR: 0.00000303  \n",
      "Epoch: [1][15000/76895] Elapsed 39m 1s (remain 161m 1s) Loss: 0.2308(0.3000) Grad: 2.3252  LR: 0.00000325  \n",
      "Epoch: [1][16000/76895] Elapsed 41m 37s (remain 158m 25s) Loss: 0.3034(0.2963) Grad: 4.9037  LR: 0.00000347  \n",
      "Epoch: [1][17000/76895] Elapsed 44m 19s (remain 156m 8s) Loss: 0.2493(0.2927) Grad: 3.5931  LR: 0.00000368  \n",
      "Epoch: [1][18000/76895] Elapsed 46m 56s (remain 153m 35s) Loss: 0.2706(0.2902) Grad: 3.7757  LR: 0.00000390  \n",
      "Epoch: [1][19000/76895] Elapsed 49m 34s (remain 151m 3s) Loss: 0.2621(0.2873) Grad: 4.4210  LR: 0.00000412  \n",
      "Epoch: [1][20000/76895] Elapsed 52m 12s (remain 148m 31s) Loss: 0.3216(0.2847) Grad: 2.1266  LR: 0.00000434  \n",
      "Epoch: [1][21000/76895] Elapsed 54m 49s (remain 145m 54s) Loss: 0.1695(0.2824) Grad: 3.1201  LR: 0.00000455  \n",
      "Epoch: [1][22000/76895] Elapsed 57m 29s (remain 143m 26s) Loss: 0.2603(0.2802) Grad: 2.8125  LR: 0.00000477  \n",
      "Epoch: [1][23000/76895] Elapsed 60m 9s (remain 140m 57s) Loss: 0.1407(0.2779) Grad: 2.6973  LR: 0.00000499  \n",
      "Epoch: [1][24000/76895] Elapsed 62m 50s (remain 138m 29s) Loss: 0.2960(0.2758) Grad: 2.0140  LR: 0.00000520  \n",
      "Epoch: [1][25000/76895] Elapsed 65m 28s (remain 135m 55s) Loss: 0.1862(0.2738) Grad: 3.3617  LR: 0.00000542  \n",
      "Epoch: [1][26000/76895] Elapsed 68m 6s (remain 133m 19s) Loss: 0.1368(0.2717) Grad: 1.8595  LR: 0.00000564  \n",
      "Epoch: [1][27000/76895] Elapsed 70m 46s (remain 130m 47s) Loss: 0.3067(0.2697) Grad: 2.4520  LR: 0.00000585  \n",
      "Epoch: [1][28000/76895] Elapsed 73m 24s (remain 128m 10s) Loss: 0.1215(0.2680) Grad: 5.2750  LR: 0.00000607  \n",
      "Epoch: [1][29000/76895] Elapsed 76m 2s (remain 125m 34s) Loss: 0.1141(0.2664) Grad: 1.4057  LR: 0.00000629  \n",
      "Epoch: [1][30000/76895] Elapsed 78m 40s (remain 122m 58s) Loss: 0.1265(0.2647) Grad: 3.9058  LR: 0.00000650  \n",
      "Epoch: [1][31000/76895] Elapsed 81m 19s (remain 120m 24s) Loss: 0.1870(0.2633) Grad: 1.6057  LR: 0.00000672  \n",
      "Epoch: [1][32000/76895] Elapsed 83m 59s (remain 117m 49s) Loss: 0.2696(0.2618) Grad: 2.9442  LR: 0.00000694  \n",
      "Epoch: [1][33000/76895] Elapsed 86m 39s (remain 115m 16s) Loss: 0.3232(0.2603) Grad: 3.4779  LR: 0.00000715  \n",
      "Epoch: [1][34000/76895] Elapsed 89m 17s (remain 112m 38s) Loss: 0.3375(0.2591) Grad: 2.7275  LR: 0.00000737  \n",
      "Epoch: [1][35000/76895] Elapsed 91m 56s (remain 110m 2s) Loss: 0.0646(0.2576) Grad: 1.3857  LR: 0.00000759  \n",
      "Epoch: [1][36000/76895] Elapsed 94m 36s (remain 107m 27s) Loss: 0.4572(0.2562) Grad: 5.6410  LR: 0.00000780  \n",
      "Epoch: [1][37000/76895] Elapsed 97m 13s (remain 104m 49s) Loss: 0.5311(0.2550) Grad: 4.8854  LR: 0.00000802  \n",
      "Epoch: [1][38000/76895] Elapsed 99m 50s (remain 102m 11s) Loss: 0.2617(0.2537) Grad: 3.6205  LR: 0.00000824  \n",
      "Epoch: [1][39000/76895] Elapsed 102m 28s (remain 99m 34s) Loss: 0.2940(0.2525) Grad: 2.9035  LR: 0.00000845  \n",
      "Epoch: [1][40000/76895] Elapsed 105m 11s (remain 97m 0s) Loss: 0.1804(0.2514) Grad: 3.0797  LR: 0.00000867  \n",
      "Epoch: [1][41000/76895] Elapsed 107m 50s (remain 94m 24s) Loss: 0.2618(0.2502) Grad: 2.1075  LR: 0.00000889  \n",
      "Epoch: [1][42000/76895] Elapsed 110m 26s (remain 91m 45s) Loss: 0.0998(0.2491) Grad: 3.9309  LR: 0.00000910  \n",
      "Epoch: [1][43000/76895] Elapsed 113m 7s (remain 89m 10s) Loss: 0.2515(0.2479) Grad: 2.2602  LR: 0.00000932  \n",
      "Epoch: [1][44000/76895] Elapsed 115m 32s (remain 86m 22s) Loss: 0.3312(0.2468) Grad: 3.4713  LR: 0.00000954  \n",
      "Epoch: [1][45000/76895] Elapsed 118m 3s (remain 83m 40s) Loss: 0.1129(0.2458) Grad: 1.5176  LR: 0.00000975  \n",
      "Epoch: [1][46000/76895] Elapsed 120m 43s (remain 81m 4s) Loss: 0.3974(0.2449) Grad: 2.4613  LR: 0.00000997  \n",
      "Epoch: [1][47000/76895] Elapsed 123m 24s (remain 78m 29s) Loss: 0.0721(0.2439) Grad: 2.4405  LR: 0.00001000  \n",
      "Epoch: [1][48000/76895] Elapsed 125m 56s (remain 75m 48s) Loss: 0.1067(0.2430) Grad: 1.7481  LR: 0.00001000  \n",
      "Epoch: [1][49000/76895] Elapsed 128m 24s (remain 73m 5s) Loss: 0.2246(0.2421) Grad: 3.2519  LR: 0.00001000  \n",
      "Epoch: [1][50000/76895] Elapsed 130m 58s (remain 70m 26s) Loss: 0.3279(0.2411) Grad: 3.3502  LR: 0.00001000  \n",
      "Epoch: [1][51000/76895] Elapsed 133m 27s (remain 67m 45s) Loss: 0.1007(0.2402) Grad: 1.7719  LR: 0.00001000  \n",
      "Epoch: [1][52000/76895] Elapsed 135m 50s (remain 65m 1s) Loss: 0.2199(0.2393) Grad: 2.2619  LR: 0.00001000  \n",
      "Epoch: [1][53000/76895] Elapsed 138m 19s (remain 62m 21s) Loss: 0.0683(0.2384) Grad: 2.0589  LR: 0.00000999  \n",
      "Epoch: [1][54000/76895] Elapsed 140m 50s (remain 59m 42s) Loss: 0.1158(0.2376) Grad: 3.3553  LR: 0.00000999  \n",
      "Epoch: [1][55000/76895] Elapsed 143m 30s (remain 57m 7s) Loss: 0.1309(0.2369) Grad: 2.2294  LR: 0.00000999  \n",
      "Epoch: [1][56000/76895] Elapsed 146m 10s (remain 54m 32s) Loss: 0.0898(0.2361) Grad: 1.5058  LR: 0.00000999  \n",
      "Epoch: [1][57000/76895] Elapsed 148m 50s (remain 51m 56s) Loss: 0.1025(0.2353) Grad: 5.7284  LR: 0.00000998  \n",
      "Epoch: [1][58000/76895] Elapsed 151m 28s (remain 49m 20s) Loss: 0.0645(0.2346) Grad: 1.7978  LR: 0.00000998  \n",
      "Epoch: [1][59000/76895] Elapsed 154m 6s (remain 46m 44s) Loss: 0.2702(0.2338) Grad: 2.2791  LR: 0.00000998  \n",
      "Epoch: [1][60000/76895] Elapsed 156m 46s (remain 44m 8s) Loss: 0.1184(0.2330) Grad: 1.9054  LR: 0.00000997  \n",
      "Epoch: [1][61000/76895] Elapsed 159m 26s (remain 41m 32s) Loss: 0.2942(0.2323) Grad: 3.0790  LR: 0.00000997  \n",
      "Epoch: [1][62000/76895] Elapsed 162m 5s (remain 38m 56s) Loss: 0.0345(0.2316) Grad: 0.8673  LR: 0.00000996  \n",
      "Epoch: [1][63000/76895] Elapsed 164m 45s (remain 36m 20s) Loss: 0.2542(0.2308) Grad: 1.3660  LR: 0.00000996  \n",
      "Epoch: [1][64000/76895] Elapsed 167m 25s (remain 33m 43s) Loss: 0.1895(0.2301) Grad: 2.0861  LR: 0.00000995  \n",
      "Epoch: [1][65000/76895] Elapsed 170m 5s (remain 31m 7s) Loss: 0.2013(0.2294) Grad: 2.2032  LR: 0.00000995  \n",
      "Epoch: [1][66000/76895] Elapsed 172m 44s (remain 28m 30s) Loss: 0.3000(0.2289) Grad: 2.5711  LR: 0.00000994  \n",
      "Epoch: [1][67000/76895] Elapsed 175m 22s (remain 25m 53s) Loss: 0.2657(0.2282) Grad: 3.4498  LR: 0.00000994  \n",
      "Epoch: [1][68000/76895] Elapsed 177m 58s (remain 23m 16s) Loss: 0.2260(0.2275) Grad: 3.2211  LR: 0.00000993  \n",
      "Epoch: [1][69000/76895] Elapsed 180m 35s (remain 20m 39s) Loss: 0.2390(0.2268) Grad: 6.6150  LR: 0.00000993  \n",
      "Epoch: [1][70000/76895] Elapsed 183m 12s (remain 18m 2s) Loss: 0.4361(0.2261) Grad: 3.4003  LR: 0.00000992  \n",
      "Epoch: [1][71000/76895] Elapsed 185m 51s (remain 15m 25s) Loss: 0.0840(0.2256) Grad: 4.1936  LR: 0.00000991  \n",
      "Epoch: [1][72000/76895] Elapsed 188m 23s (remain 12m 48s) Loss: 0.1002(0.2250) Grad: 1.7382  LR: 0.00000990  \n",
      "Epoch: [1][73000/76895] Elapsed 191m 2s (remain 10m 11s) Loss: 0.1346(0.2243) Grad: 3.5088  LR: 0.00000990  \n",
      "Epoch: [1][74000/76895] Elapsed 193m 43s (remain 7m 34s) Loss: 0.0373(0.2237) Grad: 0.8270  LR: 0.00000989  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][75000/76895] Elapsed 196m 24s (remain 4m 57s) Loss: 0.3326(0.2231) Grad: 3.1371  LR: 0.00000988  \n",
      "Epoch: [1][76000/76895] Elapsed 199m 4s (remain 2m 20s) Loss: 0.0960(0.2225) Grad: 1.7521  LR: 0.00000987  \n",
      "Epoch: [1][76894/76895] Elapsed 201m 26s (remain 0m 0s) Loss: 0.1927(0.2220) Grad: 4.0354  LR: 0.00000987  \n",
      "EVAL: [0/19225] Elapsed 0m 1s (remain 415m 37s) Loss: 0.1352(0.1352) \n",
      "EVAL: [1000/19225] Elapsed 0m 49s (remain 15m 7s) Loss: 0.2280(0.1109) \n",
      "EVAL: [2000/19225] Elapsed 1m 33s (remain 13m 23s) Loss: 0.0363(0.1082) \n",
      "EVAL: [3000/19225] Elapsed 2m 20s (remain 12m 38s) Loss: 0.0642(0.1146) \n",
      "EVAL: [4000/19225] Elapsed 3m 11s (remain 12m 7s) Loss: 0.3070(0.1171) \n",
      "EVAL: [5000/19225] Elapsed 3m 59s (remain 11m 21s) Loss: 0.1035(0.1211) \n",
      "EVAL: [6000/19225] Elapsed 4m 53s (remain 10m 45s) Loss: 0.0338(0.1252) \n",
      "EVAL: [7000/19225] Elapsed 5m 39s (remain 9m 52s) Loss: 0.2100(0.1255) \n",
      "EVAL: [8000/19225] Elapsed 6m 26s (remain 9m 2s) Loss: 0.3318(0.1282) \n",
      "EVAL: [9000/19225] Elapsed 7m 20s (remain 8m 20s) Loss: 0.1170(0.1312) \n",
      "EVAL: [10000/19225] Elapsed 8m 11s (remain 7m 32s) Loss: 0.1539(0.1345) \n",
      "EVAL: [11000/19225] Elapsed 9m 3s (remain 6m 46s) Loss: 0.1851(0.1367) \n",
      "EVAL: [12000/19225] Elapsed 9m 55s (remain 5m 58s) Loss: 0.0729(0.1386) \n",
      "EVAL: [13000/19225] Elapsed 10m 47s (remain 5m 9s) Loss: 0.0525(0.1403) \n",
      "EVAL: [14000/19225] Elapsed 11m 37s (remain 4m 20s) Loss: 0.0636(0.1427) \n",
      "EVAL: [15000/19225] Elapsed 12m 29s (remain 3m 30s) Loss: 0.1637(0.1439) \n",
      "EVAL: [16000/19225] Elapsed 13m 23s (remain 2m 41s) Loss: 0.2437(0.1462) \n",
      "EVAL: [17000/19225] Elapsed 14m 17s (remain 1m 52s) Loss: 0.0414(0.1484) \n",
      "EVAL: [18000/19225] Elapsed 15m 12s (remain 1m 2s) Loss: 0.0235(0.1524) \n",
      "EVAL: [19000/19225] Elapsed 16m 7s (remain 0m 11s) Loss: 0.1401(0.1574) \n",
      "EVAL: [19224/19225] Elapsed 16m 18s (remain 0m 0s) Loss: 0.2141(0.1594) \n",
      "Epoch 1 - avg_train_loss: 0.2220  avg_val_loss: 0.1594  time: 13285s\n",
      "Epoch 1 - Score: 0.4725 - Threshold: 0.04500\n",
      "Epoch 1 - Save Best Score: 0.4725 Model\n",
      "Epoch: [2][0/76895] Elapsed 0m 2s (remain 2719m 4s) Loss: 0.1499(0.1499) Grad: 1.4130  LR: 0.00000987  \n",
      "Epoch: [2][1000/76895] Elapsed 2m 39s (remain 201m 45s) Loss: 0.0907(0.1702) Grad: 3.8670  LR: 0.00000986  \n",
      "Epoch: [2][2000/76895] Elapsed 5m 20s (remain 199m 44s) Loss: 0.1407(0.1663) Grad: 2.4947  LR: 0.00000985  \n",
      "Epoch: [2][3000/76895] Elapsed 7m 53s (remain 194m 29s) Loss: 0.0290(0.1667) Grad: 0.7295  LR: 0.00000984  \n",
      "Epoch: [2][4000/76895] Elapsed 10m 31s (remain 191m 46s) Loss: 0.0360(0.1665) Grad: 2.4558  LR: 0.00000983  \n",
      "Epoch: [2][5000/76895] Elapsed 13m 10s (remain 189m 24s) Loss: 0.0758(0.1659) Grad: 1.8349  LR: 0.00000982  \n",
      "Epoch: [2][6000/76895] Elapsed 15m 50s (remain 187m 6s) Loss: 0.0797(0.1653) Grad: 1.9583  LR: 0.00000981  \n",
      "Epoch: [2][7000/76895] Elapsed 18m 28s (remain 184m 29s) Loss: 0.0426(0.1660) Grad: 1.2786  LR: 0.00000980  \n",
      "Epoch: [2][8000/76895] Elapsed 21m 0s (remain 180m 55s) Loss: 0.3105(0.1658) Grad: 2.1958  LR: 0.00000979  \n",
      "Epoch: [2][9000/76895] Elapsed 23m 42s (remain 178m 47s) Loss: 0.1548(0.1656) Grad: 2.3607  LR: 0.00000978  \n",
      "Epoch: [2][10000/76895] Elapsed 26m 20s (remain 176m 8s) Loss: 0.0684(0.1656) Grad: 1.4125  LR: 0.00000976  \n",
      "Epoch: [2][11000/76895] Elapsed 29m 0s (remain 173m 43s) Loss: 0.1607(0.1657) Grad: 7.4050  LR: 0.00000975  \n",
      "Epoch: [2][12000/76895] Elapsed 31m 35s (remain 170m 51s) Loss: 0.0803(0.1656) Grad: 1.8999  LR: 0.00000974  \n",
      "Epoch: [2][13000/76895] Elapsed 34m 16s (remain 168m 24s) Loss: 0.1297(0.1658) Grad: 3.0817  LR: 0.00000973  \n",
      "Epoch: [2][14000/76895] Elapsed 36m 50s (remain 165m 27s) Loss: 0.0339(0.1657) Grad: 1.2412  LR: 0.00000972  \n",
      "Epoch: [2][15000/76895] Elapsed 39m 18s (remain 162m 11s) Loss: 0.0491(0.1654) Grad: 0.9305  LR: 0.00000970  \n",
      "Epoch: [2][16000/76895] Elapsed 41m 55s (remain 159m 34s) Loss: 0.1796(0.1652) Grad: 3.7495  LR: 0.00000969  \n",
      "Epoch: [2][17000/76895] Elapsed 44m 28s (remain 156m 40s) Loss: 0.0571(0.1648) Grad: 2.8459  LR: 0.00000968  \n",
      "Epoch: [2][18000/76895] Elapsed 47m 5s (remain 154m 5s) Loss: 0.0990(0.1646) Grad: 4.7468  LR: 0.00000966  \n",
      "Epoch: [2][19000/76895] Elapsed 49m 39s (remain 151m 19s) Loss: 0.0319(0.1647) Grad: 1.0441  LR: 0.00000965  \n",
      "Epoch: [2][20000/76895] Elapsed 52m 12s (remain 148m 31s) Loss: 0.1394(0.1649) Grad: 5.8514  LR: 0.00000964  \n",
      "Epoch: [2][21000/76895] Elapsed 54m 55s (remain 146m 11s) Loss: 0.1830(0.1646) Grad: 5.7986  LR: 0.00000962  \n",
      "Epoch: [2][22000/76895] Elapsed 57m 20s (remain 143m 3s) Loss: 0.1217(0.1644) Grad: 2.0995  LR: 0.00000961  \n",
      "Epoch: [2][23000/76895] Elapsed 59m 59s (remain 140m 34s) Loss: 0.4291(0.1640) Grad: 8.2774  LR: 0.00000959  \n",
      "Epoch: [2][24000/76895] Elapsed 62m 39s (remain 138m 5s) Loss: 0.0660(0.1639) Grad: 2.1598  LR: 0.00000958  \n",
      "Epoch: [2][25000/76895] Elapsed 65m 18s (remain 135m 34s) Loss: 0.2929(0.1636) Grad: 4.3666  LR: 0.00000956  \n",
      "Epoch: [2][26000/76895] Elapsed 67m 59s (remain 133m 4s) Loss: 0.1550(0.1634) Grad: 2.0571  LR: 0.00000955  \n",
      "Epoch: [2][27000/76895] Elapsed 70m 40s (remain 130m 36s) Loss: 0.2060(0.1636) Grad: 3.3776  LR: 0.00000953  \n",
      "Epoch: [2][28000/76895] Elapsed 73m 17s (remain 127m 59s) Loss: 0.1377(0.1633) Grad: 3.0432  LR: 0.00000951  \n",
      "Epoch: [2][29000/76895] Elapsed 75m 56s (remain 125m 25s) Loss: 0.0197(0.1632) Grad: 0.4346  LR: 0.00000950  \n",
      "Epoch: [2][30000/76895] Elapsed 78m 36s (remain 122m 52s) Loss: 0.1071(0.1632) Grad: 2.2029  LR: 0.00000948  \n",
      "Epoch: [2][31000/76895] Elapsed 81m 17s (remain 120m 20s) Loss: 0.0807(0.1629) Grad: 3.1725  LR: 0.00000946  \n",
      "Epoch: [2][32000/76895] Elapsed 83m 58s (remain 117m 48s) Loss: 0.2789(0.1628) Grad: 5.7402  LR: 0.00000945  \n",
      "Epoch: [2][33000/76895] Elapsed 86m 37s (remain 115m 13s) Loss: 0.1220(0.1627) Grad: 4.0187  LR: 0.00000943  \n",
      "Epoch: [2][34000/76895] Elapsed 89m 17s (remain 112m 39s) Loss: 0.0946(0.1625) Grad: 2.2908  LR: 0.00000941  \n",
      "Epoch: [2][35000/76895] Elapsed 91m 58s (remain 110m 5s) Loss: 0.0760(0.1625) Grad: 2.2676  LR: 0.00000939  \n",
      "Epoch: [2][36000/76895] Elapsed 94m 32s (remain 107m 23s) Loss: 0.1425(0.1624) Grad: 2.5432  LR: 0.00000938  \n",
      "Epoch: [2][37000/76895] Elapsed 97m 8s (remain 104m 43s) Loss: 0.2323(0.1624) Grad: 4.7442  LR: 0.00000936  \n",
      "Epoch: [2][38000/76895] Elapsed 99m 44s (remain 102m 4s) Loss: 0.0302(0.1622) Grad: 1.0702  LR: 0.00000934  \n",
      "Epoch: [2][39000/76895] Elapsed 102m 24s (remain 99m 30s) Loss: 0.3572(0.1621) Grad: 6.4168  LR: 0.00000932  \n",
      "Epoch: [2][40000/76895] Elapsed 105m 5s (remain 96m 55s) Loss: 0.5909(0.1619) Grad: 7.5835  LR: 0.00000930  \n",
      "Epoch: [2][41000/76895] Elapsed 107m 39s (remain 94m 14s) Loss: 0.0874(0.1618) Grad: 1.5832  LR: 0.00000928  \n",
      "Epoch: [2][42000/76895] Elapsed 110m 19s (remain 91m 39s) Loss: 0.2392(0.1616) Grad: 3.9247  LR: 0.00000926  \n",
      "Epoch: [2][43000/76895] Elapsed 112m 55s (remain 89m 0s) Loss: 0.0334(0.1616) Grad: 1.0239  LR: 0.00000924  \n",
      "Epoch: [2][44000/76895] Elapsed 115m 34s (remain 86m 24s) Loss: 0.0319(0.1615) Grad: 1.9364  LR: 0.00000922  \n",
      "Epoch: [2][45000/76895] Elapsed 118m 14s (remain 83m 48s) Loss: 0.3680(0.1613) Grad: 13.5184  LR: 0.00000920  \n",
      "Epoch: [2][46000/76895] Elapsed 120m 54s (remain 81m 12s) Loss: 0.3909(0.1611) Grad: 9.6391  LR: 0.00000918  \n",
      "Epoch: [2][47000/76895] Elapsed 123m 34s (remain 78m 35s) Loss: 0.3364(0.1611) Grad: 5.5391  LR: 0.00000916  \n",
      "Epoch: [2][48000/76895] Elapsed 126m 15s (remain 76m 0s) Loss: 0.1244(0.1610) Grad: 2.5722  LR: 0.00000914  \n",
      "Epoch: [2][49000/76895] Elapsed 128m 45s (remain 73m 17s) Loss: 0.0369(0.1608) Grad: 3.8472  LR: 0.00000912  \n",
      "Epoch: [2][50000/76895] Elapsed 131m 25s (remain 70m 41s) Loss: 0.0677(0.1606) Grad: 1.2727  LR: 0.00000910  \n",
      "Epoch: [2][51000/76895] Elapsed 134m 4s (remain 68m 4s) Loss: 0.2281(0.1605) Grad: 6.6877  LR: 0.00000907  \n",
      "Epoch: [2][52000/76895] Elapsed 136m 44s (remain 65m 27s) Loss: 0.0461(0.1603) Grad: 5.2399  LR: 0.00000905  \n",
      "Epoch: [2][53000/76895] Elapsed 139m 19s (remain 62m 48s) Loss: 0.0417(0.1602) Grad: 2.2328  LR: 0.00000903  \n",
      "Epoch: [2][54000/76895] Elapsed 141m 58s (remain 60m 11s) Loss: 0.2372(0.1601) Grad: 4.7177  LR: 0.00000901  \n",
      "Epoch: [2][55000/76895] Elapsed 144m 34s (remain 57m 33s) Loss: 0.0282(0.1601) Grad: 1.4331  LR: 0.00000898  \n",
      "Epoch: [2][56000/76895] Elapsed 147m 14s (remain 54m 56s) Loss: 0.1046(0.1599) Grad: 4.1042  LR: 0.00000896  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][57000/76895] Elapsed 149m 56s (remain 52m 20s) Loss: 0.2104(0.1598) Grad: 2.1619  LR: 0.00000894  \n",
      "Epoch: [2][58000/76895] Elapsed 152m 36s (remain 49m 42s) Loss: 0.0224(0.1596) Grad: 0.7820  LR: 0.00000891  \n",
      "Epoch: [2][59000/76895] Elapsed 155m 14s (remain 47m 4s) Loss: 0.1711(0.1595) Grad: 2.0286  LR: 0.00000889  \n",
      "Epoch: [2][60000/76895] Elapsed 157m 54s (remain 44m 27s) Loss: 0.2138(0.1594) Grad: 3.8880  LR: 0.00000887  \n",
      "Epoch: [2][61000/76895] Elapsed 160m 33s (remain 41m 49s) Loss: 0.2329(0.1592) Grad: 2.4660  LR: 0.00000884  \n",
      "Epoch: [2][62000/76895] Elapsed 163m 14s (remain 39m 12s) Loss: 0.0474(0.1591) Grad: 1.9785  LR: 0.00000882  \n",
      "Epoch: [2][63000/76895] Elapsed 165m 52s (remain 36m 34s) Loss: 0.0532(0.1590) Grad: 3.5187  LR: 0.00000879  \n",
      "Epoch: [2][64000/76895] Elapsed 168m 20s (remain 33m 54s) Loss: 0.0823(0.1589) Grad: 2.1266  LR: 0.00000877  \n",
      "Epoch: [2][65000/76895] Elapsed 171m 0s (remain 31m 17s) Loss: 0.2636(0.1589) Grad: 3.8442  LR: 0.00000874  \n",
      "Epoch: [2][66000/76895] Elapsed 173m 40s (remain 28m 39s) Loss: 0.0270(0.1587) Grad: 0.8060  LR: 0.00000872  \n",
      "Epoch: [2][67000/76895] Elapsed 176m 15s (remain 26m 1s) Loss: 0.0360(0.1586) Grad: 2.1921  LR: 0.00000869  \n",
      "Epoch: [2][68000/76895] Elapsed 178m 53s (remain 23m 23s) Loss: 0.2657(0.1584) Grad: 3.9730  LR: 0.00000867  \n",
      "Epoch: [2][69000/76895] Elapsed 181m 27s (remain 20m 45s) Loss: 0.3954(0.1583) Grad: 7.8590  LR: 0.00000864  \n",
      "Epoch: [2][70000/76895] Elapsed 184m 6s (remain 18m 7s) Loss: 0.0330(0.1582) Grad: 0.8017  LR: 0.00000862  \n",
      "Epoch: [2][71000/76895] Elapsed 186m 40s (remain 15m 29s) Loss: 0.0533(0.1581) Grad: 3.1176  LR: 0.00000859  \n",
      "Epoch: [2][72000/76895] Elapsed 189m 24s (remain 12m 52s) Loss: 0.0163(0.1580) Grad: 0.4689  LR: 0.00000856  \n",
      "Epoch: [2][73000/76895] Elapsed 192m 4s (remain 10m 14s) Loss: 0.1315(0.1579) Grad: 8.2539  LR: 0.00000854  \n",
      "Epoch: [2][74000/76895] Elapsed 194m 31s (remain 7m 36s) Loss: 0.3753(0.1577) Grad: 4.7926  LR: 0.00000851  \n",
      "Epoch: [2][75000/76895] Elapsed 197m 7s (remain 4m 58s) Loss: 0.4872(0.1576) Grad: 5.6958  LR: 0.00000848  \n",
      "Epoch: [2][76000/76895] Elapsed 199m 27s (remain 2m 20s) Loss: 0.2323(0.1575) Grad: 3.8859  LR: 0.00000846  \n",
      "Epoch: [2][76894/76895] Elapsed 201m 51s (remain 0m 0s) Loss: 0.1530(0.1574) Grad: 4.9198  LR: 0.00000843  \n",
      "EVAL: [0/19225] Elapsed 0m 1s (remain 409m 50s) Loss: 0.1421(0.1421) \n",
      "EVAL: [1000/19225] Elapsed 0m 50s (remain 15m 10s) Loss: 0.2162(0.0972) \n",
      "EVAL: [2000/19225] Elapsed 1m 33s (remain 13m 25s) Loss: 0.0142(0.0947) \n",
      "EVAL: [3000/19225] Elapsed 2m 20s (remain 12m 40s) Loss: 0.0856(0.1008) \n",
      "EVAL: [4000/19225] Elapsed 3m 11s (remain 12m 9s) Loss: 0.3067(0.1036) \n",
      "EVAL: [5000/19225] Elapsed 4m 0s (remain 11m 23s) Loss: 0.0171(0.1079) \n",
      "EVAL: [6000/19225] Elapsed 4m 53s (remain 10m 47s) Loss: 0.0566(0.1119) \n",
      "EVAL: [7000/19225] Elapsed 5m 40s (remain 9m 55s) Loss: 0.0600(0.1123) \n",
      "EVAL: [8000/19225] Elapsed 6m 27s (remain 9m 3s) Loss: 0.1264(0.1148) \n",
      "EVAL: [9000/19225] Elapsed 7m 21s (remain 8m 21s) Loss: 0.1255(0.1173) \n",
      "EVAL: [10000/19225] Elapsed 8m 12s (remain 7m 33s) Loss: 0.1648(0.1202) \n",
      "EVAL: [11000/19225] Elapsed 9m 4s (remain 6m 47s) Loss: 0.1279(0.1218) \n",
      "EVAL: [12000/19225] Elapsed 9m 55s (remain 5m 58s) Loss: 0.0644(0.1240) \n",
      "EVAL: [13000/19225] Elapsed 10m 47s (remain 5m 9s) Loss: 0.0542(0.1260) \n",
      "EVAL: [14000/19225] Elapsed 11m 37s (remain 4m 20s) Loss: 0.0409(0.1280) \n",
      "EVAL: [15000/19225] Elapsed 12m 30s (remain 3m 31s) Loss: 0.1338(0.1296) \n",
      "EVAL: [16000/19225] Elapsed 13m 24s (remain 2m 42s) Loss: 0.2831(0.1318) \n",
      "EVAL: [17000/19225] Elapsed 14m 18s (remain 1m 52s) Loss: 0.0778(0.1343) \n",
      "EVAL: [18000/19225] Elapsed 15m 14s (remain 1m 2s) Loss: 0.0150(0.1388) \n",
      "EVAL: [19000/19225] Elapsed 16m 9s (remain 0m 11s) Loss: 0.1603(0.1440) \n",
      "EVAL: [19224/19225] Elapsed 16m 21s (remain 0m 0s) Loss: 0.1864(0.1467) \n",
      "Epoch 2 - avg_train_loss: 0.1574  avg_val_loss: 0.1467  time: 13344s\n",
      "Epoch 2 - Score: 0.5127 - Threshold: 0.06500\n",
      "Epoch 2 - Save Best Score: 0.5127 Model\n",
      "Epoch: [3][0/76895] Elapsed 0m 2s (remain 2739m 7s) Loss: 0.0699(0.0699) Grad: 2.7568  LR: 0.00000843  \n",
      "Epoch: [3][1000/76895] Elapsed 2m 34s (remain 195m 22s) Loss: 0.0620(0.1301) Grad: 0.8759  LR: 0.00000840  \n",
      "Epoch: [3][2000/76895] Elapsed 5m 8s (remain 192m 44s) Loss: 0.1051(0.1288) Grad: 1.7821  LR: 0.00000838  \n",
      "Epoch: [3][3000/76895] Elapsed 7m 46s (remain 191m 25s) Loss: 0.0170(0.1285) Grad: 1.2736  LR: 0.00000835  \n",
      "Epoch: [3][4000/76895] Elapsed 10m 26s (remain 190m 5s) Loss: 0.1222(0.1288) Grad: 2.7421  LR: 0.00000832  \n",
      "Epoch: [3][5000/76895] Elapsed 13m 0s (remain 186m 56s) Loss: 0.1547(0.1291) Grad: 3.5923  LR: 0.00000829  \n",
      "Epoch: [3][6000/76895] Elapsed 15m 40s (remain 185m 5s) Loss: 0.2119(0.1290) Grad: 6.2242  LR: 0.00000826  \n",
      "Epoch: [3][7000/76895] Elapsed 18m 7s (remain 180m 56s) Loss: 0.0091(0.1289) Grad: 0.4355  LR: 0.00000823  \n",
      "Epoch: [3][8000/76895] Elapsed 20m 42s (remain 178m 19s) Loss: 0.0852(0.1286) Grad: 1.9966  LR: 0.00000820  \n",
      "Epoch: [3][9000/76895] Elapsed 23m 10s (remain 174m 50s) Loss: 0.1556(0.1287) Grad: 3.2258  LR: 0.00000818  \n",
      "Epoch: [3][10000/76895] Elapsed 25m 49s (remain 172m 45s) Loss: 0.0102(0.1289) Grad: 0.4032  LR: 0.00000815  \n",
      "Epoch: [3][11000/76895] Elapsed 28m 30s (remain 170m 46s) Loss: 0.1446(0.1287) Grad: 8.0898  LR: 0.00000812  \n",
      "Epoch: [3][12000/76895] Elapsed 31m 12s (remain 168m 45s) Loss: 0.1206(0.1286) Grad: 1.1636  LR: 0.00000809  \n",
      "Epoch: [3][13000/76895] Elapsed 33m 50s (remain 166m 16s) Loss: 0.1128(0.1283) Grad: 4.2317  LR: 0.00000806  \n",
      "Epoch: [3][14000/76895] Elapsed 36m 30s (remain 164m 0s) Loss: 0.0942(0.1283) Grad: 8.1557  LR: 0.00000803  \n",
      "Epoch: [3][15000/76895] Elapsed 39m 10s (remain 161m 40s) Loss: 0.0982(0.1286) Grad: 17.4621  LR: 0.00000800  \n",
      "Epoch: [3][16000/76895] Elapsed 41m 52s (remain 159m 20s) Loss: 0.2207(0.1284) Grad: 4.7104  LR: 0.00000797  \n",
      "Epoch: [3][17000/76895] Elapsed 44m 25s (remain 156m 31s) Loss: 0.1472(0.1283) Grad: 4.7992  LR: 0.00000794  \n",
      "Epoch: [3][18000/76895] Elapsed 47m 6s (remain 154m 6s) Loss: 0.0138(0.1282) Grad: 2.2881  LR: 0.00000791  \n",
      "Epoch: [3][19000/76895] Elapsed 49m 41s (remain 151m 24s) Loss: 0.3318(0.1282) Grad: 16.1182  LR: 0.00000787  \n",
      "Epoch: [3][20000/76895] Elapsed 52m 13s (remain 148m 33s) Loss: 0.3551(0.1284) Grad: 12.4130  LR: 0.00000784  \n",
      "Epoch: [3][21000/76895] Elapsed 54m 56s (remain 146m 13s) Loss: 0.2566(0.1284) Grad: 5.0104  LR: 0.00000781  \n",
      "Epoch: [3][22000/76895] Elapsed 57m 35s (remain 143m 40s) Loss: 0.0404(0.1286) Grad: 1.7115  LR: 0.00000778  \n",
      "Epoch: [3][23000/76895] Elapsed 60m 14s (remain 141m 9s) Loss: 0.0743(0.1286) Grad: 0.9688  LR: 0.00000775  \n",
      "Epoch: [3][24000/76895] Elapsed 62m 52s (remain 138m 33s) Loss: 0.1146(0.1287) Grad: 4.1509  LR: 0.00000772  \n",
      "Epoch: [3][25000/76895] Elapsed 65m 29s (remain 135m 56s) Loss: 0.1358(0.1288) Grad: 4.6114  LR: 0.00000769  \n",
      "Epoch: [3][26000/76895] Elapsed 68m 10s (remain 133m 27s) Loss: 0.2370(0.1290) Grad: 3.6972  LR: 0.00000765  \n",
      "Epoch: [3][27000/76895] Elapsed 70m 50s (remain 130m 53s) Loss: 0.2569(0.1289) Grad: 5.1411  LR: 0.00000762  \n",
      "Epoch: [3][28000/76895] Elapsed 73m 32s (remain 128m 24s) Loss: 0.3240(0.1289) Grad: 5.7489  LR: 0.00000759  \n",
      "Epoch: [3][29000/76895] Elapsed 76m 12s (remain 125m 50s) Loss: 0.2066(0.1290) Grad: 3.4718  LR: 0.00000756  \n",
      "Epoch: [3][30000/76895] Elapsed 78m 50s (remain 123m 13s) Loss: 0.0259(0.1289) Grad: 2.6416  LR: 0.00000752  \n",
      "Epoch: [3][31000/76895] Elapsed 81m 28s (remain 120m 36s) Loss: 0.0795(0.1289) Grad: 6.0706  LR: 0.00000749  \n",
      "Epoch: [3][32000/76895] Elapsed 84m 7s (remain 118m 1s) Loss: 0.0169(0.1288) Grad: 2.1204  LR: 0.00000746  \n",
      "Epoch: [3][33000/76895] Elapsed 86m 49s (remain 115m 29s) Loss: 0.1785(0.1289) Grad: 3.4420  LR: 0.00000743  \n",
      "Epoch: [3][34000/76895] Elapsed 89m 25s (remain 112m 49s) Loss: 0.1449(0.1289) Grad: 1.6085  LR: 0.00000739  \n",
      "Epoch: [3][35000/76895] Elapsed 92m 5s (remain 110m 14s) Loss: 0.0724(0.1288) Grad: 4.9841  LR: 0.00000736  \n",
      "Epoch: [3][36000/76895] Elapsed 94m 45s (remain 107m 38s) Loss: 0.0099(0.1289) Grad: 0.2526  LR: 0.00000733  \n",
      "Epoch: [3][37000/76895] Elapsed 97m 25s (remain 105m 2s) Loss: 0.3681(0.1289) Grad: 3.0723  LR: 0.00000729  \n",
      "Epoch: [3][38000/76895] Elapsed 100m 0s (remain 102m 21s) Loss: 0.0545(0.1289) Grad: 1.8683  LR: 0.00000726  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][39000/76895] Elapsed 102m 39s (remain 99m 44s) Loss: 0.2730(0.1289) Grad: 2.2165  LR: 0.00000723  \n",
      "Epoch: [3][40000/76895] Elapsed 105m 18s (remain 97m 8s) Loss: 0.0781(0.1288) Grad: 3.5879  LR: 0.00000719  \n",
      "Epoch: [3][41000/76895] Elapsed 108m 2s (remain 94m 35s) Loss: 0.1305(0.1288) Grad: 7.5995  LR: 0.00000716  \n",
      "Epoch: [3][42000/76895] Elapsed 110m 44s (remain 92m 0s) Loss: 0.3052(0.1288) Grad: 3.1671  LR: 0.00000712  \n",
      "Epoch: [3][43000/76895] Elapsed 113m 34s (remain 89m 31s) Loss: 0.0786(0.1288) Grad: 3.6545  LR: 0.00000709  \n",
      "Epoch: [3][44000/76895] Elapsed 116m 18s (remain 86m 57s) Loss: 0.2333(0.1289) Grad: 3.0487  LR: 0.00000705  \n",
      "Epoch: [3][45000/76895] Elapsed 119m 11s (remain 84m 28s) Loss: 0.3547(0.1288) Grad: 9.0711  LR: 0.00000702  \n",
      "Epoch: [3][46000/76895] Elapsed 122m 5s (remain 81m 59s) Loss: 0.0922(0.1289) Grad: 3.7474  LR: 0.00000699  \n",
      "Epoch: [3][47000/76895] Elapsed 125m 6s (remain 79m 34s) Loss: 0.0046(0.1288) Grad: 0.1806  LR: 0.00000695  \n",
      "Epoch: [3][48000/76895] Elapsed 128m 11s (remain 77m 9s) Loss: 0.2383(0.1288) Grad: 4.3396  LR: 0.00000692  \n",
      "Epoch: [3][49000/76895] Elapsed 130m 59s (remain 74m 33s) Loss: 0.0158(0.1287) Grad: 2.7308  LR: 0.00000688  \n",
      "Epoch: [3][50000/76895] Elapsed 133m 49s (remain 71m 58s) Loss: 0.0637(0.1287) Grad: 2.8092  LR: 0.00000685  \n",
      "Epoch: [3][51000/76895] Elapsed 136m 31s (remain 69m 19s) Loss: 0.2493(0.1286) Grad: 6.6781  LR: 0.00000681  \n",
      "Epoch: [3][52000/76895] Elapsed 139m 13s (remain 66m 39s) Loss: 0.2020(0.1286) Grad: 4.6843  LR: 0.00000677  \n",
      "Epoch: [3][53000/76895] Elapsed 141m 55s (remain 63m 58s) Loss: 0.0540(0.1286) Grad: 4.6081  LR: 0.00000674  \n",
      "Epoch: [3][54000/76895] Elapsed 144m 26s (remain 61m 14s) Loss: 0.0143(0.1287) Grad: 0.7087  LR: 0.00000670  \n",
      "Epoch: [3][55000/76895] Elapsed 147m 2s (remain 58m 31s) Loss: 0.1425(0.1286) Grad: 5.6840  LR: 0.00000667  \n",
      "Epoch: [3][56000/76895] Elapsed 149m 40s (remain 55m 50s) Loss: 0.0977(0.1286) Grad: 4.0700  LR: 0.00000663  \n",
      "Epoch: [3][57000/76895] Elapsed 152m 22s (remain 53m 10s) Loss: 0.1286(0.1286) Grad: 3.7839  LR: 0.00000660  \n",
      "Epoch: [3][58000/76895] Elapsed 155m 5s (remain 50m 31s) Loss: 0.1602(0.1285) Grad: 5.1777  LR: 0.00000656  \n",
      "Epoch: [3][59000/76895] Elapsed 157m 47s (remain 47m 51s) Loss: 0.0147(0.1284) Grad: 0.6217  LR: 0.00000652  \n",
      "Epoch: [3][60000/76895] Elapsed 160m 28s (remain 45m 11s) Loss: 0.0203(0.1285) Grad: 0.8045  LR: 0.00000649  \n",
      "Epoch: [3][61000/76895] Elapsed 163m 8s (remain 42m 30s) Loss: 0.0109(0.1285) Grad: 0.4321  LR: 0.00000645  \n",
      "Epoch: [3][62000/76895] Elapsed 165m 51s (remain 39m 50s) Loss: 0.0122(0.1284) Grad: 0.6848  LR: 0.00000642  \n",
      "Epoch: [3][63000/76895] Elapsed 168m 28s (remain 37m 9s) Loss: 0.2159(0.1284) Grad: 8.9060  LR: 0.00000638  \n",
      "Epoch: [3][64000/76895] Elapsed 171m 9s (remain 34m 29s) Loss: 0.1257(0.1284) Grad: 4.9274  LR: 0.00000634  \n",
      "Epoch: [3][65000/76895] Elapsed 173m 50s (remain 31m 48s) Loss: 0.0711(0.1284) Grad: 1.0547  LR: 0.00000631  \n",
      "Epoch: [3][66000/76895] Elapsed 176m 30s (remain 29m 8s) Loss: 0.1897(0.1284) Grad: 5.8536  LR: 0.00000627  \n",
      "Epoch: [3][67000/76895] Elapsed 179m 9s (remain 26m 27s) Loss: 0.0070(0.1284) Grad: 0.2530  LR: 0.00000623  \n",
      "Epoch: [3][68000/76895] Elapsed 181m 48s (remain 23m 46s) Loss: 0.0202(0.1283) Grad: 0.8207  LR: 0.00000620  \n",
      "Epoch: [3][69000/76895] Elapsed 184m 25s (remain 21m 5s) Loss: 0.2536(0.1284) Grad: 5.1590  LR: 0.00000616  \n",
      "Epoch: [3][70000/76895] Elapsed 187m 5s (remain 18m 25s) Loss: 0.0367(0.1284) Grad: 3.7627  LR: 0.00000612  \n",
      "Epoch: [3][71000/76895] Elapsed 189m 45s (remain 15m 45s) Loss: 0.4265(0.1283) Grad: 11.3584  LR: 0.00000609  \n",
      "Epoch: [3][72000/76895] Elapsed 192m 29s (remain 13m 5s) Loss: 0.1645(0.1283) Grad: 5.4655  LR: 0.00000605  \n",
      "Epoch: [3][73000/76895] Elapsed 195m 10s (remain 10m 24s) Loss: 0.1770(0.1283) Grad: 4.8069  LR: 0.00000601  \n",
      "Epoch: [3][74000/76895] Elapsed 197m 50s (remain 7m 44s) Loss: 0.1688(0.1283) Grad: 5.3836  LR: 0.00000598  \n",
      "Epoch: [3][75000/76895] Elapsed 200m 27s (remain 5m 3s) Loss: 0.0334(0.1283) Grad: 4.8174  LR: 0.00000594  \n",
      "Epoch: [3][76000/76895] Elapsed 203m 9s (remain 2m 23s) Loss: 0.0751(0.1283) Grad: 1.6985  LR: 0.00000590  \n",
      "Epoch: [3][76894/76895] Elapsed 205m 35s (remain 0m 0s) Loss: 0.0332(0.1282) Grad: 6.2810  LR: 0.00000587  \n",
      "EVAL: [0/19225] Elapsed 0m 1s (remain 449m 28s) Loss: 0.1960(0.1960) \n",
      "EVAL: [1000/19225] Elapsed 0m 49s (remain 15m 2s) Loss: 0.2417(0.1031) \n",
      "EVAL: [2000/19225] Elapsed 1m 33s (remain 13m 26s) Loss: 0.0595(0.0997) \n",
      "EVAL: [3000/19225] Elapsed 2m 20s (remain 12m 41s) Loss: 0.0138(0.1068) \n",
      "EVAL: [4000/19225] Elapsed 3m 12s (remain 12m 11s) Loss: 0.3847(0.1102) \n",
      "EVAL: [5000/19225] Elapsed 4m 0s (remain 11m 23s) Loss: 0.0028(0.1146) \n",
      "EVAL: [6000/19225] Elapsed 4m 54s (remain 10m 49s) Loss: 0.0315(0.1188) \n",
      "EVAL: [7000/19225] Elapsed 5m 43s (remain 9m 59s) Loss: 0.0111(0.1191) \n",
      "EVAL: [8000/19225] Elapsed 6m 34s (remain 9m 14s) Loss: 0.2073(0.1217) \n",
      "EVAL: [9000/19225] Elapsed 7m 33s (remain 8m 35s) Loss: 0.1475(0.1255) \n",
      "EVAL: [10000/19225] Elapsed 8m 30s (remain 7m 50s) Loss: 0.1118(0.1298) \n",
      "EVAL: [11000/19225] Elapsed 9m 26s (remain 7m 3s) Loss: 0.1223(0.1313) \n",
      "EVAL: [12000/19225] Elapsed 10m 16s (remain 6m 11s) Loss: 0.0518(0.1336) \n",
      "EVAL: [13000/19225] Elapsed 11m 8s (remain 5m 19s) Loss: 0.0122(0.1362) \n",
      "EVAL: [14000/19225] Elapsed 12m 2s (remain 4m 29s) Loss: 0.0952(0.1390) \n",
      "EVAL: [15000/19225] Elapsed 12m 58s (remain 3m 39s) Loss: 0.1710(0.1409) \n",
      "EVAL: [16000/19225] Elapsed 13m 55s (remain 2m 48s) Loss: 0.3841(0.1437) \n",
      "EVAL: [17000/19225] Elapsed 14m 54s (remain 1m 56s) Loss: 0.1105(0.1471) \n",
      "EVAL: [18000/19225] Elapsed 15m 53s (remain 1m 4s) Loss: 0.0191(0.1535) \n",
      "EVAL: [19000/19225] Elapsed 16m 52s (remain 0m 11s) Loss: 0.1665(0.1601) \n",
      "EVAL: [19224/19225] Elapsed 17m 4s (remain 0m 0s) Loss: 0.2438(0.1640) \n",
      "Epoch 3 - avg_train_loss: 0.1282  avg_val_loss: 0.1640  time: 13777s\n",
      "Epoch 3 - Score: 0.5335 - Threshold: 0.02300\n",
      "Epoch 3 - Save Best Score: 0.5335 Model\n",
      "Epoch: [4][0/76895] Elapsed 0m 3s (remain 4561m 49s) Loss: 0.0700(0.0700) Grad: 2.6338  LR: 0.00000587  \n",
      "Epoch: [4][1000/76895] Elapsed 3m 9s (remain 239m 42s) Loss: 0.0156(0.1016) Grad: 0.7411  LR: 0.00000583  \n",
      "Epoch: [4][2000/76895] Elapsed 6m 6s (remain 228m 39s) Loss: 0.2558(0.1045) Grad: 7.9652  LR: 0.00000579  \n",
      "Epoch: [4][3000/76895] Elapsed 8m 44s (remain 215m 26s) Loss: 0.0274(0.1060) Grad: 2.9988  LR: 0.00000576  \n",
      "Epoch: [4][4000/76895] Elapsed 11m 17s (remain 205m 38s) Loss: 0.0123(0.1068) Grad: 0.6109  LR: 0.00000572  \n",
      "Epoch: [4][5000/76895] Elapsed 13m 59s (remain 201m 7s) Loss: 0.1943(0.1075) Grad: 6.6012  LR: 0.00000568  \n",
      "Epoch: [4][6000/76895] Elapsed 16m 42s (remain 197m 24s) Loss: 0.0448(0.1077) Grad: 3.2415  LR: 0.00000564  \n",
      "Epoch: [4][7000/76895] Elapsed 19m 28s (remain 194m 25s) Loss: 0.0814(0.1089) Grad: 4.3750  LR: 0.00000561  \n",
      "Epoch: [4][8000/76895] Elapsed 21m 58s (remain 189m 10s) Loss: 0.2019(0.1099) Grad: 1.1221  LR: 0.00000557  \n",
      "Epoch: [4][9000/76895] Elapsed 24m 41s (remain 186m 15s) Loss: 0.0049(0.1103) Grad: 0.9072  LR: 0.00000553  \n",
      "Epoch: [4][10000/76895] Elapsed 27m 10s (remain 181m 44s) Loss: 0.0885(0.1103) Grad: 2.2923  LR: 0.00000549  \n",
      "Epoch: [4][11000/76895] Elapsed 29m 43s (remain 178m 3s) Loss: 0.1198(0.1099) Grad: 4.8474  LR: 0.00000546  \n",
      "Epoch: [4][12000/76895] Elapsed 32m 40s (remain 176m 41s) Loss: 0.1439(0.1099) Grad: 12.8465  LR: 0.00000542  \n",
      "Epoch: [4][13000/76895] Elapsed 35m 26s (remain 174m 13s) Loss: 0.2104(0.1103) Grad: 5.3769  LR: 0.00000538  \n",
      "Epoch: [4][14000/76895] Elapsed 38m 11s (remain 171m 34s) Loss: 0.0766(0.1110) Grad: 9.0072  LR: 0.00000534  \n",
      "Epoch: [4][15000/76895] Elapsed 40m 54s (remain 168m 47s) Loss: 0.0247(0.1117) Grad: 2.6008  LR: 0.00000531  \n",
      "Epoch: [4][16000/76895] Elapsed 43m 37s (remain 166m 1s) Loss: 0.1361(0.1124) Grad: 2.4869  LR: 0.00000527  \n",
      "Epoch: [4][17000/76895] Elapsed 46m 15s (remain 162m 56s) Loss: 0.1959(0.1130) Grad: 4.3968  LR: 0.00000523  \n",
      "Epoch: [4][18000/76895] Elapsed 48m 53s (remain 159m 57s) Loss: 0.0260(0.1133) Grad: 1.6344  LR: 0.00000519  \n",
      "Epoch: [4][19000/76895] Elapsed 51m 33s (remain 157m 6s) Loss: 0.0015(0.1134) Grad: 0.1420  LR: 0.00000515  \n",
      "Epoch: [4][20000/76895] Elapsed 54m 4s (remain 153m 48s) Loss: 0.0381(0.1135) Grad: 4.5399  LR: 0.00000512  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][21000/76895] Elapsed 56m 57s (remain 151m 35s) Loss: 0.1272(0.1139) Grad: 27.6585  LR: 0.00000508  \n",
      "Epoch: [4][22000/76895] Elapsed 59m 46s (remain 149m 9s) Loss: 0.1729(0.1142) Grad: 11.5602  LR: 0.00000504  \n",
      "Epoch: [4][23000/76895] Elapsed 62m 32s (remain 146m 33s) Loss: 0.1408(0.1143) Grad: 6.3499  LR: 0.00000500  \n",
      "Epoch: [4][24000/76895] Elapsed 65m 9s (remain 143m 36s) Loss: 0.1588(0.1146) Grad: 3.3582  LR: 0.00000496  \n",
      "Epoch: [4][25000/76895] Elapsed 67m 45s (remain 140m 39s) Loss: 0.5351(0.1149) Grad: 13.4911  LR: 0.00000493  \n",
      "Epoch: [4][26000/76895] Elapsed 70m 25s (remain 137m 51s) Loss: 0.0018(0.1150) Grad: 0.2886  LR: 0.00000489  \n",
      "Epoch: [4][27000/76895] Elapsed 72m 59s (remain 134m 52s) Loss: 0.0650(0.1150) Grad: 0.7245  LR: 0.00000485  \n",
      "Epoch: [4][28000/76895] Elapsed 75m 39s (remain 132m 6s) Loss: 0.1791(0.1152) Grad: 13.8533  LR: 0.00000481  \n",
      "Epoch: [4][29000/76895] Elapsed 78m 15s (remain 129m 15s) Loss: 0.1225(0.1152) Grad: 28.7551  LR: 0.00000478  \n",
      "Epoch: [4][30000/76895] Elapsed 80m 57s (remain 126m 32s) Loss: 0.0197(0.1153) Grad: 1.3545  LR: 0.00000474  \n",
      "Epoch: [4][31000/76895] Elapsed 83m 37s (remain 123m 48s) Loss: 0.2005(0.1153) Grad: 4.4886  LR: 0.00000470  \n",
      "Epoch: [4][32000/76895] Elapsed 86m 17s (remain 121m 3s) Loss: 0.2486(0.1154) Grad: 9.3960  LR: 0.00000466  \n",
      "Epoch: [4][33000/76895] Elapsed 88m 49s (remain 118m 8s) Loss: 0.2216(0.1154) Grad: 31.9646  LR: 0.00000462  \n",
      "Epoch: [4][34000/76895] Elapsed 91m 31s (remain 115m 27s) Loss: 0.2566(0.1156) Grad: 27.6747  LR: 0.00000459  \n",
      "Epoch: [4][35000/76895] Elapsed 94m 24s (remain 112m 59s) Loss: 0.4508(0.1156) Grad: 22.7802  LR: 0.00000455  \n",
      "Epoch: [4][36000/76895] Elapsed 97m 15s (remain 110m 28s) Loss: 0.0081(0.1158) Grad: 2.8000  LR: 0.00000451  \n",
      "Epoch: [4][37000/76895] Elapsed 99m 48s (remain 107m 36s) Loss: 0.0046(0.1159) Grad: 1.8401  LR: 0.00000447  \n",
      "Epoch: [4][38000/76895] Elapsed 102m 32s (remain 104m 57s) Loss: 0.0801(0.1162) Grad: 10.1726  LR: 0.00000444  \n",
      "Epoch: [4][39000/76895] Elapsed 104m 54s (remain 101m 55s) Loss: 0.1451(0.1163) Grad: 0.7506  LR: 0.00000440  \n",
      "Epoch: [4][40000/76895] Elapsed 107m 45s (remain 99m 23s) Loss: 0.1930(0.1165) Grad: 8.8899  LR: 0.00000436  \n",
      "Epoch: [4][41000/76895] Elapsed 110m 34s (remain 96m 47s) Loss: 0.1976(0.1166) Grad: 1.7445  LR: 0.00000432  \n",
      "Epoch: [4][42000/76895] Elapsed 113m 27s (remain 94m 15s) Loss: 0.2575(0.1167) Grad: 3.3314  LR: 0.00000429  \n",
      "Epoch: [4][43000/76895] Elapsed 116m 21s (remain 91m 42s) Loss: 0.0012(0.1166) Grad: 0.0470  LR: 0.00000425  \n",
      "Epoch: [4][44000/76895] Elapsed 119m 7s (remain 89m 3s) Loss: 0.1320(0.1167) Grad: 7.1966  LR: 0.00000421  \n",
      "Epoch: [4][45000/76895] Elapsed 121m 51s (remain 86m 21s) Loss: 0.3888(0.1168) Grad: 13.7730  LR: 0.00000417  \n",
      "Epoch: [4][46000/76895] Elapsed 124m 40s (remain 83m 43s) Loss: 0.3092(0.1170) Grad: 16.1609  LR: 0.00000414  \n",
      "Epoch: [4][47000/76895] Elapsed 127m 33s (remain 81m 7s) Loss: 0.1288(0.1171) Grad: 3.8524  LR: 0.00000410  \n",
      "Epoch: [4][48000/76895] Elapsed 130m 34s (remain 78m 36s) Loss: 0.1791(0.1172) Grad: 23.1455  LR: 0.00000406  \n",
      "Epoch: [4][49000/76895] Elapsed 133m 31s (remain 76m 0s) Loss: 0.0842(0.1173) Grad: 3.7580  LR: 0.00000403  \n",
      "Epoch: [4][50000/76895] Elapsed 136m 33s (remain 73m 26s) Loss: 0.1205(0.1173) Grad: 6.6917  LR: 0.00000399  \n",
      "Epoch: [4][51000/76895] Elapsed 139m 34s (remain 70m 52s) Loss: 0.0918(0.1173) Grad: 3.7540  LR: 0.00000395  \n",
      "Epoch: [4][52000/76895] Elapsed 142m 12s (remain 68m 4s) Loss: 0.0272(0.1173) Grad: 3.1942  LR: 0.00000391  \n",
      "Epoch: [4][53000/76895] Elapsed 144m 53s (remain 65m 19s) Loss: 0.1150(0.1173) Grad: 0.7554  LR: 0.00000388  \n",
      "Epoch: [4][54000/76895] Elapsed 147m 37s (remain 62m 35s) Loss: 0.1558(0.1173) Grad: 8.4000  LR: 0.00000384  \n",
      "Epoch: [4][55000/76895] Elapsed 150m 17s (remain 59m 49s) Loss: 0.0013(0.1173) Grad: 0.0606  LR: 0.00000380  \n",
      "Epoch: [4][56000/76895] Elapsed 152m 56s (remain 57m 3s) Loss: 0.1066(0.1174) Grad: 15.6764  LR: 0.00000377  \n",
      "Epoch: [4][57000/76895] Elapsed 155m 31s (remain 54m 16s) Loss: 0.0008(0.1174) Grad: 0.0535  LR: 0.00000373  \n",
      "Epoch: [4][58000/76895] Elapsed 158m 13s (remain 51m 32s) Loss: 0.0025(0.1175) Grad: 0.1585  LR: 0.00000369  \n",
      "Epoch: [4][59000/76895] Elapsed 160m 53s (remain 48m 47s) Loss: 0.1208(0.1175) Grad: 3.9895  LR: 0.00000366  \n",
      "Epoch: [4][60000/76895] Elapsed 163m 36s (remain 46m 3s) Loss: 0.0034(0.1175) Grad: 0.4051  LR: 0.00000362  \n",
      "Epoch: [4][61000/76895] Elapsed 166m 26s (remain 43m 22s) Loss: 0.0150(0.1175) Grad: 6.7167  LR: 0.00000358  \n",
      "Epoch: [4][62000/76895] Elapsed 169m 21s (remain 40m 41s) Loss: 0.0049(0.1175) Grad: 1.6178  LR: 0.00000355  \n",
      "Epoch: [4][63000/76895] Elapsed 172m 16s (remain 37m 59s) Loss: 0.0196(0.1175) Grad: 4.8399  LR: 0.00000351  \n",
      "Epoch: [4][64000/76895] Elapsed 175m 4s (remain 35m 16s) Loss: 0.0035(0.1175) Grad: 0.3472  LR: 0.00000348  \n",
      "Epoch: [4][65000/76895] Elapsed 177m 39s (remain 32m 30s) Loss: 0.0038(0.1175) Grad: 0.1878  LR: 0.00000344  \n",
      "Epoch: [4][66000/76895] Elapsed 180m 24s (remain 29m 46s) Loss: 0.1227(0.1175) Grad: 14.8719  LR: 0.00000340  \n",
      "Epoch: [4][67000/76895] Elapsed 182m 56s (remain 27m 0s) Loss: 0.0025(0.1175) Grad: 0.6534  LR: 0.00000337  \n",
      "Epoch: [4][68000/76895] Elapsed 185m 37s (remain 24m 16s) Loss: 0.1745(0.1175) Grad: 7.1583  LR: 0.00000333  \n",
      "Epoch: [4][69000/76895] Elapsed 188m 16s (remain 21m 32s) Loss: 0.0047(0.1176) Grad: 1.3830  LR: 0.00000330  \n",
      "Epoch: [4][70000/76895] Elapsed 190m 59s (remain 18m 48s) Loss: 0.2817(0.1175) Grad: 17.1167  LR: 0.00000326  \n",
      "Epoch: [4][71000/76895] Elapsed 193m 38s (remain 16m 4s) Loss: 0.0215(0.1176) Grad: 0.7745  LR: 0.00000323  \n",
      "Epoch: [4][72000/76895] Elapsed 196m 18s (remain 13m 20s) Loss: 0.0010(0.1177) Grad: 0.1164  LR: 0.00000319  \n",
      "Epoch: [4][73000/76895] Elapsed 199m 4s (remain 10m 37s) Loss: 0.2766(0.1177) Grad: 25.7877  LR: 0.00000316  \n",
      "Epoch: [4][74000/76895] Elapsed 201m 46s (remain 7m 53s) Loss: 0.1676(0.1177) Grad: 1.4254  LR: 0.00000312  \n",
      "Epoch: [4][75000/76895] Elapsed 204m 26s (remain 5m 9s) Loss: 0.1172(0.1178) Grad: 18.8054  LR: 0.00000309  \n",
      "Epoch: [4][76000/76895] Elapsed 207m 1s (remain 2m 26s) Loss: 0.1263(0.1178) Grad: 2.3995  LR: 0.00000305  \n",
      "Epoch: [4][76894/76895] Elapsed 209m 20s (remain 0m 0s) Loss: 0.0389(0.1178) Grad: 8.0442  LR: 0.00000302  \n",
      "EVAL: [0/19225] Elapsed 0m 1s (remain 474m 39s) Loss: 0.1565(0.1565) \n",
      "EVAL: [1000/19225] Elapsed 0m 51s (remain 15m 31s) Loss: 0.3338(0.1307) \n",
      "EVAL: [2000/19225] Elapsed 1m 35s (remain 13m 40s) Loss: 0.0242(0.1260) \n",
      "EVAL: [3000/19225] Elapsed 2m 23s (remain 12m 57s) Loss: 0.0056(0.1366) \n",
      "EVAL: [4000/19225] Elapsed 3m 16s (remain 12m 27s) Loss: 0.4097(0.1418) \n",
      "EVAL: [5000/19225] Elapsed 4m 36s (remain 13m 5s) Loss: 0.0002(0.1464) \n",
      "EVAL: [6000/19225] Elapsed 5m 43s (remain 12m 36s) Loss: 0.0074(0.1521) \n",
      "EVAL: [7000/19225] Elapsed 6m 31s (remain 11m 23s) Loss: 0.0014(0.1532) \n",
      "EVAL: [8000/19225] Elapsed 7m 57s (remain 11m 10s) Loss: 0.5957(0.1569) \n",
      "EVAL: [9000/19225] Elapsed 9m 17s (remain 10m 33s) Loss: 0.1635(0.1627) \n",
      "EVAL: [10000/19225] Elapsed 10m 31s (remain 9m 42s) Loss: 0.2097(0.1684) \n",
      "EVAL: [11000/19225] Elapsed 11m 24s (remain 8m 31s) Loss: 0.2023(0.1707) \n",
      "EVAL: [12000/19225] Elapsed 12m 24s (remain 7m 28s) Loss: 0.0306(0.1738) \n",
      "EVAL: [13000/19225] Elapsed 13m 15s (remain 6m 20s) Loss: 0.0079(0.1771) \n",
      "EVAL: [14000/19225] Elapsed 14m 6s (remain 5m 15s) Loss: 0.1607(0.1813) \n",
      "EVAL: [15000/19225] Elapsed 14m 58s (remain 4m 13s) Loss: 0.2493(0.1841) \n",
      "EVAL: [16000/19225] Elapsed 15m 52s (remain 3m 11s) Loss: 0.4146(0.1879) \n",
      "EVAL: [17000/19225] Elapsed 16m 47s (remain 2m 11s) Loss: 0.0125(0.1929) \n",
      "EVAL: [18000/19225] Elapsed 17m 43s (remain 1m 12s) Loss: 0.0480(0.2013) \n",
      "EVAL: [19000/19225] Elapsed 18m 38s (remain 0m 13s) Loss: 0.2409(0.2115) \n",
      "EVAL: [19224/19225] Elapsed 18m 50s (remain 0m 0s) Loss: 0.2933(0.2167) \n",
      "Epoch 4 - avg_train_loss: 0.1178  avg_val_loss: 0.2167  time: 13950s\n",
      "Epoch 4 - Score: 0.5362 - Threshold: 0.00600\n",
      "Epoch 4 - Save Best Score: 0.5362 Model\n",
      "Epoch: [5][0/76895] Elapsed 0m 2s (remain 2642m 24s) Loss: 0.2731(0.2731) Grad: 5.5463  LR: 0.00000302  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_137383/77431129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train and evaluate one fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Stage 1 Max Recall: 78403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_and_evaluate_one_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_137383/484700764.py\u001b[0m in \u001b[0;36mtrain_and_evaluate_one_fold\u001b[0;34m(train, correlations, fold, cfg)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mavg_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_137383/484700764.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             adamw(params_with_grad,\n\u001b[0m\u001b[1;32m    163\u001b[0m                   \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                   \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    220\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and evaluate one fold\n",
    "# Stage 1 Max Recall: 78403\n",
    "train_and_evaluate_one_fold(train, correlations, 0, CFG)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "========== fold: 0 training ==========\n",
    "Epoch: [1][0/78269] Elapsed 0m 1s (remain 2260m 36s) Loss: 0.7120(0.7120) Grad: 1.5701  LR: 0.00000000  \n",
    "Epoch: [1][1000/78269] Elapsed 1m 26s (remain 111m 22s) Loss: 0.4438(0.6271) Grad: 1.5454  LR: 0.00000021  \n",
    "Epoch: [1][2000/78269] Elapsed 2m 51s (remain 109m 7s) Loss: 0.3089(0.4817) Grad: 0.3681  LR: 0.00000043  \n",
    "Epoch: [1][3000/78269] Elapsed 4m 17s (remain 107m 31s) Loss: 0.2367(0.4207) Grad: 0.5546  LR: 0.00000064  \n",
    "Epoch: [1][4000/78269] Elapsed 5m 40s (remain 105m 25s) Loss: 0.1598(0.3892) Grad: 0.8652  LR: 0.00000085  \n",
    "Epoch: [1][5000/78269] Elapsed 7m 3s (remain 103m 30s) Loss: 0.2463(0.3702) Grad: 0.9135  LR: 0.00000106  \n",
    "Epoch: [1][6000/78269] Elapsed 8m 27s (remain 101m 46s) Loss: 0.3828(0.3560) Grad: 1.8164  LR: 0.00000128  \n",
    "Epoch: [1][7000/78269] Elapsed 9m 50s (remain 100m 8s) Loss: 0.3850(0.3452) Grad: 3.1474  LR: 0.00000149  \n",
    "Epoch: [1][8000/78269] Elapsed 11m 13s (remain 98m 36s) Loss: 0.3472(0.3370) Grad: 1.9940  LR: 0.00000170  \n",
    "Epoch: [1][9000/78269] Elapsed 12m 36s (remain 97m 4s) Loss: 0.1438(0.3303) Grad: 1.3507  LR: 0.00000192  \n",
    "Epoch: [1][10000/78269] Elapsed 14m 0s (remain 95m 36s) Loss: 0.2023(0.3241) Grad: 2.2278  LR: 0.00000213  \n",
    "Epoch: [1][11000/78269] Elapsed 15m 23s (remain 94m 7s) Loss: 0.1871(0.3191) Grad: 2.0018  LR: 0.00000234  \n",
    "Epoch: [1][12000/78269] Elapsed 16m 46s (remain 92m 39s) Loss: 0.2495(0.3149) Grad: 2.9007  LR: 0.00000256  \n",
    "Epoch: [1][13000/78269] Elapsed 18m 10s (remain 91m 14s) Loss: 0.4534(0.3111) Grad: 3.5416  LR: 0.00000277  \n",
    "Epoch: [1][14000/78269] Elapsed 19m 33s (remain 89m 48s) Loss: 0.1983(0.3073) Grad: 2.3728  LR: 0.00000298  \n",
    "Epoch: [1][15000/78269] Elapsed 20m 56s (remain 88m 19s) Loss: 0.1458(0.3038) Grad: 1.3323  LR: 0.00000319  \n",
    "Epoch: [1][16000/78269] Elapsed 22m 18s (remain 86m 49s) Loss: 0.2105(0.3008) Grad: 2.2263  LR: 0.00000341  \n",
    "Epoch: [1][17000/78269] Elapsed 23m 41s (remain 85m 22s) Loss: 0.1893(0.2981) Grad: 3.1229  LR: 0.00000362  \n",
    "Epoch: [1][18000/78269] Elapsed 25m 3s (remain 83m 55s) Loss: 0.1889(0.2953) Grad: 2.9211  LR: 0.00000383  \n",
    "Epoch: [1][19000/78269] Elapsed 26m 26s (remain 82m 27s) Loss: 0.2973(0.2930) Grad: 4.3314  LR: 0.00000405  \n",
    "Epoch: [1][20000/78269] Elapsed 27m 48s (remain 81m 0s) Loss: 0.2057(0.2907) Grad: 1.7820  LR: 0.00000426  \n",
    "Epoch: [1][21000/78269] Elapsed 29m 10s (remain 79m 33s) Loss: 0.1743(0.2886) Grad: 2.4722  LR: 0.00000447  \n",
    "Epoch: [1][22000/78269] Elapsed 30m 32s (remain 78m 6s) Loss: 0.1655(0.2866) Grad: 4.4318  LR: 0.00000468  \n",
    "Epoch: [1][23000/78269] Elapsed 31m 54s (remain 76m 40s) Loss: 0.2084(0.2848) Grad: 2.5483  LR: 0.00000490  \n",
    "Epoch: [1][24000/78269] Elapsed 33m 16s (remain 75m 15s) Loss: 0.2242(0.2831) Grad: 1.7516  LR: 0.00000511  \n",
    "Epoch: [1][25000/78269] Elapsed 34m 38s (remain 73m 48s) Loss: 0.0640(0.2816) Grad: 1.4669  LR: 0.00000532  \n",
    "Epoch: [1][26000/78269] Elapsed 36m 0s (remain 72m 23s) Loss: 0.3006(0.2797) Grad: 3.1642  LR: 0.00000554  \n",
    "Epoch: [1][27000/78269] Elapsed 37m 22s (remain 70m 57s) Loss: 0.0847(0.2779) Grad: 4.5314  LR: 0.00000575  \n",
    "Epoch: [1][28000/78269] Elapsed 38m 44s (remain 69m 33s) Loss: 0.2188(0.2764) Grad: 2.8374  LR: 0.00000596  \n",
    "Epoch: [1][29000/78269] Elapsed 40m 6s (remain 68m 8s) Loss: 0.3947(0.2747) Grad: 2.6061  LR: 0.00000618  \n",
    "Epoch: [1][30000/78269] Elapsed 41m 29s (remain 66m 45s) Loss: 0.2308(0.2735) Grad: 2.4709  LR: 0.00000639  \n",
    "Epoch: [1][31000/78269] Elapsed 42m 51s (remain 65m 21s) Loss: 0.2755(0.2723) Grad: 1.8329  LR: 0.00000660  \n",
    "Epoch: [1][32000/78269] Elapsed 44m 14s (remain 63m 57s) Loss: 0.2371(0.2709) Grad: 2.4145  LR: 0.00000681  \n",
    "Epoch: [1][33000/78269] Elapsed 45m 36s (remain 62m 34s) Loss: 0.1392(0.2697) Grad: 1.5320  LR: 0.00000703  \n",
    "Epoch: [1][34000/78269] Elapsed 46m 59s (remain 61m 10s) Loss: 0.1857(0.2685) Grad: 2.9083  LR: 0.00000724  \n",
    "Epoch: [1][35000/78269] Elapsed 48m 21s (remain 59m 46s) Loss: 0.2306(0.2672) Grad: 2.1018  LR: 0.00000745  \n",
    "Epoch: [1][36000/78269] Elapsed 49m 43s (remain 58m 23s) Loss: 0.1146(0.2660) Grad: 1.7258  LR: 0.00000767  \n",
    "Epoch: [1][37000/78269] Elapsed 51m 6s (remain 56m 59s) Loss: 0.2881(0.2650) Grad: 1.9609  LR: 0.00000788  \n",
    "Epoch: [1][38000/78269] Elapsed 52m 28s (remain 55m 36s) Loss: 0.1950(0.2640) Grad: 2.0378  LR: 0.00000809  \n",
    "Epoch: [1][39000/78269] Elapsed 53m 51s (remain 54m 13s) Loss: 0.3010(0.2630) Grad: 4.4506  LR: 0.00000830  \n",
    "Epoch: [1][40000/78269] Elapsed 55m 13s (remain 52m 49s) Loss: 0.3181(0.2621) Grad: 2.8970  LR: 0.00000852  \n",
    "Epoch: [1][41000/78269] Elapsed 56m 35s (remain 51m 26s) Loss: 0.3358(0.2611) Grad: 2.8687  LR: 0.00000873  \n",
    "Epoch: [1][42000/78269] Elapsed 57m 57s (remain 50m 3s) Loss: 0.3176(0.2602) Grad: 2.0962  LR: 0.00000894  \n",
    "Epoch: [1][43000/78269] Elapsed 59m 20s (remain 48m 39s) Loss: 0.0568(0.2592) Grad: 0.9268  LR: 0.00000916  \n",
    "Epoch: [1][44000/78269] Elapsed 60m 42s (remain 47m 16s) Loss: 0.0637(0.2582) Grad: 2.8651  LR: 0.00000937  \n",
    "Epoch: [1][45000/78269] Elapsed 62m 4s (remain 45m 53s) Loss: 0.2726(0.2573) Grad: 5.7472  LR: 0.00000958  \n",
    "Epoch: [1][46000/78269] Elapsed 63m 26s (remain 44m 30s) Loss: 0.3803(0.2564) Grad: 2.5286  LR: 0.00000980  \n",
    "Epoch: [1][47000/78269] Elapsed 64m 49s (remain 43m 7s) Loss: 0.0446(0.2554) Grad: 0.8770  LR: 0.00001000  \n",
    "Epoch: [1][48000/78269] Elapsed 66m 11s (remain 41m 43s) Loss: 0.2244(0.2546) Grad: 3.8869  LR: 0.00001000  \n",
    "Epoch: [1][49000/78269] Elapsed 67m 33s (remain 40m 21s) Loss: 0.1609(0.2539) Grad: 2.1088  LR: 0.00001000  \n",
    "Epoch: [1][50000/78269] Elapsed 68m 55s (remain 38m 57s) Loss: 0.2398(0.2531) Grad: 2.0056  LR: 0.00001000  \n",
    "Epoch: [1][51000/78269] Elapsed 70m 17s (remain 37m 34s) Loss: 0.2350(0.2523) Grad: 2.8891  LR: 0.00001000  \n",
    "Epoch: [1][52000/78269] Elapsed 71m 39s (remain 36m 11s) Loss: 0.2421(0.2516) Grad: 2.7053  LR: 0.00001000  \n",
    "Epoch: [1][53000/78269] Elapsed 73m 1s (remain 34m 48s) Loss: 0.1218(0.2508) Grad: 1.6472  LR: 0.00000999  \n",
    "Epoch: [1][54000/78269] Elapsed 74m 23s (remain 33m 26s) Loss: 0.2621(0.2500) Grad: 6.2667  LR: 0.00000999  \n",
    "Epoch: [1][55000/78269] Elapsed 75m 45s (remain 32m 3s) Loss: 0.2797(0.2493) Grad: 1.7595  LR: 0.00000999  \n",
    "Epoch: [1][56000/78269] Elapsed 77m 8s (remain 30m 40s) Loss: 0.0275(0.2486) Grad: 0.8356  LR: 0.00000999  \n",
    "Epoch: [1][57000/78269] Elapsed 78m 30s (remain 29m 17s) Loss: 0.4394(0.2478) Grad: 4.1900  LR: 0.00000999  \n",
    "Epoch: [1][58000/78269] Elapsed 79m 52s (remain 27m 54s) Loss: 0.0763(0.2471) Grad: 1.3862  LR: 0.00000998  \n",
    "Epoch: [1][59000/78269] Elapsed 81m 14s (remain 26m 31s) Loss: 0.1655(0.2465) Grad: 1.4423  LR: 0.00000998  \n",
    "Epoch: [1][60000/78269] Elapsed 82m 37s (remain 25m 9s) Loss: 0.1615(0.2458) Grad: 1.8937  LR: 0.00000998  \n",
    "Epoch: [1][61000/78269] Elapsed 83m 59s (remain 23m 46s) Loss: 0.1440(0.2452) Grad: 1.6254  LR: 0.00000997  \n",
    "Epoch: [1][62000/78269] Elapsed 85m 21s (remain 22m 23s) Loss: 0.1134(0.2446) Grad: 2.3672  LR: 0.00000997  \n",
    "Epoch: [1][63000/78269] Elapsed 86m 44s (remain 21m 1s) Loss: 0.2467(0.2439) Grad: 3.7494  LR: 0.00000996  \n",
    "Epoch: [1][64000/78269] Elapsed 88m 6s (remain 19m 38s) Loss: 0.0889(0.2433) Grad: 1.0698  LR: 0.00000996  \n",
    "Epoch: [1][65000/78269] Elapsed 89m 28s (remain 18m 15s) Loss: 0.1732(0.2427) Grad: 2.4345  LR: 0.00000996  \n",
    "Epoch: [1][66000/78269] Elapsed 90m 50s (remain 16m 53s) Loss: 0.0685(0.2421) Grad: 1.1233  LR: 0.00000995  \n",
    "Epoch: [1][67000/78269] Elapsed 92m 13s (remain 15m 30s) Loss: 0.1262(0.2415) Grad: 2.1719  LR: 0.00000994  \n",
    "Epoch: [1][68000/78269] Elapsed 93m 35s (remain 14m 7s) Loss: 0.1344(0.2409) Grad: 1.6418  LR: 0.00000994  \n",
    "Epoch: [1][69000/78269] Elapsed 94m 57s (remain 12m 45s) Loss: 0.0877(0.2404) Grad: 0.9137  LR: 0.00000993  \n",
    "Epoch: [1][70000/78269] Elapsed 96m 20s (remain 11m 22s) Loss: 0.1290(0.2399) Grad: 1.1786  LR: 0.00000993  \n",
    "Epoch: [1][71000/78269] Elapsed 97m 42s (remain 10m 0s) Loss: 0.3026(0.2395) Grad: 2.4442  LR: 0.00000992  \n",
    "Epoch: [1][72000/78269] Elapsed 99m 4s (remain 8m 37s) Loss: 0.2661(0.2389) Grad: 10.2384  LR: 0.00000991  \n",
    "Epoch: [1][73000/78269] Elapsed 100m 26s (remain 7m 14s) Loss: 0.0842(0.2383) Grad: 2.1289  LR: 0.00000991  \n",
    "Epoch: [1][74000/78269] Elapsed 101m 49s (remain 5m 52s) Loss: 0.0691(0.2378) Grad: 17.0663  LR: 0.00000990  \n",
    "Epoch: [1][75000/78269] Elapsed 103m 11s (remain 4m 29s) Loss: 0.2485(0.2373) Grad: 2.9377  LR: 0.00000989  \n",
    "Epoch: [1][76000/78269] Elapsed 104m 33s (remain 3m 7s) Loss: 0.3029(0.2368) Grad: 3.2337  LR: 0.00000988  \n",
    "Epoch: [1][77000/78269] Elapsed 105m 56s (remain 1m 44s) Loss: 0.2348(0.2362) Grad: 1.7777  LR: 0.00000988  \n",
    "Epoch: [1][78000/78269] Elapsed 107m 18s (remain 0m 22s) Loss: 0.2132(0.2357) Grad: 3.7708  LR: 0.00000987  \n",
    "Epoch: [1][78268/78269] Elapsed 107m 40s (remain 0m 0s) Loss: 0.2977(0.2355) Grad: 3.1356  LR: 0.00000987  \n",
    "EVAL: [0/19225] Elapsed 0m 0s (remain 320m 4s) Loss: 0.0162(0.0162) \n",
    "EVAL: [1000/19225] Elapsed 0m 13s (remain 4m 0s) Loss: 0.4839(0.1657) \n",
    "EVAL: [2000/19225] Elapsed 0m 24s (remain 3m 33s) Loss: 0.0008(0.1470) \n",
    "EVAL: [3000/19225] Elapsed 0m 35s (remain 3m 10s) Loss: 0.0008(0.1233) \n",
    "EVAL: [4000/19225] Elapsed 0m 48s (remain 3m 3s) Loss: 0.1303(0.1200) \n",
    "EVAL: [5000/19225] Elapsed 1m 1s (remain 2m 55s) Loss: 0.0074(0.1293) \n",
    "EVAL: [6000/19225] Elapsed 1m 15s (remain 2m 46s) Loss: 0.1010(0.1365) \n",
    "EVAL: [7000/19225] Elapsed 1m 29s (remain 2m 36s) Loss: 0.1494(0.1442) \n",
    "EVAL: [8000/19225] Elapsed 1m 44s (remain 2m 26s) Loss: 0.4008(0.1490) \n",
    "EVAL: [9000/19225] Elapsed 1m 59s (remain 2m 15s) Loss: 0.3023(0.1515) \n",
    "EVAL: [10000/19225] Elapsed 2m 14s (remain 2m 3s) Loss: 0.0901(0.1550) \n",
    "EVAL: [11000/19225] Elapsed 2m 29s (remain 1m 51s) Loss: 0.2495(0.1564) \n",
    "EVAL: [12000/19225] Elapsed 2m 45s (remain 1m 39s) Loss: 0.0063(0.1591) \n",
    "EVAL: [13000/19225] Elapsed 3m 0s (remain 1m 26s) Loss: 0.0733(0.1602) \n",
    "EVAL: [14000/19225] Elapsed 3m 17s (remain 1m 13s) Loss: 0.0047(0.1612) \n",
    "EVAL: [15000/19225] Elapsed 3m 34s (remain 1m 0s) Loss: 0.3380(0.1639) \n",
    "EVAL: [16000/19225] Elapsed 3m 51s (remain 0m 46s) Loss: 0.3581(0.1646) \n",
    "EVAL: [17000/19225] Elapsed 4m 9s (remain 0m 32s) Loss: 0.1334(0.1652) \n",
    "EVAL: [18000/19225] Elapsed 4m 28s (remain 0m 18s) Loss: 0.1469(0.1658) \n",
    "EVAL: [19000/19225] Elapsed 4m 49s (remain 0m 3s) Loss: 0.5381(0.1656) \n",
    "EVAL: [19224/19225] Elapsed 4m 54s (remain 0m 0s) Loss: 0.1550(0.1653) \n",
    "Epoch 1 - avg_train_loss: 0.2355  avg_val_loss: 0.1653  time: 6951s\n",
    "Epoch 1 - Score: 0.4112 - Threshold: 0.03700\n",
    "Epoch 1 - Save Best Score: 0.4112 Model\n",
    "Epoch: [2][0/78269] Elapsed 0m 1s (remain 1554m 30s) Loss: 0.2025(0.2025) Grad: 2.8341  LR: 0.00000987  \n",
    "Epoch: [2][1000/78269] Elapsed 1m 23s (remain 107m 33s) Loss: 0.0399(0.1889) Grad: 0.6968  LR: 0.00000986  \n",
    "Epoch: [2][2000/78269] Elapsed 2m 46s (remain 105m 32s) Loss: 0.1259(0.1852) Grad: 1.9818  LR: 0.00000985  \n",
    "Epoch: [2][3000/78269] Elapsed 4m 8s (remain 104m 4s) Loss: 0.2472(0.1834) Grad: 2.4412  LR: 0.00000984  \n",
    "Epoch: [2][4000/78269] Elapsed 5m 31s (remain 102m 38s) Loss: 0.1045(0.1826) Grad: 3.4805  LR: 0.00000983  \n",
    "Epoch: [2][5000/78269] Elapsed 6m 54s (remain 101m 14s) Loss: 0.3046(0.1834) Grad: 3.6378  LR: 0.00000982  \n",
    "Epoch: [2][6000/78269] Elapsed 8m 17s (remain 99m 52s) Loss: 0.0773(0.1837) Grad: 5.5224  LR: 0.00000981  \n",
    "Epoch: [2][7000/78269] Elapsed 9m 40s (remain 98m 26s) Loss: 0.3321(0.1840) Grad: 6.0008  LR: 0.00000980  \n",
    "Epoch: [2][8000/78269] Elapsed 11m 2s (remain 96m 55s) Loss: 0.0260(0.1842) Grad: 0.6304  LR: 0.00000979  \n",
    "Epoch: [2][9000/78269] Elapsed 12m 23s (remain 95m 25s) Loss: 0.0956(0.1842) Grad: 3.2497  LR: 0.00000978  \n",
    "Epoch: [2][10000/78269] Elapsed 13m 45s (remain 93m 57s) Loss: 0.1409(0.1839) Grad: 1.9610  LR: 0.00000977  \n",
    "Epoch: [2][11000/78269] Elapsed 15m 8s (remain 92m 32s) Loss: 0.1797(0.1841) Grad: 2.7107  LR: 0.00000975  \n",
    "Epoch: [2][12000/78269] Elapsed 16m 30s (remain 91m 8s) Loss: 0.2780(0.1842) Grad: 4.6539  LR: 0.00000974  \n",
    "Epoch: [2][13000/78269] Elapsed 17m 52s (remain 89m 46s) Loss: 0.3391(0.1842) Grad: 6.2706  LR: 0.00000973  \n",
    "Epoch: [2][14000/78269] Elapsed 19m 14s (remain 88m 20s) Loss: 0.1622(0.1843) Grad: 4.7731  LR: 0.00000972  \n",
    "Epoch: [2][15000/78269] Elapsed 20m 37s (remain 86m 58s) Loss: 0.2451(0.1838) Grad: 2.6115  LR: 0.00000971  \n",
    "Epoch: [2][16000/78269] Elapsed 21m 59s (remain 85m 34s) Loss: 0.1700(0.1837) Grad: 3.1568  LR: 0.00000969  \n",
    "Epoch: [2][17000/78269] Elapsed 23m 21s (remain 84m 11s) Loss: 0.2333(0.1836) Grad: 2.5243  LR: 0.00000968  \n",
    "Epoch: [2][18000/78269] Elapsed 24m 43s (remain 82m 47s) Loss: 0.3095(0.1834) Grad: 14.3975  LR: 0.00000967  \n",
    "Epoch: [2][19000/78269] Elapsed 26m 5s (remain 81m 22s) Loss: 0.0790(0.1835) Grad: 2.0043  LR: 0.00000965  \n",
    "Epoch: [2][20000/78269] Elapsed 27m 27s (remain 79m 59s) Loss: 0.0739(0.1835) Grad: 3.6580  LR: 0.00000964  \n",
    "Epoch: [2][21000/78269] Elapsed 28m 50s (remain 78m 38s) Loss: 0.1280(0.1836) Grad: 3.3149  LR: 0.00000963  \n",
    "Epoch: [2][22000/78269] Elapsed 30m 13s (remain 77m 18s) Loss: 0.0497(0.1835) Grad: 5.3077  LR: 0.00000961  \n",
    "Epoch: [2][23000/78269] Elapsed 31m 36s (remain 75m 57s) Loss: 0.1505(0.1834) Grad: 5.2129  LR: 0.00000960  \n",
    "Epoch: [2][24000/78269] Elapsed 32m 59s (remain 74m 35s) Loss: 0.1382(0.1833) Grad: 2.2067  LR: 0.00000958  \n",
    "Epoch: [2][25000/78269] Elapsed 34m 21s (remain 73m 12s) Loss: 0.0783(0.1833) Grad: 1.7962  LR: 0.00000957  \n",
    "Epoch: [2][26000/78269] Elapsed 35m 43s (remain 71m 49s) Loss: 0.1580(0.1832) Grad: 2.3131  LR: 0.00000955  \n",
    "Epoch: [2][27000/78269] Elapsed 37m 6s (remain 70m 26s) Loss: 0.3288(0.1829) Grad: 2.7680  LR: 0.00000954  \n",
    "Epoch: [2][28000/78269] Elapsed 38m 28s (remain 69m 3s) Loss: 0.2772(0.1827) Grad: 5.3784  LR: 0.00000952  \n",
    "Epoch: [2][29000/78269] Elapsed 39m 50s (remain 67m 40s) Loss: 0.3262(0.1826) Grad: 18.1003  LR: 0.00000951  \n",
    "Epoch: [2][30000/78269] Elapsed 41m 12s (remain 66m 17s) Loss: 0.4018(0.1825) Grad: 3.4773  LR: 0.00000949  \n",
    "Epoch: [2][31000/78269] Elapsed 42m 34s (remain 64m 54s) Loss: 0.0829(0.1824) Grad: 2.0196  LR: 0.00000947  \n",
    "Epoch: [2][32000/78269] Elapsed 43m 56s (remain 63m 31s) Loss: 0.0404(0.1822) Grad: 0.9983  LR: 0.00000946  \n",
    "Epoch: [2][33000/78269] Elapsed 45m 18s (remain 62m 8s) Loss: 0.5250(0.1820) Grad: 7.0574  LR: 0.00000944  \n",
    "Epoch: [2][34000/78269] Elapsed 46m 39s (remain 60m 45s) Loss: 0.3483(0.1819) Grad: 5.2442  LR: 0.00000942  \n",
    "Epoch: [2][35000/78269] Elapsed 48m 1s (remain 59m 22s) Loss: 0.3625(0.1817) Grad: 4.1163  LR: 0.00000940  \n",
    "Epoch: [2][36000/78269] Elapsed 49m 23s (remain 57m 59s) Loss: 0.0594(0.1814) Grad: 1.6955  LR: 0.00000939  \n",
    "Epoch: [2][37000/78269] Elapsed 50m 46s (remain 56m 37s) Loss: 0.0263(0.1813) Grad: 0.6898  LR: 0.00000937  \n",
    "Epoch: [2][38000/78269] Elapsed 52m 7s (remain 55m 14s) Loss: 0.0213(0.1813) Grad: 0.4150  LR: 0.00000935  \n",
    "Epoch: [2][39000/78269] Elapsed 53m 30s (remain 53m 52s) Loss: 0.1078(0.1811) Grad: 2.1407  LR: 0.00000933  \n",
    "Epoch: [2][40000/78269] Elapsed 54m 52s (remain 52m 29s) Loss: 0.4046(0.1811) Grad: 4.7918  LR: 0.00000931  \n",
    "Epoch: [2][41000/78269] Elapsed 56m 14s (remain 51m 7s) Loss: 0.1465(0.1809) Grad: 2.5341  LR: 0.00000930  \n",
    "Epoch: [2][42000/78269] Elapsed 57m 36s (remain 49m 44s) Loss: 0.2297(0.1807) Grad: 4.7288  LR: 0.00000928  \n",
    "Epoch: [2][43000/78269] Elapsed 58m 58s (remain 48m 21s) Loss: 0.0546(0.1805) Grad: 2.0774  LR: 0.00000926  \n",
    "Epoch: [2][44000/78269] Elapsed 60m 20s (remain 46m 59s) Loss: 0.1377(0.1804) Grad: 1.9747  LR: 0.00000924  \n",
    "Epoch: [2][45000/78269] Elapsed 61m 41s (remain 45m 36s) Loss: 0.3536(0.1802) Grad: 5.7389  LR: 0.00000922  \n",
    "Epoch: [2][46000/78269] Elapsed 63m 3s (remain 44m 14s) Loss: 0.0540(0.1800) Grad: 1.6381  LR: 0.00000920  \n",
    "Epoch: [2][47000/78269] Elapsed 64m 26s (remain 42m 51s) Loss: 0.1714(0.1800) Grad: 2.0969  LR: 0.00000918  \n",
    "Epoch: [2][48000/78269] Elapsed 65m 48s (remain 41m 29s) Loss: 0.4451(0.1798) Grad: 4.2398  LR: 0.00000916  \n",
    "Epoch: [2][49000/78269] Elapsed 67m 10s (remain 40m 7s) Loss: 0.0345(0.1797) Grad: 1.3840  LR: 0.00000914  \n",
    "Epoch: [2][50000/78269] Elapsed 68m 33s (remain 38m 45s) Loss: 0.1123(0.1795) Grad: 2.7178  LR: 0.00000911  \n",
    "Epoch: [2][51000/78269] Elapsed 69m 55s (remain 37m 23s) Loss: 0.3410(0.1793) Grad: 9.0874  LR: 0.00000909  \n",
    "Epoch: [2][52000/78269] Elapsed 71m 18s (remain 36m 1s) Loss: 0.1586(0.1791) Grad: 5.6084  LR: 0.00000907  \n",
    "Epoch: [2][53000/78269] Elapsed 72m 40s (remain 34m 38s) Loss: 0.1436(0.1790) Grad: 3.2307  LR: 0.00000905  \n",
    "Epoch: [2][54000/78269] Elapsed 74m 2s (remain 33m 16s) Loss: 0.2368(0.1789) Grad: 3.4278  LR: 0.00000903  \n",
    "Epoch: [2][55000/78269] Elapsed 75m 24s (remain 31m 54s) Loss: 0.2059(0.1787) Grad: 2.6555  LR: 0.00000901  \n",
    "Epoch: [2][56000/78269] Elapsed 76m 46s (remain 30m 31s) Loss: 0.1267(0.1786) Grad: 3.6763  LR: 0.00000898  \n",
    "Epoch: [2][57000/78269] Elapsed 78m 7s (remain 29m 9s) Loss: 0.1438(0.1785) Grad: 1.0754  LR: 0.00000896  \n",
    "Epoch: [2][58000/78269] Elapsed 79m 30s (remain 27m 46s) Loss: 0.0725(0.1783) Grad: 4.5157  LR: 0.00000894  \n",
    "Epoch: [2][59000/78269] Elapsed 80m 52s (remain 26m 24s) Loss: 0.0162(0.1782) Grad: 0.4782  LR: 0.00000892  \n",
    "Epoch: [2][60000/78269] Elapsed 82m 13s (remain 25m 2s) Loss: 0.0462(0.1780) Grad: 3.1942  LR: 0.00000889  \n",
    "Epoch: [2][61000/78269] Elapsed 83m 35s (remain 23m 39s) Loss: 0.1121(0.1779) Grad: 1.9557  LR: 0.00000887  \n",
    "Epoch: [2][62000/78269] Elapsed 84m 58s (remain 22m 17s) Loss: 0.1868(0.1778) Grad: 2.5406  LR: 0.00000884  \n",
    "Epoch: [2][63000/78269] Elapsed 86m 21s (remain 20m 55s) Loss: 0.1567(0.1777) Grad: 3.8759  LR: 0.00000882  \n",
    "Epoch: [2][64000/78269] Elapsed 87m 43s (remain 19m 33s) Loss: 0.0542(0.1776) Grad: 1.4913  LR: 0.00000880  \n",
    "Epoch: [2][65000/78269] Elapsed 89m 5s (remain 18m 11s) Loss: 0.2154(0.1774) Grad: 3.4836  LR: 0.00000877  \n",
    "Epoch: [2][66000/78269] Elapsed 90m 28s (remain 16m 48s) Loss: 0.2375(0.1773) Grad: 4.1597  LR: 0.00000875  \n",
    "Epoch: [2][67000/78269] Elapsed 91m 50s (remain 15m 26s) Loss: 0.6167(0.1772) Grad: 6.2454  LR: 0.00000872  \n",
    "Epoch: [2][68000/78269] Elapsed 93m 12s (remain 14m 4s) Loss: 0.1605(0.1770) Grad: 9.4820  LR: 0.00000870  \n",
    "Epoch: [2][69000/78269] Elapsed 94m 34s (remain 12m 42s) Loss: 0.2384(0.1768) Grad: 3.9497  LR: 0.00000867  \n",
    "Epoch: [2][70000/78269] Elapsed 95m 57s (remain 11m 20s) Loss: 0.2052(0.1767) Grad: 3.0865  LR: 0.00000865  \n",
    "Epoch: [2][71000/78269] Elapsed 97m 19s (remain 9m 57s) Loss: 0.3914(0.1765) Grad: 4.6967  LR: 0.00000862  \n",
    "Epoch: [2][72000/78269] Elapsed 98m 41s (remain 8m 35s) Loss: 0.1057(0.1764) Grad: 4.7622  LR: 0.00000860  \n",
    "Epoch: [2][73000/78269] Elapsed 100m 3s (remain 7m 13s) Loss: 0.4083(0.1763) Grad: 4.2763  LR: 0.00000857  \n",
    "Epoch: [2][74000/78269] Elapsed 101m 25s (remain 5m 50s) Loss: 0.1306(0.1762) Grad: 4.3913  LR: 0.00000854  \n",
    "Epoch: [2][75000/78269] Elapsed 102m 47s (remain 4m 28s) Loss: 0.0482(0.1761) Grad: 0.7264  LR: 0.00000852  \n",
    "Epoch: [2][76000/78269] Elapsed 104m 9s (remain 3m 6s) Loss: 0.3148(0.1759) Grad: 3.7670  LR: 0.00000849  \n",
    "Epoch: [2][77000/78269] Elapsed 105m 31s (remain 1m 44s) Loss: 0.0344(0.1758) Grad: 0.7638  LR: 0.00000847  \n",
    "Epoch: [2][78000/78269] Elapsed 106m 53s (remain 0m 22s) Loss: 0.1278(0.1757) Grad: 2.5362  LR: 0.00000844  \n",
    "Epoch: [2][78268/78269] Elapsed 107m 15s (remain 0m 0s) Loss: 0.1044(0.1756) Grad: 5.1211  LR: 0.00000843  \n",
    "EVAL: [0/19225] Elapsed 0m 1s (remain 341m 43s) Loss: 0.0150(0.0150) \n",
    "EVAL: [1000/19225] Elapsed 0m 13s (remain 4m 1s) Loss: 0.6518(0.1646) \n",
    "EVAL: [2000/19225] Elapsed 0m 24s (remain 3m 33s) Loss: 0.0003(0.1452) \n",
    "EVAL: [3000/19225] Elapsed 0m 35s (remain 3m 9s) Loss: 0.0004(0.1195) \n",
    "EVAL: [4000/19225] Elapsed 0m 47s (remain 3m 2s) Loss: 0.1219(0.1145) \n",
    "EVAL: [5000/19225] Elapsed 1m 1s (remain 2m 54s) Loss: 0.0060(0.1232) \n",
    "EVAL: [6000/19225] Elapsed 1m 15s (remain 2m 46s) Loss: 0.1560(0.1291) \n",
    "EVAL: [7000/19225] Elapsed 1m 29s (remain 2m 36s) Loss: 0.1396(0.1345) \n",
    "EVAL: [8000/19225] Elapsed 1m 44s (remain 2m 26s) Loss: 0.0964(0.1391) \n",
    "EVAL: [9000/19225] Elapsed 1m 58s (remain 2m 15s) Loss: 0.0593(0.1414) \n",
    "EVAL: [10000/19225] Elapsed 2m 14s (remain 2m 3s) Loss: 0.0829(0.1440) \n",
    "EVAL: [11000/19225] Elapsed 2m 29s (remain 1m 51s) Loss: 0.2283(0.1452) \n",
    "EVAL: [12000/19225] Elapsed 2m 44s (remain 1m 39s) Loss: 0.0038(0.1478) \n",
    "EVAL: [13000/19225] Elapsed 3m 0s (remain 1m 26s) Loss: 0.0322(0.1489) \n",
    "EVAL: [14000/19225] Elapsed 3m 17s (remain 1m 13s) Loss: 0.0065(0.1491) \n",
    "EVAL: [15000/19225] Elapsed 3m 33s (remain 1m 0s) Loss: 0.0434(0.1509) \n",
    "EVAL: [16000/19225] Elapsed 3m 51s (remain 0m 46s) Loss: 0.4001(0.1515) \n",
    "EVAL: [17000/19225] Elapsed 4m 9s (remain 0m 32s) Loss: 0.1224(0.1517) \n",
    "EVAL: [18000/19225] Elapsed 4m 28s (remain 0m 18s) Loss: 0.1166(0.1518) \n",
    "EVAL: [19000/19225] Elapsed 4m 48s (remain 0m 3s) Loss: 0.6019(0.1513) \n",
    "EVAL: [19224/19225] Elapsed 4m 54s (remain 0m 0s) Loss: 0.1739(0.1509) \n",
    "Epoch 2 - avg_train_loss: 0.1756  avg_val_loss: 0.1509  time: 6960s\n",
    "Epoch 2 - Score: 0.4517 - Threshold: 0.05200\n",
    "Epoch 2 - Save Best Score: 0.4517 Model\n",
    "Epoch: [3][0/78269] Elapsed 0m 1s (remain 1625m 3s) Loss: 0.1774(0.1774) Grad: 2.5465  LR: 0.00000843  \n",
    "Epoch: [3][1000/78269] Elapsed 1m 23s (remain 107m 20s) Loss: 0.1112(0.1407) Grad: 5.9497  LR: 0.00000840  \n",
    "Epoch: [3][2000/78269] Elapsed 2m 45s (remain 105m 20s) Loss: 0.0589(0.1405) Grad: 3.0625  LR: 0.00000838  \n",
    "Epoch: [3][3000/78269] Elapsed 4m 8s (remain 103m 43s) Loss: 0.1126(0.1423) Grad: 3.2302  LR: 0.00000835  \n",
    "Epoch: [3][4000/78269] Elapsed 5m 30s (remain 102m 11s) Loss: 0.0997(0.1417) Grad: 3.2495  LR: 0.00000832  \n",
    "Epoch: [3][5000/78269] Elapsed 6m 52s (remain 100m 49s) Loss: 0.0168(0.1426) Grad: 1.0159  LR: 0.00000829  \n",
    "Epoch: [3][6000/78269] Elapsed 8m 15s (remain 99m 24s) Loss: 0.1213(0.1430) Grad: 5.8144  LR: 0.00000827  \n",
    "Epoch: [3][7000/78269] Elapsed 9m 37s (remain 97m 59s) Loss: 0.4095(0.1433) Grad: 12.1648  LR: 0.00000824  \n",
    "Epoch: [3][8000/78269] Elapsed 11m 0s (remain 96m 36s) Loss: 0.0633(0.1429) Grad: 5.3389  LR: 0.00000821  \n",
    "Epoch: [3][9000/78269] Elapsed 12m 22s (remain 95m 12s) Loss: 0.0913(0.1430) Grad: 4.2671  LR: 0.00000818  \n",
    "Epoch: [3][10000/78269] Elapsed 13m 44s (remain 93m 48s) Loss: 0.0412(0.1427) Grad: 1.8418  LR: 0.00000815  \n",
    "Epoch: [3][11000/78269] Elapsed 15m 6s (remain 92m 22s) Loss: 0.1077(0.1432) Grad: 4.1621  LR: 0.00000812  \n",
    "Epoch: [3][12000/78269] Elapsed 16m 28s (remain 90m 56s) Loss: 0.0904(0.1432) Grad: 10.8576  LR: 0.00000809  \n",
    "Epoch: [3][13000/78269] Elapsed 17m 50s (remain 89m 32s) Loss: 0.0078(0.1436) Grad: 0.3570  LR: 0.00000806  \n",
    "Epoch: [3][14000/78269] Elapsed 19m 12s (remain 88m 8s) Loss: 0.1793(0.1437) Grad: 2.4320  LR: 0.00000803  \n",
    "Epoch: [3][15000/78269] Elapsed 20m 34s (remain 86m 45s) Loss: 0.1398(0.1435) Grad: 4.8244  LR: 0.00000801  \n",
    "Epoch: [3][16000/78269] Elapsed 21m 56s (remain 85m 23s) Loss: 0.3166(0.1437) Grad: 4.7457  LR: 0.00000798  \n",
    "Epoch: [3][17000/78269] Elapsed 23m 18s (remain 84m 0s) Loss: 0.0182(0.1437) Grad: 0.7381  LR: 0.00000795  \n",
    "Epoch: [3][18000/78269] Elapsed 24m 41s (remain 82m 39s) Loss: 0.1103(0.1437) Grad: 1.4073  LR: 0.00000792  \n",
    "Epoch: [3][19000/78269] Elapsed 26m 3s (remain 81m 17s) Loss: 0.3994(0.1438) Grad: 7.5770  LR: 0.00000789  \n",
    "Epoch: [3][20000/78269] Elapsed 27m 26s (remain 79m 56s) Loss: 0.0807(0.1438) Grad: 4.4626  LR: 0.00000785  \n",
    "Epoch: [3][21000/78269] Elapsed 28m 48s (remain 78m 34s) Loss: 0.0739(0.1437) Grad: 1.9062  LR: 0.00000782  \n",
    "Epoch: [3][22000/78269] Elapsed 30m 11s (remain 77m 12s) Loss: 0.2840(0.1437) Grad: 7.1349  LR: 0.00000779  \n",
    "Epoch: [3][23000/78269] Elapsed 31m 33s (remain 75m 49s) Loss: 0.0153(0.1438) Grad: 0.4523  LR: 0.00000776  \n",
    "Epoch: [3][24000/78269] Elapsed 32m 55s (remain 74m 27s) Loss: 0.1814(0.1436) Grad: 5.3543  LR: 0.00000773  \n",
    "Epoch: [3][25000/78269] Elapsed 34m 18s (remain 73m 5s) Loss: 0.2057(0.1436) Grad: 8.1152  LR: 0.00000770  \n",
    "Epoch: [3][26000/78269] Elapsed 35m 40s (remain 71m 43s) Loss: 0.0827(0.1436) Grad: 2.0687  LR: 0.00000767  \n",
    "Epoch: [3][27000/78269] Elapsed 37m 2s (remain 70m 20s) Loss: 0.1990(0.1435) Grad: 3.6222  LR: 0.00000764  \n",
    "Epoch: [3][28000/78269] Elapsed 38m 25s (remain 68m 58s) Loss: 0.1271(0.1435) Grad: 4.2146  LR: 0.00000761  \n",
    "Epoch: [3][29000/78269] Elapsed 39m 47s (remain 67m 35s) Loss: 0.1412(0.1435) Grad: 6.3729  LR: 0.00000757  \n",
    "Epoch: [3][30000/78269] Elapsed 41m 8s (remain 66m 11s) Loss: 0.0185(0.1436) Grad: 0.9966  LR: 0.00000754  \n",
    "Epoch: [3][31000/78269] Elapsed 42m 30s (remain 64m 48s) Loss: 0.0469(0.1435) Grad: 1.8109  LR: 0.00000751  \n",
    "Epoch: [3][32000/78269] Elapsed 43m 52s (remain 63m 26s) Loss: 0.0462(0.1436) Grad: 6.2127  LR: 0.00000748  \n",
    "Epoch: [3][33000/78269] Elapsed 45m 14s (remain 62m 3s) Loss: 0.0377(0.1436) Grad: 1.8423  LR: 0.00000745  \n",
    "Epoch: [3][34000/78269] Elapsed 46m 36s (remain 60m 40s) Loss: 0.0306(0.1436) Grad: 2.2957  LR: 0.00000741  \n",
    "Epoch: [3][35000/78269] Elapsed 47m 58s (remain 59m 18s) Loss: 0.2056(0.1435) Grad: 2.9757  LR: 0.00000738  \n",
    "Epoch: [3][36000/78269] Elapsed 49m 19s (remain 57m 55s) Loss: 0.0800(0.1435) Grad: 3.9836  LR: 0.00000735  \n",
    "Epoch: [3][37000/78269] Elapsed 50m 41s (remain 56m 32s) Loss: 0.1108(0.1435) Grad: 4.2843  LR: 0.00000731  \n",
    "Epoch: [3][38000/78269] Elapsed 52m 4s (remain 55m 10s) Loss: 0.3289(0.1435) Grad: 6.6986  LR: 0.00000728  \n",
    "Epoch: [3][39000/78269] Elapsed 53m 26s (remain 53m 48s) Loss: 0.1239(0.1434) Grad: 2.7497  LR: 0.00000725  \n",
    "Epoch: [3][40000/78269] Elapsed 54m 48s (remain 52m 25s) Loss: 0.1813(0.1435) Grad: 7.8206  LR: 0.00000722  \n",
    "Epoch: [3][41000/78269] Elapsed 56m 10s (remain 51m 3s) Loss: 0.1504(0.1434) Grad: 5.5100  LR: 0.00000718  \n",
    "Epoch: [3][42000/78269] Elapsed 57m 32s (remain 49m 41s) Loss: 0.1242(0.1434) Grad: 3.9862  LR: 0.00000715  \n",
    "Epoch: [3][43000/78269] Elapsed 58m 54s (remain 48m 19s) Loss: 0.1607(0.1433) Grad: 2.1083  LR: 0.00000711  \n",
    "Epoch: [3][44000/78269] Elapsed 60m 16s (remain 46m 56s) Loss: 0.1883(0.1433) Grad: 5.8072  LR: 0.00000708  \n",
    "Epoch: [3][45000/78269] Elapsed 61m 39s (remain 45m 34s) Loss: 0.1410(0.1433) Grad: 3.5307  LR: 0.00000705  \n",
    "Epoch: [3][46000/78269] Elapsed 63m 1s (remain 44m 12s) Loss: 0.1314(0.1433) Grad: 6.4447  LR: 0.00000701  \n",
    "Epoch: [3][47000/78269] Elapsed 64m 23s (remain 42m 49s) Loss: 0.1883(0.1433) Grad: 5.3653  LR: 0.00000698  \n",
    "Epoch: [3][48000/78269] Elapsed 65m 44s (remain 41m 27s) Loss: 0.0491(0.1433) Grad: 2.0615  LR: 0.00000694  \n",
    "Epoch: [3][49000/78269] Elapsed 67m 6s (remain 40m 5s) Loss: 0.2297(0.1432) Grad: 6.6034  LR: 0.00000691  \n",
    "Epoch: [3][50000/78269] Elapsed 68m 28s (remain 38m 42s) Loss: 0.0887(0.1431) Grad: 2.1417  LR: 0.00000688  \n",
    "Epoch: [3][51000/78269] Elapsed 69m 50s (remain 37m 20s) Loss: 0.1391(0.1430) Grad: 2.1794  LR: 0.00000684  \n",
    "Epoch: [3][52000/78269] Elapsed 71m 13s (remain 35m 58s) Loss: 0.2919(0.1429) Grad: 4.4615  LR: 0.00000681  \n",
    "Epoch: [3][53000/78269] Elapsed 72m 35s (remain 34m 36s) Loss: 0.0503(0.1429) Grad: 1.3734  LR: 0.00000677  \n",
    "Epoch: [3][54000/78269] Elapsed 73m 58s (remain 33m 14s) Loss: 0.2209(0.1429) Grad: 3.6470  LR: 0.00000674  \n",
    "Epoch: [3][55000/78269] Elapsed 75m 20s (remain 31m 52s) Loss: 0.1589(0.1429) Grad: 8.8402  LR: 0.00000670  \n",
    "Epoch: [3][56000/78269] Elapsed 76m 42s (remain 30m 30s) Loss: 0.0953(0.1429) Grad: 2.6187  LR: 0.00000667  \n",
    "Epoch: [3][57000/78269] Elapsed 78m 4s (remain 29m 8s) Loss: 0.0307(0.1429) Grad: 1.9651  LR: 0.00000663  \n",
    "Epoch: [3][58000/78269] Elapsed 79m 27s (remain 27m 46s) Loss: 0.1187(0.1428) Grad: 5.2935  LR: 0.00000660  \n",
    "Epoch: [3][59000/78269] Elapsed 80m 49s (remain 26m 23s) Loss: 0.1175(0.1428) Grad: 1.9497  LR: 0.00000656  \n",
    "Epoch: [3][60000/78269] Elapsed 82m 12s (remain 25m 1s) Loss: 0.0338(0.1427) Grad: 0.7761  LR: 0.00000653  \n",
    "Epoch: [3][61000/78269] Elapsed 83m 35s (remain 23m 39s) Loss: 0.1593(0.1427) Grad: 6.4811  LR: 0.00000649  \n",
    "Epoch: [3][62000/78269] Elapsed 84m 56s (remain 22m 17s) Loss: 0.1608(0.1427) Grad: 2.0383  LR: 0.00000646  \n",
    "Epoch: [3][63000/78269] Elapsed 86m 19s (remain 20m 55s) Loss: 0.1540(0.1426) Grad: 3.7394  LR: 0.00000642  \n",
    "Epoch: [3][64000/78269] Elapsed 87m 41s (remain 19m 32s) Loss: 0.3065(0.1426) Grad: 4.2110  LR: 0.00000638  \n",
    "Epoch: [3][65000/78269] Elapsed 89m 3s (remain 18m 10s) Loss: 0.2382(0.1425) Grad: 4.8881  LR: 0.00000635  \n",
    "Epoch: [3][66000/78269] Elapsed 90m 25s (remain 16m 48s) Loss: 0.3179(0.1425) Grad: 6.2530  LR: 0.00000631  \n",
    "Epoch: [3][67000/78269] Elapsed 91m 47s (remain 15m 26s) Loss: 0.4612(0.1425) Grad: 15.5437  LR: 0.00000628  \n",
    "Epoch: [3][68000/78269] Elapsed 93m 9s (remain 14m 3s) Loss: 0.0113(0.1425) Grad: 0.3660  LR: 0.00000624  \n",
    "Epoch: [3][69000/78269] Elapsed 94m 31s (remain 12m 41s) Loss: 0.0064(0.1424) Grad: 0.3354  LR: 0.00000621  \n",
    "Epoch: [3][70000/78269] Elapsed 95m 53s (remain 11m 19s) Loss: 0.0434(0.1423) Grad: 1.6838  LR: 0.00000617  \n",
    "Epoch: [3][71000/78269] Elapsed 97m 16s (remain 9m 57s) Loss: 0.0666(0.1422) Grad: 2.0335  LR: 0.00000613  \n",
    "Epoch: [3][72000/78269] Elapsed 98m 38s (remain 8m 35s) Loss: 0.0803(0.1422) Grad: 2.4764  LR: 0.00000610  \n",
    "Epoch: [3][73000/78269] Elapsed 100m 1s (remain 7m 13s) Loss: 0.1608(0.1422) Grad: 5.9266  LR: 0.00000606  \n",
    "Epoch: [3][74000/78269] Elapsed 101m 25s (remain 5m 50s) Loss: 0.0131(0.1421) Grad: 0.8371  LR: 0.00000602  \n",
    "Epoch: [3][75000/78269] Elapsed 102m 49s (remain 4m 28s) Loss: 0.1045(0.1420) Grad: 6.6890  LR: 0.00000599  \n",
    "Epoch: [3][76000/78269] Elapsed 104m 13s (remain 3m 6s) Loss: 0.0382(0.1420) Grad: 1.5747  LR: 0.00000595  \n",
    "Epoch: [3][77000/78269] Elapsed 105m 36s (remain 1m 44s) Loss: 0.1493(0.1419) Grad: 9.0514  LR: 0.00000591  \n",
    "Epoch: [3][78000/78269] Elapsed 107m 0s (remain 0m 22s) Loss: 0.1486(0.1419) Grad: 3.7138  LR: 0.00000588  \n",
    "Epoch: [3][78268/78269] Elapsed 107m 22s (remain 0m 0s) Loss: 0.0921(0.1419) Grad: 7.9862  LR: 0.00000587  \n",
    "EVAL: [0/19225] Elapsed 0m 1s (remain 339m 58s) Loss: 0.0150(0.0150) \n",
    "EVAL: [1000/19225] Elapsed 0m 13s (remain 4m 4s) Loss: 0.5902(0.1780) \n",
    "EVAL: [2000/19225] Elapsed 0m 25s (remain 3m 36s) Loss: 0.0001(0.1543) \n",
    "EVAL: [3000/19225] Elapsed 0m 35s (remain 3m 12s) Loss: 0.0002(0.1248) \n",
    "EVAL: [4000/19225] Elapsed 0m 48s (remain 3m 5s) Loss: 0.0901(0.1186) \n",
    "EVAL: [5000/19225] Elapsed 1m 2s (remain 2m 58s) Loss: 0.0130(0.1290) \n",
    "EVAL: [6000/19225] Elapsed 1m 16s (remain 2m 49s) Loss: 0.2311(0.1349) \n",
    "EVAL: [7000/19225] Elapsed 1m 31s (remain 2m 39s) Loss: 0.0826(0.1389) \n",
    "EVAL: [8000/19225] Elapsed 1m 45s (remain 2m 28s) Loss: 0.0312(0.1433) \n",
    "EVAL: [9000/19225] Elapsed 2m 0s (remain 2m 17s) Loss: 0.0056(0.1481) \n",
    "EVAL: [10000/19225] Elapsed 2m 16s (remain 2m 5s) Loss: 0.0348(0.1504) \n",
    "EVAL: [11000/19225] Elapsed 2m 31s (remain 1m 53s) Loss: 0.1647(0.1510) \n",
    "EVAL: [12000/19225] Elapsed 2m 47s (remain 1m 41s) Loss: 0.0009(0.1534) \n",
    "EVAL: [13000/19225] Elapsed 3m 4s (remain 1m 28s) Loss: 0.0093(0.1540) \n",
    "EVAL: [14000/19225] Elapsed 3m 20s (remain 1m 14s) Loss: 0.0020(0.1535) \n",
    "EVAL: [15000/19225] Elapsed 3m 37s (remain 1m 1s) Loss: 0.0103(0.1548) \n",
    "EVAL: [16000/19225] Elapsed 3m 54s (remain 0m 47s) Loss: 0.4392(0.1554) \n",
    "EVAL: [17000/19225] Elapsed 4m 13s (remain 0m 33s) Loss: 0.0658(0.1553) \n",
    "EVAL: [18000/19225] Elapsed 4m 32s (remain 0m 18s) Loss: 0.0358(0.1549) \n",
    "EVAL: [19000/19225] Elapsed 4m 53s (remain 0m 3s) Loss: 0.6194(0.1542) \n",
    "EVAL: [19224/19225] Elapsed 4m 58s (remain 0m 0s) Loss: 0.1502(0.1540) \n",
    "Epoch 3 - avg_train_loss: 0.1419  avg_val_loss: 0.1540  time: 6990s\n",
    "Epoch 3 - Score: 0.4759 - Threshold: 0.05200\n",
    "Epoch 3 - Save Best Score: 0.4759 Model\n",
    "Epoch: [4][0/78269] Elapsed 0m 1s (remain 1672m 38s) Loss: 0.0836(0.0836) Grad: 3.6663  LR: 0.00000587  \n",
    "Epoch: [4][1000/78269] Elapsed 1m 24s (remain 109m 12s) Loss: 0.3014(0.1149) Grad: 7.9873  LR: 0.00000583  \n",
    "Epoch: [4][2000/78269] Elapsed 2m 48s (remain 107m 10s) Loss: 0.1561(0.1137) Grad: 12.8383  LR: 0.00000579  \n",
    "Epoch: [4][3000/78269] Elapsed 4m 12s (remain 105m 28s) Loss: 0.1692(0.1139) Grad: 4.0225  LR: 0.00000576  \n",
    "Epoch: [4][4000/78269] Elapsed 5m 35s (remain 103m 54s) Loss: 0.0170(0.1138) Grad: 0.8162  LR: 0.00000572  \n",
    "Epoch: [4][5000/78269] Elapsed 6m 59s (remain 102m 19s) Loss: 0.1645(0.1147) Grad: 4.8377  LR: 0.00000568  \n",
    "Epoch: [4][6000/78269] Elapsed 8m 22s (remain 100m 50s) Loss: 0.1896(0.1148) Grad: 3.9425  LR: 0.00000565  \n",
    "Epoch: [4][7000/78269] Elapsed 9m 45s (remain 99m 21s) Loss: 0.3016(0.1146) Grad: 10.8695  LR: 0.00000561  \n",
    "Epoch: [4][8000/78269] Elapsed 11m 9s (remain 98m 3s) Loss: 0.0034(0.1148) Grad: 0.2040  LR: 0.00000557  \n",
    "Epoch: [4][9000/78269] Elapsed 12m 34s (remain 96m 43s) Loss: 0.0238(0.1150) Grad: 1.5024  LR: 0.00000554  \n",
    "Epoch: [4][10000/78269] Elapsed 13m 57s (remain 95m 15s) Loss: 0.2139(0.1156) Grad: 7.1529  LR: 0.00000550  \n",
    "Epoch: [4][11000/78269] Elapsed 15m 20s (remain 93m 49s) Loss: 0.1891(0.1154) Grad: 11.3115  LR: 0.00000546  \n",
    "Epoch: [4][12000/78269] Elapsed 16m 43s (remain 92m 22s) Loss: 0.2439(0.1156) Grad: 8.2289  LR: 0.00000543  \n",
    "Epoch: [4][13000/78269] Elapsed 18m 7s (remain 91m 0s) Loss: 0.0541(0.1157) Grad: 1.8750  LR: 0.00000539  \n",
    "Epoch: [4][14000/78269] Elapsed 19m 30s (remain 89m 31s) Loss: 0.1161(0.1157) Grad: 4.3886  LR: 0.00000535  \n",
    "Epoch: [4][15000/78269] Elapsed 20m 52s (remain 88m 1s) Loss: 0.0390(0.1155) Grad: 3.1928  LR: 0.00000532  \n",
    "Epoch: [4][16000/78269] Elapsed 22m 14s (remain 86m 32s) Loss: 0.0378(0.1155) Grad: 2.6146  LR: 0.00000528  \n",
    "Epoch: [4][17000/78269] Elapsed 23m 36s (remain 85m 3s) Loss: 0.1264(0.1153) Grad: 9.4748  LR: 0.00000524  \n",
    "Epoch: [4][18000/78269] Elapsed 24m 57s (remain 83m 35s) Loss: 0.0534(0.1153) Grad: 2.4955  LR: 0.00000520  \n",
    "Epoch: [4][19000/78269] Elapsed 26m 21s (remain 82m 11s) Loss: 0.0089(0.1154) Grad: 0.6422  LR: 0.00000517  \n",
    "Epoch: [4][20000/78269] Elapsed 27m 43s (remain 80m 45s) Loss: 0.1634(0.1156) Grad: 5.2496  LR: 0.00000513  \n",
    "Epoch: [4][21000/78269] Elapsed 29m 5s (remain 79m 19s) Loss: 0.1241(0.1160) Grad: 2.0833  LR: 0.00000509  \n",
    "Epoch: [4][22000/78269] Elapsed 30m 27s (remain 77m 52s) Loss: 0.0983(0.1160) Grad: 4.1797  LR: 0.00000506  \n",
    "Epoch: [4][23000/78269] Elapsed 31m 48s (remain 76m 26s) Loss: 0.2882(0.1163) Grad: 7.0536  LR: 0.00000502  \n",
    "Epoch: [4][24000/78269] Elapsed 33m 10s (remain 75m 1s) Loss: 0.1114(0.1163) Grad: 9.1775  LR: 0.00000498  \n",
    "Epoch: [4][25000/78269] Elapsed 34m 32s (remain 73m 36s) Loss: 0.0625(0.1164) Grad: 5.5776  LR: 0.00000494  \n",
    "Epoch: [4][26000/78269] Elapsed 35m 54s (remain 72m 11s) Loss: 0.0213(0.1163) Grad: 3.0613  LR: 0.00000491  \n",
    "Epoch: [4][27000/78269] Elapsed 37m 18s (remain 70m 49s) Loss: 0.0242(0.1163) Grad: 1.1685  LR: 0.00000487  \n",
    "Epoch: [4][28000/78269] Elapsed 38m 40s (remain 69m 26s) Loss: 0.0455(0.1163) Grad: 1.3897  LR: 0.00000483  \n",
    "Epoch: [4][29000/78269] Elapsed 40m 3s (remain 68m 3s) Loss: 0.0386(0.1163) Grad: 1.1915  LR: 0.00000479  \n",
    "Epoch: [4][30000/78269] Elapsed 41m 25s (remain 66m 39s) Loss: 0.1321(0.1164) Grad: 4.3093  LR: 0.00000476  \n",
    "Epoch: [4][31000/78269] Elapsed 42m 47s (remain 65m 14s) Loss: 0.1538(0.1164) Grad: 9.0142  LR: 0.00000472  \n",
    "Epoch: [4][32000/78269] Elapsed 44m 9s (remain 63m 51s) Loss: 0.0631(0.1165) Grad: 3.5223  LR: 0.00000468  \n",
    "Epoch: [4][33000/78269] Elapsed 45m 31s (remain 62m 27s) Loss: 0.1877(0.1167) Grad: 9.4599  LR: 0.00000465  \n",
    "Epoch: [4][34000/78269] Elapsed 46m 54s (remain 61m 3s) Loss: 0.0270(0.1168) Grad: 1.7469  LR: 0.00000461  \n",
    "Epoch: [4][35000/78269] Elapsed 48m 16s (remain 59m 40s) Loss: 0.1180(0.1167) Grad: 3.1831  LR: 0.00000457  \n",
    "Epoch: [4][36000/78269] Elapsed 49m 38s (remain 58m 16s) Loss: 0.1168(0.1168) Grad: 16.0367  LR: 0.00000454  \n",
    "Epoch: [4][37000/78269] Elapsed 51m 1s (remain 56m 54s) Loss: 0.1324(0.1169) Grad: 5.3015  LR: 0.00000450  \n",
    "Epoch: [4][38000/78269] Elapsed 52m 23s (remain 55m 31s) Loss: 0.0721(0.1169) Grad: 16.5087  LR: 0.00000446  \n",
    "Epoch: [4][39000/78269] Elapsed 53m 45s (remain 54m 7s) Loss: 0.0794(0.1169) Grad: 2.8116  LR: 0.00000442  \n",
    "Epoch: [4][40000/78269] Elapsed 55m 8s (remain 52m 44s) Loss: 0.0866(0.1167) Grad: 2.2142  LR: 0.00000439  \n",
    "Epoch: [4][41000/78269] Elapsed 56m 30s (remain 51m 21s) Loss: 0.0160(0.1169) Grad: 1.3203  LR: 0.00000435  \n",
    "Epoch: [4][42000/78269] Elapsed 57m 51s (remain 49m 58s) Loss: 0.0069(0.1168) Grad: 1.0643  LR: 0.00000431  \n",
    "Epoch: [4][43000/78269] Elapsed 59m 14s (remain 48m 35s) Loss: 0.0871(0.1168) Grad: 3.2210  LR: 0.00000428  \n",
    "Epoch: [4][44000/78269] Elapsed 60m 36s (remain 47m 12s) Loss: 0.0576(0.1167) Grad: 1.3671  LR: 0.00000424  \n",
    "Epoch: [4][45000/78269] Elapsed 61m 58s (remain 45m 49s) Loss: 0.0236(0.1167) Grad: 1.7502  LR: 0.00000420  \n",
    "Epoch: [4][46000/78269] Elapsed 63m 21s (remain 44m 26s) Loss: 0.3283(0.1167) Grad: 15.1705  LR: 0.00000417  \n",
    "Epoch: [4][47000/78269] Elapsed 64m 44s (remain 43m 3s) Loss: 0.0154(0.1167) Grad: 1.3577  LR: 0.00000413  \n",
    "Epoch: [4][48000/78269] Elapsed 66m 6s (remain 41m 41s) Loss: 0.0192(0.1167) Grad: 1.2634  LR: 0.00000409  \n",
    "Epoch: [4][49000/78269] Elapsed 67m 28s (remain 40m 18s) Loss: 0.0289(0.1167) Grad: 1.2232  LR: 0.00000406  \n",
    "Epoch: [4][50000/78269] Elapsed 68m 50s (remain 38m 55s) Loss: 0.1753(0.1168) Grad: 2.5455  LR: 0.00000402  \n",
    "Epoch: [4][51000/78269] Elapsed 70m 13s (remain 37m 32s) Loss: 0.0011(0.1168) Grad: 0.0544  LR: 0.00000398  \n",
    "Epoch: [4][52000/78269] Elapsed 71m 36s (remain 36m 10s) Loss: 0.1583(0.1168) Grad: 5.1064  LR: 0.00000395  \n",
    "Epoch: [4][53000/78269] Elapsed 72m 58s (remain 34m 47s) Loss: 0.0352(0.1168) Grad: 1.2604  LR: 0.00000391  \n",
    "Epoch: [4][54000/78269] Elapsed 74m 20s (remain 33m 24s) Loss: 0.2317(0.1168) Grad: 10.0635  LR: 0.00000388  \n",
    "Epoch: [4][55000/78269] Elapsed 75m 42s (remain 32m 1s) Loss: 0.1634(0.1167) Grad: 7.1916  LR: 0.00000384  \n",
    "Epoch: [4][56000/78269] Elapsed 77m 5s (remain 30m 39s) Loss: 0.1642(0.1167) Grad: 9.6656  LR: 0.00000380  \n",
    "Epoch: [4][57000/78269] Elapsed 78m 26s (remain 29m 16s) Loss: 0.0357(0.1167) Grad: 4.3991  LR: 0.00000377  \n",
    "Epoch: [4][58000/78269] Elapsed 79m 49s (remain 27m 53s) Loss: 0.0175(0.1167) Grad: 0.9953  LR: 0.00000373  \n",
    "Epoch: [4][59000/78269] Elapsed 81m 11s (remain 26m 30s) Loss: 0.2502(0.1166) Grad: 6.3837  LR: 0.00000370  \n",
    "Epoch: [4][60000/78269] Elapsed 82m 33s (remain 25m 8s) Loss: 0.1418(0.1166) Grad: 6.7617  LR: 0.00000366  \n",
    "Epoch: [4][61000/78269] Elapsed 83m 55s (remain 23m 45s) Loss: 0.1070(0.1166) Grad: 9.0369  LR: 0.00000362  \n",
    "Epoch: [4][62000/78269] Elapsed 85m 17s (remain 22m 22s) Loss: 0.0500(0.1166) Grad: 3.1001  LR: 0.00000359  \n",
    "Epoch: [4][63000/78269] Elapsed 86m 39s (remain 21m 0s) Loss: 0.1183(0.1165) Grad: 17.1415  LR: 0.00000355  \n",
    "Epoch: [4][64000/78269] Elapsed 88m 1s (remain 19m 37s) Loss: 0.4003(0.1165) Grad: 9.2638  LR: 0.00000352  \n",
    "Epoch: [4][65000/78269] Elapsed 89m 23s (remain 18m 14s) Loss: 0.0507(0.1165) Grad: 3.3327  LR: 0.00000348  \n",
    "Epoch: [4][66000/78269] Elapsed 90m 45s (remain 16m 52s) Loss: 0.0968(0.1165) Grad: 12.0742  LR: 0.00000345  \n",
    "Epoch: [4][67000/78269] Elapsed 92m 7s (remain 15m 29s) Loss: 0.1324(0.1164) Grad: 11.8265  LR: 0.00000341  \n",
    "Epoch: [4][68000/78269] Elapsed 93m 29s (remain 14m 7s) Loss: 0.0856(0.1164) Grad: 9.9422  LR: 0.00000338  \n",
    "Epoch: [4][69000/78269] Elapsed 94m 53s (remain 12m 44s) Loss: 0.2066(0.1165) Grad: 3.3257  LR: 0.00000334  \n",
    "Epoch: [4][70000/78269] Elapsed 96m 15s (remain 11m 22s) Loss: 0.1440(0.1165) Grad: 5.9186  LR: 0.00000331  \n",
    "Epoch: [4][71000/78269] Elapsed 97m 37s (remain 9m 59s) Loss: 0.1999(0.1165) Grad: 9.5694  LR: 0.00000327  \n",
    "Epoch: [4][72000/78269] Elapsed 98m 59s (remain 8m 37s) Loss: 0.1547(0.1165) Grad: 4.5974  LR: 0.00000324  \n",
    "Epoch: [4][73000/78269] Elapsed 100m 22s (remain 7m 14s) Loss: 0.1118(0.1165) Grad: 2.4996  LR: 0.00000320  \n",
    "Epoch: [4][74000/78269] Elapsed 101m 44s (remain 5m 52s) Loss: 0.0766(0.1165) Grad: 1.8763  LR: 0.00000317  \n",
    "Epoch: [4][75000/78269] Elapsed 103m 6s (remain 4m 29s) Loss: 0.2030(0.1165) Grad: 8.3310  LR: 0.00000313  \n",
    "Epoch: [4][76000/78269] Elapsed 104m 28s (remain 3m 7s) Loss: 0.1206(0.1164) Grad: 7.0817  LR: 0.00000310  \n",
    "Epoch: [4][77000/78269] Elapsed 105m 50s (remain 1m 44s) Loss: 0.0878(0.1164) Grad: 7.2147  LR: 0.00000306  \n",
    "Epoch: [4][78000/78269] Elapsed 107m 12s (remain 0m 22s) Loss: 0.1705(0.1164) Grad: 8.9395  LR: 0.00000303  \n",
    "Epoch: [4][78268/78269] Elapsed 107m 34s (remain 0m 0s) Loss: 0.2606(0.1164) Grad: 7.8040  LR: 0.00000302  \n",
    "EVAL: [0/19225] Elapsed 0m 1s (remain 349m 47s) Loss: 0.0035(0.0035) \n",
    "EVAL: [1000/19225] Elapsed 0m 13s (remain 4m 0s) Loss: 0.7636(0.2184) \n",
    "EVAL: [2000/19225] Elapsed 0m 24s (remain 3m 32s) Loss: 0.0000(0.1861) \n",
    "EVAL: [3000/19225] Elapsed 0m 35s (remain 3m 9s) Loss: 0.0000(0.1494) \n",
    "EVAL: [4000/19225] Elapsed 0m 47s (remain 3m 2s) Loss: 0.0965(0.1407) \n",
    "EVAL: [5000/19225] Elapsed 1m 1s (remain 2m 54s) Loss: 0.0057(0.1522) \n",
    "EVAL: [6000/19225] Elapsed 1m 15s (remain 2m 46s) Loss: 0.2783(0.1587) \n",
    "EVAL: [7000/19225] Elapsed 1m 29s (remain 2m 36s) Loss: 0.0069(0.1632) \n",
    "EVAL: [8000/19225] Elapsed 1m 44s (remain 2m 25s) Loss: 0.0020(0.1696) \n",
    "EVAL: [9000/19225] Elapsed 1m 58s (remain 2m 14s) Loss: 0.0025(0.1763) \n",
    "EVAL: [10000/19225] Elapsed 2m 13s (remain 2m 3s) Loss: 0.0316(0.1787) \n",
    "EVAL: [11000/19225] Elapsed 2m 29s (remain 1m 51s) Loss: 0.1695(0.1798) \n",
    "EVAL: [12000/19225] Elapsed 2m 44s (remain 1m 39s) Loss: 0.0010(0.1830) \n",
    "EVAL: [13000/19225] Elapsed 3m 0s (remain 1m 26s) Loss: 0.0066(0.1846) \n",
    "EVAL: [14000/19225] Elapsed 3m 17s (remain 1m 13s) Loss: 0.0012(0.1837) \n",
    "EVAL: [15000/19225] Elapsed 3m 33s (remain 1m 0s) Loss: 0.0079(0.1849) \n",
    "EVAL: [16000/19225] Elapsed 3m 51s (remain 0m 46s) Loss: 0.5376(0.1854) \n",
    "EVAL: [17000/19225] Elapsed 4m 9s (remain 0m 32s) Loss: 0.0253(0.1849) \n",
    "EVAL: [18000/19225] Elapsed 4m 27s (remain 0m 18s) Loss: 0.0088(0.1838) \n",
    "EVAL: [19000/19225] Elapsed 4m 48s (remain 0m 3s) Loss: 0.9802(0.1832) \n",
    "EVAL: [19224/19225] Elapsed 4m 54s (remain 0m 0s) Loss: 0.1449(0.1830) \n",
    "Epoch 4 - avg_train_loss: 0.1164  avg_val_loss: 0.1830  time: 7006s\n",
    "Epoch 4 - Score: 0.4857 - Threshold: 0.03700\n",
    "Epoch 4 - Save Best Score: 0.4857 Model\n",
    "Epoch: [5][0/78269] Elapsed 0m 1s (remain 1536m 37s) Loss: 0.1153(0.1153) Grad: 6.0139  LR: 0.00000302  \n",
    "Epoch: [5][1000/78269] Elapsed 1m 23s (remain 107m 5s) Loss: 0.0450(0.0969) Grad: 2.7287  LR: 0.00000299  \n",
    "Epoch: [5][2000/78269] Elapsed 2m 45s (remain 104m 55s) Loss: 0.1526(0.0951) Grad: 9.4190  LR: 0.00000295  \n",
    "Epoch: [5][3000/78269] Elapsed 4m 7s (remain 103m 20s) Loss: 0.6710(0.0981) Grad: 25.0426  LR: 0.00000292  \n",
    "Epoch: [5][4000/78269] Elapsed 5m 29s (remain 101m 54s) Loss: 0.0444(0.0993) Grad: 4.0984  LR: 0.00000288  \n",
    "Epoch: [5][5000/78269] Elapsed 6m 51s (remain 100m 28s) Loss: 0.0101(0.1014) Grad: 1.5684  LR: 0.00000285  \n",
    "Epoch: [5][6000/78269] Elapsed 8m 13s (remain 99m 2s) Loss: 0.0068(0.1016) Grad: 0.3127  LR: 0.00000282  \n",
    "Epoch: [5][7000/78269] Elapsed 9m 35s (remain 97m 38s) Loss: 0.1396(0.1019) Grad: 5.1966  LR: 0.00000278  \n",
    "Epoch: [5][8000/78269] Elapsed 10m 57s (remain 96m 17s) Loss: 0.2667(0.1028) Grad: 37.3245  LR: 0.00000275  \n",
    "Epoch: [5][9000/78269] Elapsed 12m 19s (remain 94m 54s) Loss: 0.1562(0.1030) Grad: 13.7763  LR: 0.00000272  \n",
    "Epoch: [5][10000/78269] Elapsed 13m 42s (remain 93m 32s) Loss: 0.0722(0.1039) Grad: 6.3934  LR: 0.00000268  \n",
    "Epoch: [5][11000/78269] Elapsed 15m 3s (remain 92m 7s) Loss: 0.2104(0.1040) Grad: 13.5479  LR: 0.00000265  \n",
    "Epoch: [5][12000/78269] Elapsed 16m 26s (remain 90m 45s) Loss: 0.1643(0.1041) Grad: 8.7678  LR: 0.00000262  \n",
    "Epoch: [5][13000/78269] Elapsed 17m 48s (remain 89m 22s) Loss: 0.2269(0.1039) Grad: 6.5629  LR: 0.00000259  \n",
    "Epoch: [5][14000/78269] Elapsed 19m 10s (remain 87m 58s) Loss: 0.1639(0.1041) Grad: 17.9874  LR: 0.00000255  \n",
    "Epoch: [5][15000/78269] Elapsed 20m 32s (remain 86m 37s) Loss: 0.0052(0.1042) Grad: 0.6602  LR: 0.00000252  \n",
    "Epoch: [5][16000/78269] Elapsed 21m 54s (remain 85m 14s) Loss: 0.0215(0.1047) Grad: 2.5555  LR: 0.00000249  \n",
    "Epoch: [5][17000/78269] Elapsed 23m 16s (remain 83m 51s) Loss: 0.0110(0.1045) Grad: 1.0891  LR: 0.00000246  \n",
    "Epoch: [5][18000/78269] Elapsed 24m 38s (remain 82m 29s) Loss: 0.3395(0.1045) Grad: 11.1527  LR: 0.00000242  \n",
    "Epoch: [5][19000/78269] Elapsed 26m 1s (remain 81m 9s) Loss: 0.0051(0.1046) Grad: 0.8798  LR: 0.00000239  \n",
    "Epoch: [5][20000/78269] Elapsed 27m 24s (remain 79m 49s) Loss: 0.0073(0.1045) Grad: 0.4446  LR: 0.00000236  \n",
    "Epoch: [5][21000/78269] Elapsed 28m 47s (remain 78m 31s) Loss: 0.0395(0.1042) Grad: 1.9581  LR: 0.00000233  \n",
    "Epoch: [5][22000/78269] Elapsed 30m 11s (remain 77m 12s) Loss: 0.0006(0.1044) Grad: 0.0566  LR: 0.00000230  \n",
    "Epoch: [5][23000/78269] Elapsed 31m 34s (remain 75m 52s) Loss: 0.2185(0.1041) Grad: 8.1562  LR: 0.00000227  \n",
    "Epoch: [5][24000/78269] Elapsed 32m 57s (remain 74m 31s) Loss: 0.0028(0.1043) Grad: 0.3289  LR: 0.00000224  \n",
    "Epoch: [5][25000/78269] Elapsed 34m 20s (remain 73m 11s) Loss: 0.0042(0.1042) Grad: 0.4896  LR: 0.00000221  \n",
    "Epoch: [5][26000/78269] Elapsed 35m 44s (remain 71m 50s) Loss: 0.0031(0.1044) Grad: 1.0223  LR: 0.00000217  \n",
    "Epoch: [5][27000/78269] Elapsed 37m 7s (remain 70m 30s) Loss: 0.0305(0.1044) Grad: 3.7406  LR: 0.00000214  \n",
    "Epoch: [5][28000/78269] Elapsed 38m 31s (remain 69m 9s) Loss: 0.0290(0.1045) Grad: 1.5718  LR: 0.00000211  \n",
    "Epoch: [5][29000/78269] Elapsed 39m 54s (remain 67m 48s) Loss: 0.0609(0.1046) Grad: 4.8479  LR: 0.00000208  \n",
    "Epoch: [5][30000/78269] Elapsed 41m 18s (remain 66m 26s) Loss: 0.0009(0.1050) Grad: 0.1091  LR: 0.00000205  \n",
    "Epoch: [5][31000/78269] Elapsed 42m 41s (remain 65m 5s) Loss: 0.0761(0.1049) Grad: 14.9728  LR: 0.00000202  \n",
    "Epoch: [5][32000/78269] Elapsed 44m 4s (remain 63m 43s) Loss: 0.0102(0.1049) Grad: 1.9177  LR: 0.00000199  \n",
    "Epoch: [5][33000/78269] Elapsed 45m 28s (remain 62m 22s) Loss: 0.0020(0.1050) Grad: 0.3353  LR: 0.00000196  \n",
    "Epoch: [5][34000/78269] Elapsed 46m 51s (remain 61m 1s) Loss: 0.0553(0.1051) Grad: 3.4040  LR: 0.00000193  \n",
    "Epoch: [5][35000/78269] Elapsed 48m 16s (remain 59m 40s) Loss: 0.1996(0.1051) Grad: 8.5004  LR: 0.00000191  \n",
    "Epoch: [5][36000/78269] Elapsed 49m 39s (remain 58m 17s) Loss: 0.3101(0.1050) Grad: 27.7870  LR: 0.00000188  \n",
    "Epoch: [5][37000/78269] Elapsed 51m 2s (remain 56m 55s) Loss: 0.3348(0.1050) Grad: 12.6695  LR: 0.00000185  \n",
    "Epoch: [5][38000/78269] Elapsed 52m 25s (remain 55m 33s) Loss: 0.0026(0.1050) Grad: 0.2642  LR: 0.00000182  \n",
    "Epoch: [5][39000/78269] Elapsed 53m 48s (remain 54m 10s) Loss: 0.2682(0.1049) Grad: 29.3156  LR: 0.00000179  \n",
    "Epoch: [5][40000/78269] Elapsed 55m 10s (remain 52m 46s) Loss: 0.0273(0.1049) Grad: 2.5182  LR: 0.00000176  \n",
    "Epoch: [5][41000/78269] Elapsed 56m 32s (remain 51m 23s) Loss: 0.0897(0.1050) Grad: 5.5454  LR: 0.00000173  \n",
    "Epoch: [5][42000/78269] Elapsed 57m 54s (remain 50m 0s) Loss: 0.3062(0.1050) Grad: 10.6544  LR: 0.00000171  \n",
    "Epoch: [5][43000/78269] Elapsed 59m 16s (remain 48m 37s) Loss: 0.0037(0.1051) Grad: 0.3295  LR: 0.00000168  \n",
    "Epoch: [5][44000/78269] Elapsed 60m 39s (remain 47m 14s) Loss: 0.0979(0.1051) Grad: 8.1895  LR: 0.00000165  \n",
    "Epoch: [5][45000/78269] Elapsed 62m 1s (remain 45m 51s) Loss: 0.1088(0.1051) Grad: 39.3498  LR: 0.00000162  \n",
    "Epoch: [5][46000/78269] Elapsed 63m 23s (remain 44m 28s) Loss: 0.1440(0.1051) Grad: 11.0786  LR: 0.00000159  \n",
    "Epoch: [5][47000/78269] Elapsed 64m 45s (remain 43m 5s) Loss: 0.0852(0.1050) Grad: 6.7354  LR: 0.00000157  \n",
    "Epoch: [5][48000/78269] Elapsed 66m 7s (remain 41m 41s) Loss: 0.0055(0.1051) Grad: 0.6591  LR: 0.00000154  \n",
    "Epoch: [5][49000/78269] Elapsed 67m 29s (remain 40m 18s) Loss: 0.0846(0.1051) Grad: 5.3233  LR: 0.00000151  \n",
    "Epoch: [5][50000/78269] Elapsed 68m 51s (remain 38m 55s) Loss: 0.1464(0.1052) Grad: 13.5174  LR: 0.00000149  \n",
    "Epoch: [5][51000/78269] Elapsed 70m 14s (remain 37m 33s) Loss: 0.0469(0.1052) Grad: 16.9718  LR: 0.00000146  \n",
    "Epoch: [5][52000/78269] Elapsed 71m 36s (remain 36m 10s) Loss: 0.0005(0.1052) Grad: 0.0724  LR: 0.00000144  \n",
    "Epoch: [5][53000/78269] Elapsed 72m 58s (remain 34m 47s) Loss: 0.0143(0.1053) Grad: 0.7911  LR: 0.00000141  \n",
    "Epoch: [5][54000/78269] Elapsed 74m 20s (remain 33m 24s) Loss: 0.0025(0.1054) Grad: 0.1830  LR: 0.00000138  \n",
    "Epoch: [5][55000/78269] Elapsed 75m 42s (remain 32m 1s) Loss: 0.1263(0.1054) Grad: 7.0805  LR: 0.00000136  \n",
    "Epoch: [5][56000/78269] Elapsed 77m 4s (remain 30m 38s) Loss: 0.5877(0.1054) Grad: 17.5836  LR: 0.00000133  \n",
    "Epoch: [5][57000/78269] Elapsed 78m 27s (remain 29m 16s) Loss: 0.0047(0.1053) Grad: 0.6484  LR: 0.00000131  \n",
    "Epoch: [5][58000/78269] Elapsed 79m 49s (remain 27m 53s) Loss: 0.1924(0.1053) Grad: 25.1076  LR: 0.00000128  \n",
    "Epoch: [5][59000/78269] Elapsed 81m 11s (remain 26m 30s) Loss: 0.0955(0.1053) Grad: 8.4341  LR: 0.00000126  \n",
    "Epoch: [5][60000/78269] Elapsed 82m 33s (remain 25m 8s) Loss: 0.2105(0.1052) Grad: 3.7268  LR: 0.00000123  \n",
    "Epoch: [5][61000/78269] Elapsed 83m 56s (remain 23m 45s) Loss: 0.1870(0.1052) Grad: 13.9925  LR: 0.00000121  \n",
    "Epoch: [5][62000/78269] Elapsed 85m 18s (remain 22m 23s) Loss: 0.2326(0.1052) Grad: 5.3898  LR: 0.00000118  \n",
    "Epoch: [5][63000/78269] Elapsed 86m 40s (remain 21m 0s) Loss: 0.0160(0.1052) Grad: 13.7896  LR: 0.00000116  \n",
    "Epoch: [5][64000/78269] Elapsed 88m 3s (remain 19m 37s) Loss: 0.0005(0.1052) Grad: 0.0518  LR: 0.00000114  \n",
    "Epoch: [5][65000/78269] Elapsed 89m 25s (remain 18m 15s) Loss: 0.0254(0.1052) Grad: 2.0818  LR: 0.00000111  \n",
    "Epoch: [5][66000/78269] Elapsed 90m 47s (remain 16m 52s) Loss: 0.0038(0.1051) Grad: 0.3509  LR: 0.00000109  \n",
    "Epoch: [5][67000/78269] Elapsed 92m 9s (remain 15m 29s) Loss: 0.0359(0.1051) Grad: 5.1266  LR: 0.00000107  \n",
    "Epoch: [5][68000/78269] Elapsed 93m 31s (remain 14m 7s) Loss: 0.2468(0.1051) Grad: 13.8404  LR: 0.00000104  \n",
    "Epoch: [5][69000/78269] Elapsed 94m 53s (remain 12m 44s) Loss: 0.0005(0.1051) Grad: 0.0594  LR: 0.00000102  \n",
    "Epoch: [5][70000/78269] Elapsed 96m 15s (remain 11m 22s) Loss: 0.0411(0.1052) Grad: 31.5831  LR: 0.00000100  \n",
    "Epoch: [5][71000/78269] Elapsed 97m 37s (remain 9m 59s) Loss: 0.0253(0.1052) Grad: 1.0145  LR: 0.00000098  \n",
    "Epoch: [5][72000/78269] Elapsed 98m 59s (remain 8m 37s) Loss: 0.0441(0.1051) Grad: 9.5842  LR: 0.00000096  \n",
    "Epoch: [5][73000/78269] Elapsed 100m 21s (remain 7m 14s) Loss: 0.0007(0.1051) Grad: 0.0417  LR: 0.00000093  \n",
    "Epoch: [5][74000/78269] Elapsed 101m 43s (remain 5m 52s) Loss: 0.2893(0.1052) Grad: 7.0211  LR: 0.00000091  \n",
    "Epoch: [5][75000/78269] Elapsed 103m 5s (remain 4m 29s) Loss: 0.0167(0.1051) Grad: 0.8156  LR: 0.00000089  \n",
    "Epoch: [5][76000/78269] Elapsed 104m 27s (remain 3m 7s) Loss: 0.0971(0.1051) Grad: 6.3094  LR: 0.00000087  \n",
    "Epoch: [5][77000/78269] Elapsed 105m 49s (remain 1m 44s) Loss: 0.5003(0.1052) Grad: 12.4228  LR: 0.00000085  \n",
    "Epoch: [5][78000/78269] Elapsed 107m 12s (remain 0m 22s) Loss: 0.3080(0.1052) Grad: 22.8545  LR: 0.00000083  \n",
    "Epoch: [5][78268/78269] Elapsed 107m 34s (remain 0m 0s) Loss: 0.0424(0.1052) Grad: 2.3095  LR: 0.00000082  \n",
    "EVAL: [0/19225] Elapsed 0m 1s (remain 326m 18s) Loss: 0.0003(0.0003) \n",
    "EVAL: [1000/19225] Elapsed 0m 13s (remain 3m 59s) Loss: 1.1442(0.3031) \n",
    "EVAL: [2000/19225] Elapsed 0m 24s (remain 3m 32s) Loss: 0.0000(0.2527) \n",
    "EVAL: [3000/19225] Elapsed 0m 35s (remain 3m 9s) Loss: 0.0000(0.2022) \n",
    "EVAL: [4000/19225] Elapsed 0m 47s (remain 3m 2s) Loss: 0.1137(0.1916) \n",
    "EVAL: [5000/19225] Elapsed 1m 1s (remain 2m 54s) Loss: 0.0005(0.2088) \n",
    "EVAL: [6000/19225] Elapsed 1m 15s (remain 2m 46s) Loss: 0.4788(0.2164) \n",
    "EVAL: [7000/19225] Elapsed 1m 29s (remain 2m 36s) Loss: 0.0678(0.2237) \n",
    "EVAL: [8000/19225] Elapsed 1m 44s (remain 2m 26s) Loss: 0.0151(0.2334) \n",
    "EVAL: [9000/19225] Elapsed 1m 58s (remain 2m 15s) Loss: 0.0001(0.2425) \n",
    "EVAL: [10000/19225] Elapsed 2m 14s (remain 2m 3s) Loss: 0.0210(0.2465) \n",
    "EVAL: [11000/19225] Elapsed 2m 29s (remain 1m 51s) Loss: 0.3022(0.2483) \n",
    "EVAL: [12000/19225] Elapsed 2m 44s (remain 1m 39s) Loss: 0.0001(0.2520) \n",
    "EVAL: [13000/19225] Elapsed 3m 0s (remain 1m 26s) Loss: 0.0008(0.2547) \n",
    "EVAL: [14000/19225] Elapsed 3m 17s (remain 1m 13s) Loss: 0.0001(0.2534) \n",
    "EVAL: [15000/19225] Elapsed 3m 33s (remain 1m 0s) Loss: 0.0019(0.2551) \n",
    "EVAL: [16000/19225] Elapsed 3m 51s (remain 0m 46s) Loss: 0.8604(0.2558) \n",
    "EVAL: [17000/19225] Elapsed 4m 9s (remain 0m 32s) Loss: 0.0044(0.2549) \n",
    "EVAL: [18000/19225] Elapsed 4m 27s (remain 0m 18s) Loss: 0.0026(0.2534) \n",
    "EVAL: [19000/19225] Elapsed 4m 48s (remain 0m 3s) Loss: 1.5066(0.2529) \n",
    "EVAL: [19224/19225] Elapsed 4m 54s (remain 0m 0s) Loss: 0.0455(0.2532) \n",
    "Epoch 5 - avg_train_loss: 0.1052  avg_val_loss: 0.2532  time: 7013s\n",
    "Epoch 5 - Score: 0.4893 - Threshold: 0.01000\n",
    "Epoch 5 - Save Best Score: 0.4893 Model\n",
    "Epoch: [6][0/78269] Elapsed 0m 1s (remain 1633m 56s) Loss: 0.0172(0.0172) Grad: 4.8524  LR: 0.00000082  \n",
    "Epoch: [6][1000/78269] Elapsed 1m 23s (remain 107m 55s) Loss: 0.0008(0.0882) Grad: 0.1213  LR: 0.00000080  \n",
    "Epoch: [6][2000/78269] Elapsed 2m 46s (remain 105m 48s) Loss: 0.1588(0.0919) Grad: 11.8090  LR: 0.00000078  \n",
    "Epoch: [6][3000/78269] Elapsed 4m 8s (remain 104m 0s) Loss: 0.0417(0.0923) Grad: 7.6065  LR: 0.00000076  \n",
    "Epoch: [6][4000/78269] Elapsed 5m 30s (remain 102m 19s) Loss: 0.2161(0.0947) Grad: 4.7783  LR: 0.00000074  \n",
    "Epoch: [6][5000/78269] Elapsed 6m 52s (remain 100m 50s) Loss: 0.1747(0.0958) Grad: 11.8670  LR: 0.00000072  \n",
    "Epoch: [6][6000/78269] Elapsed 8m 15s (remain 99m 21s) Loss: 0.0029(0.0969) Grad: 0.7641  LR: 0.00000070  \n",
    "Epoch: [6][7000/78269] Elapsed 9m 36s (remain 97m 53s) Loss: 0.0124(0.0981) Grad: 5.6094  LR: 0.00000069  \n",
    "Epoch: [6][8000/78269] Elapsed 10m 59s (remain 96m 28s) Loss: 0.1377(0.0977) Grad: 15.5789  LR: 0.00000067  \n",
    "Epoch: [6][9000/78269] Elapsed 12m 21s (remain 95m 3s) Loss: 0.2864(0.0979) Grad: 11.8884  LR: 0.00000065  \n",
    "Epoch: [6][10000/78269] Elapsed 13m 43s (remain 93m 39s) Loss: 0.2581(0.0982) Grad: 10.8977  LR: 0.00000063  \n",
    "Epoch: [6][11000/78269] Elapsed 15m 5s (remain 92m 16s) Loss: 0.0055(0.0986) Grad: 3.0285  LR: 0.00000061  \n",
    "Epoch: [6][12000/78269] Elapsed 16m 27s (remain 90m 51s) Loss: 0.0514(0.0988) Grad: 22.5642  LR: 0.00000059  \n",
    "Epoch: [6][13000/78269] Elapsed 17m 49s (remain 89m 30s) Loss: 0.0884(0.0993) Grad: 14.8080  LR: 0.00000058  \n",
    "Epoch: [6][14000/78269] Elapsed 19m 11s (remain 88m 7s) Loss: 0.0980(0.0990) Grad: 10.7209  LR: 0.00000056  \n",
    "Epoch: [6][15000/78269] Elapsed 20m 34s (remain 86m 45s) Loss: 0.0444(0.0994) Grad: 11.5517  LR: 0.00000054  \n",
    "Epoch: [6][16000/78269] Elapsed 21m 56s (remain 85m 21s) Loss: 0.0001(0.0998) Grad: 0.0036  LR: 0.00000053  \n",
    "Epoch: [6][17000/78269] Elapsed 23m 18s (remain 83m 58s) Loss: 0.3638(0.1003) Grad: 9.9523  LR: 0.00000051  \n",
    "Epoch: [6][18000/78269] Elapsed 24m 39s (remain 82m 35s) Loss: 0.0143(0.1007) Grad: 2.9004  LR: 0.00000049  \n",
    "Epoch: [6][19000/78269] Elapsed 26m 2s (remain 81m 12s) Loss: 0.0081(0.1008) Grad: 3.5322  LR: 0.00000048  \n",
    "Epoch: [6][20000/78269] Elapsed 27m 24s (remain 79m 50s) Loss: 0.0005(0.1012) Grad: 0.0410  LR: 0.00000046  \n",
    "Epoch: [6][21000/78269] Elapsed 28m 47s (remain 78m 31s) Loss: 0.1814(0.1012) Grad: 19.9241  LR: 0.00000045  \n",
    "Epoch: [6][22000/78269] Elapsed 30m 12s (remain 77m 16s) Loss: 0.0008(0.1012) Grad: 0.1451  LR: 0.00000043  \n",
    "Epoch: [6][23000/78269] Elapsed 31m 37s (remain 75m 58s) Loss: 0.0176(0.1016) Grad: 13.2058  LR: 0.00000042  \n",
    "Epoch: [6][24000/78269] Elapsed 33m 3s (remain 74m 44s) Loss: 0.1447(0.1015) Grad: 14.2226  LR: 0.00000040  \n",
    "Epoch: [6][25000/78269] Elapsed 34m 30s (remain 73m 31s) Loss: 0.0129(0.1015) Grad: 1.5069  LR: 0.00000039  \n",
    "Epoch: [6][26000/78269] Elapsed 35m 53s (remain 72m 9s) Loss: 0.4651(0.1017) Grad: 13.4672  LR: 0.00000037  \n",
    "Epoch: [6][27000/78269] Elapsed 37m 17s (remain 70m 48s) Loss: 0.1195(0.1020) Grad: 54.6208  LR: 0.00000036  \n",
    "Epoch: [6][28000/78269] Elapsed 38m 40s (remain 69m 26s) Loss: 0.1052(0.1020) Grad: 7.0461  LR: 0.00000035  \n",
    "Epoch: [6][29000/78269] Elapsed 40m 4s (remain 68m 4s) Loss: 0.0265(0.1022) Grad: 9.0796  LR: 0.00000033  \n",
    "Epoch: [6][30000/78269] Elapsed 41m 27s (remain 66m 41s) Loss: 0.0004(0.1023) Grad: 0.0806  LR: 0.00000032  \n",
    "Epoch: [6][31000/78269] Elapsed 42m 50s (remain 65m 19s) Loss: 0.0028(0.1023) Grad: 0.1901  LR: 0.00000031  \n",
    "Epoch: [6][32000/78269] Elapsed 44m 14s (remain 63m 57s) Loss: 0.0031(0.1023) Grad: 0.5145  LR: 0.00000029  \n",
    "Epoch: [6][33000/78269] Elapsed 45m 37s (remain 62m 35s) Loss: 0.1399(0.1023) Grad: 6.9382  LR: 0.00000028  \n",
    "Epoch: [6][34000/78269] Elapsed 47m 1s (remain 61m 12s) Loss: 0.0112(0.1025) Grad: 1.5765  LR: 0.00000027  \n",
    "Epoch: [6][35000/78269] Elapsed 48m 24s (remain 59m 50s) Loss: 0.2363(0.1025) Grad: 3.0355  LR: 0.00000026  \n",
    "Epoch: [6][36000/78269] Elapsed 49m 47s (remain 58m 27s) Loss: 0.0809(0.1025) Grad: 15.5139  LR: 0.00000024  \n",
    "Epoch: [6][37000/78269] Elapsed 51m 11s (remain 57m 5s) Loss: 0.1013(0.1025) Grad: 12.5861  LR: 0.00000023  \n",
    "Epoch: [6][38000/78269] Elapsed 52m 34s (remain 55m 43s) Loss: 0.0189(0.1027) Grad: 11.2407  LR: 0.00000022  \n",
    "Epoch: [6][39000/78269] Elapsed 53m 58s (remain 54m 20s) Loss: 0.0019(0.1027) Grad: 0.3571  LR: 0.00000021  \n",
    "Epoch: [6][40000/78269] Elapsed 55m 21s (remain 52m 57s) Loss: 0.0015(0.1027) Grad: 1.1053  LR: 0.00000020  \n",
    "Epoch: [6][41000/78269] Elapsed 56m 45s (remain 51m 35s) Loss: 0.0003(0.1028) Grad: 0.0504  LR: 0.00000019  \n",
    "Epoch: [6][42000/78269] Elapsed 58m 8s (remain 50m 12s) Loss: 0.3783(0.1027) Grad: 20.1644  LR: 0.00000018  \n",
    "Epoch: [6][43000/78269] Elapsed 59m 31s (remain 48m 49s) Loss: 0.0008(0.1027) Grad: 0.1719  LR: 0.00000017  \n",
    "Epoch: [6][44000/78269] Elapsed 60m 55s (remain 47m 26s) Loss: 0.3284(0.1029) Grad: 7.6233  LR: 0.00000016  \n",
    "Epoch: [6][45000/78269] Elapsed 62m 18s (remain 46m 3s) Loss: 0.0001(0.1030) Grad: 0.0058  LR: 0.00000015  \n",
    "Epoch: [6][46000/78269] Elapsed 63m 41s (remain 44m 40s) Loss: 0.1920(0.1031) Grad: 6.4039  LR: 0.00000014  \n",
    "Epoch: [6][47000/78269] Elapsed 65m 5s (remain 43m 17s) Loss: 0.0002(0.1032) Grad: 0.0434  LR: 0.00000013  \n",
    "Epoch: [6][48000/78269] Elapsed 66m 28s (remain 41m 55s) Loss: 0.0002(0.1032) Grad: 0.0136  LR: 0.00000013  \n",
    "Epoch: [6][49000/78269] Elapsed 67m 51s (remain 40m 32s) Loss: 0.2572(0.1032) Grad: 5.9379  LR: 0.00000012  \n",
    "Epoch: [6][50000/78269] Elapsed 69m 15s (remain 39m 9s) Loss: 0.0251(0.1032) Grad: 2.5216  LR: 0.00000011  \n",
    "Epoch: [6][51000/78269] Elapsed 70m 38s (remain 37m 46s) Loss: 0.4045(0.1033) Grad: 20.5239  LR: 0.00000010  \n",
    "Epoch: [6][52000/78269] Elapsed 72m 2s (remain 36m 23s) Loss: 0.2744(0.1033) Grad: 20.5902  LR: 0.00000010  \n",
    "Epoch: [6][53000/78269] Elapsed 73m 25s (remain 35m 0s) Loss: 0.0804(0.1033) Grad: 9.8693  LR: 0.00000009  \n",
    "Epoch: [6][54000/78269] Elapsed 74m 49s (remain 33m 37s) Loss: 0.0385(0.1033) Grad: 20.7497  LR: 0.00000008  \n",
    "Epoch: [6][55000/78269] Elapsed 76m 12s (remain 32m 14s) Loss: 0.0328(0.1034) Grad: 3.8528  LR: 0.00000007  \n",
    "Epoch: [6][56000/78269] Elapsed 77m 35s (remain 30m 51s) Loss: 0.0012(0.1034) Grad: 1.7573  LR: 0.00000007  \n",
    "Epoch: [6][57000/78269] Elapsed 78m 59s (remain 29m 28s) Loss: 0.0540(0.1034) Grad: 9.7871  LR: 0.00000006  \n",
    "Epoch: [6][58000/78269] Elapsed 80m 24s (remain 28m 5s) Loss: 0.0004(0.1034) Grad: 0.2009  LR: 0.00000006  \n",
    "Epoch: [6][59000/78269] Elapsed 81m 49s (remain 26m 43s) Loss: 0.0012(0.1033) Grad: 0.3285  LR: 0.00000005  \n",
    "Epoch: [6][60000/78269] Elapsed 83m 12s (remain 25m 20s) Loss: 0.0840(0.1035) Grad: 39.1272  LR: 0.00000005  \n",
    "Epoch: [6][61000/78269] Elapsed 84m 35s (remain 23m 56s) Loss: 0.4088(0.1035) Grad: 37.1337  LR: 0.00000004  \n",
    "Epoch: [6][62000/78269] Elapsed 85m 59s (remain 22m 33s) Loss: 0.2186(0.1035) Grad: 2.6124  LR: 0.00000004  \n",
    "Epoch: [6][63000/78269] Elapsed 87m 22s (remain 21m 10s) Loss: 0.0173(0.1038) Grad: 13.1934  LR: 0.00000003  \n",
    "Epoch: [6][64000/78269] Elapsed 88m 45s (remain 19m 47s) Loss: 0.1634(0.1037) Grad: 6.8625  LR: 0.00000003  \n",
    "Epoch: [6][65000/78269] Elapsed 90m 7s (remain 18m 23s) Loss: 0.0138(0.1038) Grad: 7.4357  LR: 0.00000002  \n",
    "Epoch: [6][66000/78269] Elapsed 91m 30s (remain 17m 0s) Loss: 0.0003(0.1039) Grad: 0.0221  LR: 0.00000002  \n",
    "Epoch: [6][67000/78269] Elapsed 92m 52s (remain 15m 37s) Loss: 0.0002(0.1040) Grad: 0.1851  LR: 0.00000002  \n",
    "Epoch: [6][68000/78269] Elapsed 94m 14s (remain 14m 13s) Loss: 0.1367(0.1041) Grad: 31.7022  LR: 0.00000001  \n",
    "Epoch: [6][69000/78269] Elapsed 95m 36s (remain 12m 50s) Loss: 0.0089(0.1040) Grad: 4.0353  LR: 0.00000001  \n",
    "Epoch: [6][70000/78269] Elapsed 96m 58s (remain 11m 27s) Loss: 0.0014(0.1040) Grad: 0.4575  LR: 0.00000001  \n",
    "Epoch: [6][71000/78269] Elapsed 98m 20s (remain 10m 3s) Loss: 0.0044(0.1041) Grad: 0.5259  LR: 0.00000001  \n",
    "Epoch: [6][72000/78269] Elapsed 99m 42s (remain 8m 40s) Loss: 0.0005(0.1041) Grad: 0.3099  LR: 0.00000001  \n",
    "Epoch: [6][73000/78269] Elapsed 101m 4s (remain 7m 17s) Loss: 0.1738(0.1041) Grad: 38.6550  LR: 0.00000000  \n",
    "Epoch: [6][74000/78269] Elapsed 102m 26s (remain 5m 54s) Loss: 0.0055(0.1041) Grad: 1.9344  LR: 0.00000000  \n",
    "Epoch: [6][75000/78269] Elapsed 103m 49s (remain 4m 31s) Loss: 0.4133(0.1041) Grad: 19.1720  LR: 0.00000000  \n",
    "Epoch: [6][76000/78269] Elapsed 105m 11s (remain 3m 8s) Loss: 0.3042(0.1042) Grad: 12.8148  LR: 0.00000000  \n",
    "Epoch: [6][77000/78269] Elapsed 106m 33s (remain 1m 45s) Loss: 0.0042(0.1042) Grad: 0.9930  LR: 0.00000000  \n",
    "Epoch: [6][78000/78269] Elapsed 107m 56s (remain 0m 22s) Loss: 0.2722(0.1043) Grad: 15.4058  LR: 0.00000000  \n",
    "Epoch: [6][78268/78269] Elapsed 108m 19s (remain 0m 0s) Loss: 0.0002(0.1043) Grad: 0.0330  LR: 0.00000000  \n",
    "EVAL: [0/19225] Elapsed 0m 1s (remain 369m 34s) Loss: 0.0001(0.0001) \n",
    "EVAL: [1000/19225] Elapsed 0m 14s (remain 4m 16s) Loss: 1.3322(0.3857) \n",
    "EVAL: [2000/19225] Elapsed 0m 25s (remain 3m 42s) Loss: 0.0000(0.3232) \n",
    "EVAL: [3000/19225] Elapsed 0m 36s (remain 3m 16s) Loss: 0.0000(0.2577) \n",
    "EVAL: [4000/19225] Elapsed 0m 49s (remain 3m 8s) Loss: 0.1669(0.2453) \n",
    "EVAL: [5000/19225] Elapsed 1m 3s (remain 2m 59s) Loss: 0.0001(0.2669) \n",
    "EVAL: [6000/19225] Elapsed 1m 17s (remain 2m 50s) Loss: 0.6223(0.2772) \n",
    "EVAL: [7000/19225] Elapsed 1m 32s (remain 2m 41s) Loss: 0.0033(0.2871) \n",
    "EVAL: [8000/19225] Elapsed 1m 47s (remain 2m 30s) Loss: 0.0020(0.2984) \n",
    "EVAL: [9000/19225] Elapsed 2m 2s (remain 2m 18s) Loss: 0.0000(0.3099) \n",
    "EVAL: [10000/19225] Elapsed 2m 17s (remain 2m 6s) Loss: 0.0079(0.3150) \n",
    "EVAL: [11000/19225] Elapsed 2m 33s (remain 1m 54s) Loss: 0.3436(0.3179) \n",
    "EVAL: [12000/19225] Elapsed 2m 49s (remain 1m 42s) Loss: 0.0000(0.3221) \n",
    "EVAL: [13000/19225] Elapsed 3m 6s (remain 1m 29s) Loss: 0.0001(0.3255) \n",
    "EVAL: [14000/19225] Elapsed 3m 23s (remain 1m 15s) Loss: 0.0000(0.3243) \n",
    "EVAL: [15000/19225] Elapsed 3m 40s (remain 1m 2s) Loss: 0.0003(0.3262) \n",
    "EVAL: [16000/19225] Elapsed 3m 58s (remain 0m 48s) Loss: 0.9963(0.3269) \n",
    "EVAL: [17000/19225] Elapsed 4m 16s (remain 0m 33s) Loss: 0.0001(0.3257) \n",
    "EVAL: [18000/19225] Elapsed 4m 36s (remain 0m 18s) Loss: 0.0008(0.3240) \n",
    "EVAL: [19000/19225] Elapsed 4m 58s (remain 0m 3s) Loss: 2.0782(0.3238) \n",
    "EVAL: [19224/19225] Elapsed 5m 3s (remain 0m 0s) Loss: 0.0063(0.3242) \n",
    "Epoch 6 - avg_train_loss: 0.1043  avg_val_loss: 0.3242  time: 7074s\n",
    "Epoch 6 - Score: 0.4796 - Threshold: 0.01000\n",
    "Our CV score is 0.4893 using a threshold of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
